{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Claude in the Shell","text":"<p>An embodied AI agent for the Reachy Mini desktop robot, powered by Claude Agent SDK and MCP.</p> <p>\"Your ghost, my shell.\" - Inspired by Ghost in the Shell</p>"},{"location":"#overview","title":"Overview","text":"<p>Claude in the Shell transforms your Reachy Mini into an autonomous AI assistant that can:</p> <ul> <li>Respond to voice commands with \"Hey Reachy\" wake word</li> <li>Control its head, body, and antennas expressively</li> <li>See through its camera and respond to visual cues</li> <li>Remember context across conversations</li> <li>Connect to external services via MCP (Home Assistant, Calendar, GitHub, etc.)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Clone and setup\ngit clone https://github.com/jawhnycooke/claude-in-the-shell.git\ncd claude-in-the-shell\nuv venv &amp;&amp; source .venv/bin/activate\nuv pip install -r requirements.txt\n\n# Run with mock daemon (no hardware needed)\npython -m reachy_agent run --mock\n</code></pre> <p>For detailed setup instructions, see the Getting Started Tutorial.</p>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph Cloud[\"Cloud\"]\n        CLAUDE[(Claude API)]\n    end\n\n    subgraph Agent[\"Claude in the Shell\"]\n        subgraph SDK[\"Claude Agent SDK\"]\n            SESSION[\"Session Manager\"]\n            MCPCLIENT[\"MCP Client\"]\n            HOOKS[\"PreToolUse Hooks\"]\n        end\n\n        subgraph Core[\"Agent Core\"]\n            LOOP[\"ReachyAgentLoop\"]\n            PERM[\"4-Tier Permissions\"]\n        end\n\n        subgraph Memory[\"Memory System\"]\n            MGR[\"MemoryManager\"]\n            CHROMA[(\"ChromaDB&lt;br/&gt;Vector Store\")]\n            SQLITE[(\"SQLite&lt;br/&gt;Profiles\")]\n            EMBED[\"Embeddings&lt;br/&gt;(MiniLM)\"]\n        end\n\n        subgraph Interface[\"Interfaces\"]\n            CLI[\"CLI REPL\"]\n            WEB[\"Web Dashboard\"]\n        end\n\n        subgraph MCP[\"MCP Servers (stdio)\"]\n            REACHY[\"Reachy MCP (23)\"]\n            MEMMCP[\"Memory MCP (4)\"]\n        end\n    end\n\n    subgraph Hardware[\"Reachy Mini\"]\n        DAEMON[\"Daemon API\"]\n        ROBOT[\"Head | Body | Antennas\"]\n    end\n\n    CLAUDE &lt;--&gt;|HTTPS| SESSION\n    SESSION --&gt; LOOP\n    LOOP --&gt; HOOKS\n    HOOKS --&gt; PERM\n    PERM --&gt; MCPCLIENT\n    MCPCLIENT --&gt;|stdio| MCP\n    LOOP &lt;--&gt;|context| MGR\n    MGR --&gt; CHROMA\n    MGR --&gt; SQLITE\n    MGR --&gt; EMBED\n    MEMMCP --&gt; MGR\n    CLI --&gt; LOOP\n    WEB --&gt; LOOP\n    REACHY --&gt;|HTTP| DAEMON\n    DAEMON --&gt; ROBOT\n\n    style SDK fill:#7c4dff,color:#fff\n    style Memory fill:#e1bee7\n    style CLAUDE fill:#f9a825\n    style DAEMON fill:#4caf50</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#27-mcp-tools","title":"27 MCP Tools","text":"<p>The agent exposes robot control and memory via two MCP servers:</p> Server Tools Description Reachy 23 Movement, expressions, audio, perception, lifecycle Memory 4 Semantic search, memory storage, user profiles <p>See MCP Tools Reference for complete documentation.</p>"},{"location":"#4-tier-permission-system","title":"4-Tier Permission System","text":"Tier Behavior Examples 1. Autonomous Execute immediately Body control, reading data 2. Notify Execute and inform Smart home control 3. Confirm Ask before executing Creating events, PRs 4. Forbidden Never execute Security-critical ops"},{"location":"#multiple-interfaces","title":"Multiple Interfaces","text":"<ul> <li>CLI REPL: Rich terminal interface with slash commands</li> <li>Web Dashboard: Browser-based chat with video streaming</li> <li>API: Programmatic access for integrations</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>Phase 1: Foundation - Complete</p> <ul> <li>Agent loop with Claude SDK</li> <li>23 Reachy MCP tools + 4 Memory tools</li> <li>Permission system with SDK hooks</li> <li>CLI, Web, and API interfaces</li> <li>MuJoCo simulation support</li> </ul> <p>Phase 2: Hardware - Next</p> <ul> <li>Raspberry Pi environment</li> <li>Wake word detection</li> <li>Attention state machine</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li> <p> Getting Started</p> <p>Set up your development environment and run your first demo.</p> <p> Tutorial</p> </li> <li> <p> Architecture</p> <p>Understand the system design and component relationships.</p> <p> Overview</p> </li> <li> <p> ReachyAgentLoop Deep Dive</p> <p>Learn how the Perceive \u2192 Think \u2192 Act cycle works under the hood.</p> <p> Deep Dive</p> </li> <li> <p> API Reference</p> <p>Auto-generated documentation from source code.</p> <p> Reference</p> </li> <li> <p> Troubleshooting</p> <p>Common issues and their solutions.</p> <p> Guide</p> </li> </ul>"},{"location":"#development","title":"Development","text":"<pre><code># Install dev dependencies\nuv pip install -r requirements-dev.txt\n\n# Run tests\npytest -v\n\n# Format and lint\nuvx black . &amp;&amp; uvx isort . &amp;&amp; uvx ruff check .\n\n# Build documentation\nmkdocs serve\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Reachy Mini SDK</li> <li>Claude Agent SDK</li> <li>MCP Python SDK</li> </ul>"},{"location":"api/agent/","title":"Agent Module API","text":"<p>The agent module provides the core agent loop implementation using the Claude Agent SDK.</p>"},{"location":"api/agent/#reachyagentloop","title":"ReachyAgentLoop","text":""},{"location":"api/agent/#reachy_agent.agent.agent.ReachyAgentLoop","title":"<code>ReachyAgentLoop(config=None, daemon_url='http://localhost:8000', api_key=None, enable_idle_behavior=True, idle_config=None, enable_memory=True, enable_github=False, github_toolsets=None, enable_motion_blend=True, blend_config=None, breathing_config=None, wobble_config=None)</code>","text":"<p>Main agent loop for Reachy embodied AI.</p> <p>Uses the official Claude Agent SDK (ClaudeSDKClient) for: - Session continuity across multiple exchanges - Hook-based permission enforcement (PreToolUse) - MCP server integration via ClaudeAgentOptions - Interrupt support for robot control</p> <p>The core interaction cycle: 1. Perceive - Gather input from user/sensors 2. Think - Process with Claude via SDK 3. Act - Execute tool calls with permission enforcement</p> <p>Initialize the agent loop.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ReachyConfig | None</code> <p>Reachy configuration.</p> <code>None</code> <code>daemon_url</code> <code>str</code> <p>URL of the Reachy daemon.</p> <code>'http://localhost:8000'</code> <code>api_key</code> <code>str | None</code> <p>Anthropic API key. Uses ANTHROPIC_API_KEY env var if not provided.</p> <code>None</code> <code>enable_idle_behavior</code> <code>bool</code> <p>Whether to enable idle look-around behavior.</p> <code>True</code> <code>idle_config</code> <code>IdleBehaviorConfig | None</code> <p>Optional idle behavior configuration.</p> <code>None</code> <code>enable_memory</code> <code>bool</code> <p>Whether to enable memory system for personalization.</p> <code>True</code> <code>enable_github</code> <code>bool</code> <p>Whether to enable GitHub MCP server integration.</p> <code>False</code> <code>github_toolsets</code> <code>list[str] | None</code> <p>List of GitHub toolsets to enable (repos, issues, etc.).</p> <code>None</code> <code>enable_motion_blend</code> <code>bool</code> <p>Whether to enable motion blending system.</p> <code>True</code> <code>blend_config</code> <code>BlendControllerConfig | None</code> <p>Optional motion blend controller configuration.</p> <code>None</code> <code>breathing_config</code> <code>BreathingConfig | None</code> <p>Optional breathing motion configuration.</p> <code>None</code> <code>wobble_config</code> <code>WobbleConfig | None</code> <p>Optional head wobble configuration.</p> <code>None</code> Source code in <code>src/reachy_agent/agent/agent.py</code> <pre><code>def __init__(\n    self,\n    config: ReachyConfig | None = None,\n    daemon_url: str = \"http://localhost:8000\",\n    api_key: str | None = None,\n    enable_idle_behavior: bool = True,\n    idle_config: IdleBehaviorConfig | None = None,\n    enable_memory: bool = True,\n    enable_github: bool = False,\n    github_toolsets: list[str] | None = None,\n    enable_motion_blend: bool = True,\n    blend_config: BlendControllerConfig | None = None,\n    breathing_config: BreathingConfig | None = None,\n    wobble_config: WobbleConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize the agent loop.\n\n    Args:\n        config: Reachy configuration.\n        daemon_url: URL of the Reachy daemon.\n        api_key: Anthropic API key. Uses ANTHROPIC_API_KEY env var if not provided.\n        enable_idle_behavior: Whether to enable idle look-around behavior.\n        idle_config: Optional idle behavior configuration.\n        enable_memory: Whether to enable memory system for personalization.\n        enable_github: Whether to enable GitHub MCP server integration.\n        github_toolsets: List of GitHub toolsets to enable (repos, issues, etc.).\n        enable_motion_blend: Whether to enable motion blending system.\n        blend_config: Optional motion blend controller configuration.\n        breathing_config: Optional breathing motion configuration.\n        wobble_config: Optional head wobble configuration.\n    \"\"\"\n    self.config = config\n    self.daemon_url = daemon_url\n    self.state = AgentState.INITIALIZING\n\n    # SDK client (replaces manual anthropic.AsyncAnthropic)\n    self._client: ClaudeSDKClient | None = None\n    self._api_key = api_key\n\n    # Permission evaluator for SDK hooks\n    self._permission_evaluator = PermissionEvaluator()\n\n    # Agent state\n    self._turn_counter = 0\n    self._system_prompt: str = \"\"\n\n    # Idle behavior controller\n    self._enable_idle_behavior = enable_idle_behavior\n    self._idle_config = idle_config\n    self._idle_controller: IdleBehaviorController | None = None\n    self._daemon_client: ReachyDaemonClient | None = None\n\n    # Motion blending system\n    self._enable_motion_blend = enable_motion_blend\n    self._blend_config = blend_config\n    self._breathing_config = breathing_config\n    self._wobble_config = wobble_config\n    self._blend_controller: MotionBlendController | None = None\n    self._breathing_motion: BreathingMotion | None = None\n    self._head_wobble: HeadWobble | None = None\n    self._sdk_client: ReachySDKClient | None = None\n\n    # Memory system\n    self._enable_memory = enable_memory\n    self._memory_manager: MemoryManager | None = None\n    self._user_profile: UserProfile | None = None\n    self._last_session: SessionSummary | None = None\n\n    # GitHub MCP integration\n    self._enable_github = enable_github\n    self._github_toolsets = github_toolsets\n\n    # Voice mode flag - when True, skip speak tool (pipeline handles TTS)\n    self._voice_mode: bool = False\n    # Queue to capture speak tool text for pipeline TTS when in voice mode\n    self._voice_mode_speak_queue: list[str] = []\n\n    # Motion degraded mode - set when robot wake-up fails\n    self._motion_degraded: bool = False\n</code></pre>"},{"location":"api/agent/#reachy_agent.agent.agent.ReachyAgentLoop.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize the agent and its components.</p> <p>Loads system prompt, initializes memory, starts idle behavior, and connects to MCP servers via SDK.</p> Source code in <code>src/reachy_agent/agent/agent.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Initialize the agent and its components.\n\n    Loads system prompt, initializes memory, starts idle behavior,\n    and connects to MCP servers via SDK.\n    \"\"\"\n    log.info(\"Initializing Reachy agent loop with Claude Agent SDK\")\n\n    try:\n        # Load system prompt\n        self._system_prompt = load_system_prompt(config=self.config)\n\n        # Initialize memory system if enabled\n        if self._enable_memory:\n            await self._initialize_memory()\n\n        # Update system prompt with memory context\n        if self._enable_memory and (self._user_profile or self._last_session):\n            memory_context = build_memory_context(\n                profile=self._user_profile,\n                last_session=self._last_session,\n            )\n            if memory_context:\n                self._system_prompt = f\"{self._system_prompt}\\n\\n{memory_context}\"\n\n        # Initialize daemon client (shared by behaviors)\n        self._daemon_client = ReachyDaemonClient(base_url=self.daemon_url)\n\n        # Ensure robot is awake before starting behaviors\n        await self._ensure_robot_awake()\n\n        # Initialize motion blending system if enabled\n        if self._enable_motion_blend:\n            await self._initialize_motion_blend()\n        elif self._enable_idle_behavior:\n            # Fallback: standalone idle behavior without blend controller\n            self._idle_controller = IdleBehaviorController(\n                daemon_client=self._daemon_client,\n                config=self._idle_config,\n            )\n            await self._idle_controller.start()\n            log.info(\"Idle behavior controller started (standalone mode)\")\n\n        # Create and connect SDK client (persistent connection for low latency)\n        options = self._build_sdk_options()\n        self._client = ClaudeSDKClient(options)\n        await self._client.connect()\n        log.info(\"SDK client connected (persistent mode)\")\n\n        # Get the model being used for logging\n        model = \"claude-haiku-4-5-20251001\"  # Default\n        if self.config and hasattr(self.config, \"agent\"):\n            model = self.config.agent.model.value\n\n        self.state = AgentState.READY\n        log.info(\n            \"Agent loop initialized with Claude Agent SDK\",\n            model=model,\n            idle_behavior=self._enable_idle_behavior,\n            memory_enabled=self._enable_memory,\n            mcp_servers=list(self._build_mcp_servers().keys()),\n        )\n\n    except Exception as e:\n        self.state = AgentState.ERROR\n        log.error(\"Failed to initialize agent loop\", error=str(e))\n        raise\n</code></pre>"},{"location":"api/agent/#reachy_agent.agent.agent.ReachyAgentLoop.shutdown","title":"<code>shutdown(session_summary='')</code>  <code>async</code>","text":"<p>Shutdown the agent loop gracefully.</p> <p>Parameters:</p> Name Type Description Default <code>session_summary</code> <code>str</code> <p>Optional summary of the session for memory.</p> <code>''</code> Source code in <code>src/reachy_agent/agent/agent.py</code> <pre><code>async def shutdown(self, session_summary: str = \"\") -&gt; None:\n    \"\"\"Shutdown the agent loop gracefully.\n\n    Args:\n        session_summary: Optional summary of the session for memory.\n    \"\"\"\n    log.info(\"Shutting down agent loop\")\n    self.state = AgentState.SHUTDOWN\n\n    # Stop motion blend controller (also stops registered sources)\n    if self._blend_controller:\n        await self._blend_controller.stop()\n        log.info(\"Motion blend controller stopped\")\n\n    # Disconnect Reachy SDK client (for motion control)\n    if self._sdk_client:\n        try:\n            await self._sdk_client.disconnect()\n            log.info(\"Reachy SDK client disconnected\")\n        except Exception as e:\n            log.warning(\"Error disconnecting Reachy SDK client\", error=str(e))\n\n    # Stop idle behavior controller (if running standalone)\n    if self._idle_controller and not self._blend_controller:\n        await self._idle_controller.stop()\n        log.info(\"Idle behavior controller stopped\")\n\n    # Save session and close memory system\n    if self._memory_manager:\n        try:\n            await self._memory_manager.end_session(\n                summary_text=session_summary,\n                key_topics=[],  # Could be populated by conversation analysis\n            )\n            await self._memory_manager.close()\n            log.info(\"Memory system closed\")\n        except Exception as e:\n            log.warning(\"Error closing memory system\", error=str(e))\n\n    # Disconnect persistent SDK client\n    if self._client:\n        try:\n            await self._client.disconnect()\n            log.info(\"SDK client disconnected\")\n        except Exception as e:\n            log.warning(\"Error disconnecting SDK client\", error=str(e))\n\n    # Close daemon client if used\n    if self._daemon_client:\n        await self._daemon_client.close()\n\n    log.info(\"Agent loop shutdown complete\")\n</code></pre>"},{"location":"api/agent/#reachy_agent.agent.agent.ReachyAgentLoop.process_input","title":"<code>process_input(user_input, context=None)</code>  <code>async</code>","text":"<p>Process user input through the agent loop.</p> <p>This is the main entry point for the Perceive \u2192 Think \u2192 Act cycle. Uses ClaudeSDKClient for session continuity and hook-based permissions.</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>str</code> <p>Text input from the user.</p> required <code>context</code> <code>AgentContext | None</code> <p>Optional context information.</p> <code>None</code> <p>Returns:</p> Type Description <code>AgentResponse</code> <p>AgentResponse with the result.</p> Source code in <code>src/reachy_agent/agent/agent.py</code> <pre><code>async def process_input(\n    self,\n    user_input: str,\n    context: AgentContext | None = None,\n) -&gt; AgentResponse:\n    \"\"\"Process user input through the agent loop.\n\n    This is the main entry point for the Perceive \u2192 Think \u2192 Act cycle.\n    Uses ClaudeSDKClient for session continuity and hook-based permissions.\n\n    Args:\n        user_input: Text input from the user.\n        context: Optional context information.\n\n    Returns:\n        AgentResponse with the result.\n    \"\"\"\n    if self.state != AgentState.READY:\n        return AgentResponse(\n            text=\"\",\n            error=f\"Agent not ready. Current state: {self.state.value}\",\n        )\n\n    if self._client is None:\n        return AgentResponse(\n            text=\"\",\n            error=\"Agent client not initialized\",\n        )\n\n    self.state = AgentState.PROCESSING\n    self._turn_counter += 1\n\n    # Set listening state and pause idle behavior during user interaction\n    self.set_listening_state(True)\n    if self._idle_controller:\n        await self._idle_controller.pause()\n\n    # Create context if not provided\n    if context is None:\n        context = AgentContext(\n            user_input=user_input,\n            turn_number=self._turn_counter,\n        )\n    else:\n        context.user_input = user_input\n        context.turn_number = self._turn_counter\n\n    # Bind logging context\n    bind_context(\n        turn=self._turn_counter,\n        input_length=len(user_input),\n    )\n\n    log.info(\"Processing user input via SDK\", input_preview=user_input[:50])\n\n    try:\n        # Build augmented input with context\n        augmented_input = self._build_augmented_input(user_input, context)\n\n        # Process with SDK client\n        response = await self._process_with_sdk(augmented_input, context)\n\n        self.state = AgentState.READY\n        return response\n\n    except Exception as e:\n        log.error(\"Error processing input\", error=str(e))\n        self.state = AgentState.READY  # Recover to ready state\n        return AgentResponse(\n            text=\"\",\n            error=str(e),\n            context=context,\n        )\n\n    finally:\n        clear_context()\n        # Exit listening state and resume idle behavior after processing\n        self.set_listening_state(False)\n        if self._idle_controller:\n            await self._idle_controller.resume()\n</code></pre>"},{"location":"api/agent/#reachy_agent.agent.agent.ReachyAgentLoop.session","title":"<code>session()</code>  <code>async</code>","text":"<p>Context manager for agent session.</p> <p>Handles initialization and cleanup automatically.</p> <p>Yields:</p> Type Description <code>AsyncGenerator[ReachyAgentLoop, None]</code> <p>Initialized agent loop.</p> Source code in <code>src/reachy_agent/agent/agent.py</code> <pre><code>@asynccontextmanager\nasync def session(self) -&gt; AsyncGenerator[ReachyAgentLoop, None]:\n    \"\"\"Context manager for agent session.\n\n    Handles initialization and cleanup automatically.\n\n    Yields:\n        Initialized agent loop.\n    \"\"\"\n    try:\n        await self.initialize()\n        yield self\n    finally:\n        await self.shutdown()\n</code></pre>"},{"location":"api/agent/#agentstate","title":"AgentState","text":""},{"location":"api/agent/#reachy_agent.agent.agent.AgentState","title":"<code>AgentState</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Current state of the agent.</p>"},{"location":"api/agent/#agentcontext","title":"AgentContext","text":""},{"location":"api/agent/#reachy_agent.agent.agent.AgentContext","title":"<code>AgentContext(current_time=datetime.now(), user_input='', conversation_id='', turn_number=0, metadata=dict())</code>  <code>dataclass</code>","text":"<p>Context information passed to the agent loop.</p>"},{"location":"api/agent/#reachy_agent.agent.agent.AgentContext.to_context_string","title":"<code>to_context_string()</code>","text":"<p>Convert context to string for injection into prompts.</p> Source code in <code>src/reachy_agent/agent/agent.py</code> <pre><code>    def to_context_string(self) -&gt; str:\n        \"\"\"Convert context to string for injection into prompts.\"\"\"\n        return f\"\"\"\n## Current Context\n- Time: {self.current_time.strftime('%Y-%m-%d %H:%M:%S')}\n- Day: {self.current_time.strftime('%A')}\n- Conversation turn: {self.turn_number}\n\"\"\"\n</code></pre>"},{"location":"api/agent/#agentresponse","title":"AgentResponse","text":""},{"location":"api/agent/#reachy_agent.agent.agent.AgentResponse","title":"<code>AgentResponse(text, tool_calls=list(), context=None, error=None, cost_usd=None, duration_ms=None)</code>  <code>dataclass</code>","text":"<p>Response from the agent loop.</p>"},{"location":"api/agent/#reachy_agent.agent.agent.AgentResponse.success","title":"<code>success</code>  <code>property</code>","text":"<p>Whether the response was successful.</p>"},{"location":"api/agent/#voice-mode-methods","title":"Voice Mode Methods","text":"<p>Methods for integrating with the voice pipeline and persona system.</p>"},{"location":"api/agent/#set_voice_mode","title":"set_voice_mode","text":"<p>Enable or disable voice mode for TTS integration.</p> <pre><code>def set_voice_mode(self, enabled: bool) -&gt; None:\n    \"\"\"Enable or disable voice mode.\n\n    When voice mode is enabled, the `speak` tool is skipped because\n    responses are synthesized via the voice pipeline's OpenAI TTS.\n    \"\"\"\n</code></pre>"},{"location":"api/agent/#get_voice_mode_speak_text","title":"get_voice_mode_speak_text","text":"<p>Retrieve captured speak tool text for TTS synthesis.</p> <pre><code>def get_voice_mode_speak_text(self) -&gt; str:\n    \"\"\"Get captured speak tool text and clear the queue.\n\n    In voice mode, speak tool calls are intercepted and their text\n    is queued for TTS synthesis by the voice pipeline.\n\n    Returns:\n        Concatenated text from speak tool calls, or empty string.\n    \"\"\"\n</code></pre>"},{"location":"api/agent/#update_system_prompt","title":"update_system_prompt","text":"<p>Update the system prompt for persona switching.</p> <pre><code>async def update_system_prompt(self, new_prompt: str) -&gt; bool:\n    \"\"\"Update the system prompt for persona switching.\n\n    Used by the voice pipeline for Ghost in the Shell themed personas\n    (Motoko/Batou). Requires an active SDK session.\n\n    Args:\n        new_prompt: New system prompt to use.\n\n    Returns:\n        True if updated successfully, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"api/agent/#set_listening_state","title":"set_listening_state","text":"<p>Control antenna freeze during user speech.</p> <pre><code>def set_listening_state(self, listening: bool) -&gt; None:\n    \"\"\"Set whether the robot is listening to the user.\n\n    When listening, antenna motion is frozen via the blend controller\n    to avoid distraction while the user speaks.\n    \"\"\"\n</code></pre>"},{"location":"api/agent/#usage-example","title":"Usage Example","text":"<pre><code>from reachy_agent.agent.agent import ReachyAgentLoop\n\n# Using context manager (recommended)\nasync with ReachyAgentLoop(\n    daemon_url=\"http://localhost:8765\",\n    enable_memory=True,\n).session() as agent:\n    response = await agent.process_input(\"Hello, Reachy!\")\n    print(response.text)\n\n# Manual lifecycle\nagent = ReachyAgentLoop(daemon_url=\"http://localhost:8765\")\nawait agent.initialize()\ntry:\n    response = await agent.process_input(\"Wave at me!\")\n    if response.success:\n        print(response.text)\nfinally:\n    await agent.shutdown()\n</code></pre>"},{"location":"api/agent/#configuration","title":"Configuration","text":"<p>The agent can be configured via:</p> <ul> <li>Constructor parameters</li> <li><code>config/default.yaml</code></li> <li>Environment variables</li> </ul> <p>See Options Module for configuration details.</p>"},{"location":"api/agent/#options-module","title":"Options Module","text":""},{"location":"api/agent/#reachy_agent.agent.options","title":"<code>options</code>","text":"<p>Claude Agent SDK options configuration.</p> <p>Configures the Claude Agent with appropriate settings for embodied AI operation on Reachy Mini.</p> <p>Now uses the official Claude Agent SDK (ClaudeAgentOptions) for configuration. Maintains backwards compatibility with existing dictionary-based options.</p>"},{"location":"api/agent/#reachy_agent.agent.options.load_system_prompt","title":"<code>load_system_prompt(prompt_path=None, config=None, prompts_dir=None)</code>","text":"<p>Load and render the system prompt from external files.</p> <p>Search order: 1. Explicit prompt_path if provided 2. prompts/system/default.md 3. prompts/system/personality.md (full personality) 4. Legacy paths (CLAUDE.md, config/CLAUDE.md)</p> <p>Parameters:</p> Name Type Description Default <code>prompt_path</code> <code>Path | None</code> <p>Optional path to specific prompt file.</p> <code>None</code> <code>config</code> <code>ReachyConfig | None</code> <p>Optional configuration for dynamic context.</p> <code>None</code> <code>prompts_dir</code> <code>Path | None</code> <p>Optional prompts directory override.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Rendered system prompt string.</p> Source code in <code>src/reachy_agent/agent/options.py</code> <pre><code>def load_system_prompt(\n    prompt_path: Path | None = None,\n    config: ReachyConfig | None = None,\n    prompts_dir: Path | None = None,\n) -&gt; str:\n    \"\"\"Load and render the system prompt from external files.\n\n    Search order:\n    1. Explicit prompt_path if provided\n    2. prompts/system/default.md\n    3. prompts/system/personality.md (full personality)\n    4. Legacy paths (CLAUDE.md, config/CLAUDE.md)\n\n    Args:\n        prompt_path: Optional path to specific prompt file.\n        config: Optional configuration for dynamic context.\n        prompts_dir: Optional prompts directory override.\n\n    Returns:\n        Rendered system prompt string.\n    \"\"\"\n    context = get_default_context(config)\n    base_dir = prompts_dir or PROMPTS_DIR\n\n    # 1. Explicit path\n    if prompt_path and prompt_path.exists():\n        log.info(\"Loading system prompt from explicit path\", path=str(prompt_path))\n        template = prompt_path.read_text()\n        return render_template(template, context)\n\n    # 2. Default prompt from prompts folder\n    default_prompt = load_prompt_file(\"system/default.md\", base_dir)\n    if default_prompt:\n        log.info(\"Loading system prompt\", path=\"prompts/system/default.md\")\n        return render_template(default_prompt, context)\n\n    # 3. Personality prompt (fuller version)\n    personality_prompt = load_prompt_file(\"system/personality.md\", base_dir)\n    if personality_prompt:\n        log.info(\"Loading system prompt\", path=\"prompts/system/personality.md\")\n        return render_template(personality_prompt, context)\n\n    # 4. Legacy paths for backwards compatibility\n    legacy_paths = [\n        Path(\"CLAUDE.md\"),\n        Path(\"config/CLAUDE.md\"),\n        Path.home() / \".reachy\" / \"CLAUDE.md\",\n    ]\n\n    for path in legacy_paths:\n        if path.exists():\n            log.info(\"Loading system prompt from legacy path\", path=str(path))\n            template = path.read_text()\n            return render_template(template, context)\n\n    # 5. Minimal fallback (should not happen with proper prompts folder)\n    log.warning(\"No prompt files found, using minimal fallback\")\n    name = config.agent.name if config else \"Jarvis\"\n    return f\"You are {name}, an embodied AI assistant robot. Be helpful and expressive.\"\n</code></pre>"},{"location":"api/agent/#reachy_agent.agent.options.load_persona_prompt","title":"<code>load_persona_prompt(persona, config=None, prompts_dir=None)</code>","text":"<p>Load and render a persona-specific system prompt.</p> <p>Used for persona-based wake word switching. Falls back to default system prompt if persona prompt not found or on error.</p> <p>Security: Validates that resolved paths are within the allowed directory to prevent path traversal attacks (e.g., \"../../../etc/passwd\").</p> <p>Parameters:</p> Name Type Description Default <code>persona</code> <code>Any</code> <p>PersonaConfig instance with prompt_path attribute</p> required <code>config</code> <code>ReachyConfig | None</code> <p>Optional configuration for dynamic context</p> <code>None</code> <code>prompts_dir</code> <code>Path | None</code> <p>Optional prompts directory override</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Rendered persona prompt string</p> Source code in <code>src/reachy_agent/agent/options.py</code> <pre><code>def load_persona_prompt(\n    persona: Any,  # PersonaConfig, but avoid circular import\n    config: ReachyConfig | None = None,\n    prompts_dir: Path | None = None,\n) -&gt; str:\n    \"\"\"Load and render a persona-specific system prompt.\n\n    Used for persona-based wake word switching.\n    Falls back to default system prompt if persona prompt not found or on error.\n\n    Security: Validates that resolved paths are within the allowed directory\n    to prevent path traversal attacks (e.g., \"../../../etc/passwd\").\n\n    Args:\n        persona: PersonaConfig instance with prompt_path attribute\n        config: Optional configuration for dynamic context\n        prompts_dir: Optional prompts directory override\n\n    Returns:\n        Rendered persona prompt string\n    \"\"\"\n    context = get_default_context(config)\n    base_dir = prompts_dir or PROMPTS_DIR\n    persona_name = getattr(persona, \"name\", \"unknown\")\n\n    def _is_safe_path(candidate: Path, allowed_base: Path) -&gt; bool:\n        \"\"\"Validate that candidate path is within allowed_base directory.\n\n        Prevents path traversal attacks by resolving symlinks and checking\n        that the resolved path starts with the allowed base directory.\n        \"\"\"\n        try:\n            resolved = candidate.resolve()\n            base_resolved = allowed_base.resolve()\n            # Use is_relative_to (Python 3.9+) or check str prefix\n            return resolved.is_relative_to(base_resolved)\n        except (ValueError, OSError):\n            return False\n\n    def _try_load_prompt(prompt_path: Path, allowed_base: Path | None = None) -&gt; str | None:\n        \"\"\"Attempt to load and render a prompt file with error handling.\"\"\"\n        if not prompt_path.exists():\n            return None\n\n        # SECURITY: Validate path is within allowed directory\n        if allowed_base is not None and not _is_safe_path(prompt_path, allowed_base):\n            log.warning(\n                \"Rejected persona prompt path outside allowed directory\",\n                persona=persona_name,\n                path=str(prompt_path),\n                allowed_base=str(allowed_base),\n            )\n            return None\n\n        try:\n            log.info(\n                \"Loading persona prompt\",\n                persona=persona_name,\n                path=str(prompt_path),\n            )\n            template = prompt_path.read_text(encoding=\"utf-8\")\n            return render_template(template, context)\n        except (OSError, UnicodeDecodeError) as e:\n            log.warning(\n                \"Failed to read persona prompt file\",\n                persona=persona_name,\n                path=str(prompt_path),\n                error=str(e),\n                error_type=type(e).__name__,\n            )\n            return None\n        except (TypeError, AttributeError, KeyError) as e:\n            # Specific exceptions from template rendering: type conversion errors,\n            # missing attributes, or invalid context keys\n            log.warning(\n                \"Failed to render persona prompt template\",\n                persona=persona_name,\n                path=str(prompt_path),\n                error=str(e),\n                error_type=type(e).__name__,\n            )\n            return None\n\n    # Try persona-specific prompt path from multiple locations\n    if hasattr(persona, \"prompt_path\") and persona.prompt_path:\n        # Try relative to project root (base_dir.parent)\n        # Allow paths within the project root\n        project_root = base_dir.parent\n        result = _try_load_prompt(project_root / persona.prompt_path, project_root)\n        if result is not None:\n            return result\n\n        # Try relative to prompts dir\n        result = _try_load_prompt(base_dir / persona.prompt_path, base_dir)\n        if result is not None:\n            return result\n\n        # NOTE: Direct path from cwd removed for security - only allow paths\n        # relative to known base directories to prevent path traversal\n\n    # Fallback to default system prompt\n    log.warning(\n        \"Persona prompt not found, using default\",\n        persona=persona_name,\n        prompt_path=getattr(persona, \"prompt_path\", None),\n    )\n    return load_system_prompt(config=config, prompts_dir=prompts_dir)\n</code></pre>"},{"location":"api/agent/#reachy_agent.agent.options.build_sdk_agent_options","title":"<code>build_sdk_agent_options(system_prompt, mcp_servers=None, permission_hook=None, allowed_tools=None, max_turns=10, daemon_url='http://localhost:8000', enable_memory=True)</code>","text":"<p>Build ClaudeAgentOptions for the SDK.</p> <p>Creates a fully-configured ClaudeAgentOptions instance for use with ClaudeSDKClient. This is the primary factory for SDK usage.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>System prompt for Claude.</p> required <code>mcp_servers</code> <code>dict[str, dict[str, Any]] | None</code> <p>MCP server configuration. Uses defaults if None.</p> <code>None</code> <code>permission_hook</code> <code>HookFunction | None</code> <p>Optional PreToolUse hook for permissions.</p> <code>None</code> <code>allowed_tools</code> <code>list[str] | None</code> <p>List of allowed MCP tools. Defaults to empty list if None (SDK behavior: empty list means no tool filtering).</p> <code>None</code> <code>max_turns</code> <code>int</code> <p>Maximum agent turns per query.</p> <code>10</code> <code>daemon_url</code> <code>str</code> <p>Reachy daemon URL (used if mcp_servers is None).</p> <code>'http://localhost:8000'</code> <code>enable_memory</code> <code>bool</code> <p>Enable memory MCP server (used if mcp_servers is None).</p> <code>True</code> <p>Returns:</p> Type Description <code>ClaudeAgentOptions</code> <p>Configured ClaudeAgentOptions instance.</p> Example <p>options = build_sdk_agent_options( ...     system_prompt=\"You are Reachy, a helpful robot.\", ...     permission_hook=my_permission_hook, ... ) async with ClaudeSDKClient(options=options) as client: ...     await client.query(\"Hello!\")</p> Source code in <code>src/reachy_agent/agent/options.py</code> <pre><code>def build_sdk_agent_options(\n    system_prompt: str,\n    mcp_servers: dict[str, dict[str, Any]] | None = None,\n    permission_hook: HookFunction | None = None,\n    allowed_tools: list[str] | None = None,\n    max_turns: int = 10,\n    daemon_url: str = \"http://localhost:8000\",\n    enable_memory: bool = True,\n) -&gt; ClaudeAgentOptions:\n    \"\"\"Build ClaudeAgentOptions for the SDK.\n\n    Creates a fully-configured ClaudeAgentOptions instance for use\n    with ClaudeSDKClient. This is the primary factory for SDK usage.\n\n    Args:\n        system_prompt: System prompt for Claude.\n        mcp_servers: MCP server configuration. Uses defaults if None.\n        permission_hook: Optional PreToolUse hook for permissions.\n        allowed_tools: List of allowed MCP tools. Defaults to empty list if None\n            (SDK behavior: empty list means no tool filtering).\n        max_turns: Maximum agent turns per query.\n        daemon_url: Reachy daemon URL (used if mcp_servers is None).\n        enable_memory: Enable memory MCP server (used if mcp_servers is None).\n\n    Returns:\n        Configured ClaudeAgentOptions instance.\n\n    Example:\n        &gt;&gt;&gt; options = build_sdk_agent_options(\n        ...     system_prompt=\"You are Reachy, a helpful robot.\",\n        ...     permission_hook=my_permission_hook,\n        ... )\n        &gt;&gt;&gt; async with ClaudeSDKClient(options=options) as client:\n        ...     await client.query(\"Hello!\")\n    \"\"\"\n    # Use provided servers or build defaults\n    servers = mcp_servers or build_mcp_server_config(daemon_url, enable_memory)\n\n    # Build hooks if permission_hook provided\n    hooks: dict[str, list[HookMatcher]] | None = None\n    if permission_hook is not None:\n        hooks = {\n            \"PreToolUse\": [HookMatcher(matcher=None, hooks=[permission_hook])]\n        }\n\n    return ClaudeAgentOptions(\n        system_prompt=system_prompt,\n        mcp_servers=servers,\n        hooks=hooks,\n        allowed_tools=allowed_tools or [],\n        max_turns=max_turns,\n    )\n</code></pre>"},{"location":"api/agent/#voice-mode-integration","title":"Voice Mode Integration","text":"<p>The agent integrates with the voice pipeline for real-time voice interaction:</p> <pre><code>sequenceDiagram\n    participant VP as VoicePipeline\n    participant A as ReachyAgentLoop\n    participant SDK as Claude SDK\n    participant TTS as OpenAI TTS\n\n    VP-&gt;&gt;A: set_voice_mode(True)\n    VP-&gt;&gt;A: process_input(transcription)\n    A-&gt;&gt;SDK: Send to Claude\n    SDK-&gt;&gt;A: Response with speak tool\n    A-&gt;&gt;A: Queue speak text (skip tool)\n    A-&gt;&gt;VP: Return response\n    VP-&gt;&gt;A: get_voice_mode_speak_text()\n    A-&gt;&gt;VP: Queued TTS text\n    VP-&gt;&gt;TTS: Synthesize speech</code></pre>"},{"location":"api/agent/#persona-switching","title":"Persona Switching","text":"<p>When a new wake word is detected, the voice pipeline switches personas:</p> <pre><code>flowchart LR\n    WW[Wake Word Detected] --&gt; PM[PersonaManager]\n    PM --&gt; LP[load_persona_prompt]\n    LP --&gt; UP[agent.update_system_prompt]\n    UP --&gt; NEW[New Persona Active]\n\n    style WW fill:#e8f5e9\n    style NEW fill:#c8e6c9</code></pre>"},{"location":"api/configuration/","title":"Configuration Reference","text":"<p>Complete reference for all configuration options in <code>config/default.yaml</code>.</p>"},{"location":"api/configuration/#configuration-file-location","title":"Configuration File Location","text":"<p>The agent looks for configuration in the following order:</p> <ol> <li><code>~/.reachy/config.yaml</code> (user override)</li> <li><code>config/default.yaml</code> (project defaults)</li> </ol>"},{"location":"api/configuration/#agent-settings","title":"Agent Settings","text":"<p>Core agent configuration:</p> <pre><code>agent:\n  name: Jarvis              # Agent's name for personality\n  wake_word: hey jarvis     # Wake word phrase\n  model: claude-haiku-4-5-20251001  # Claude model to use\n  max_tokens: 512           # Max tokens per response\n</code></pre> Option Type Default Description <code>name</code> string <code>\"Jarvis\"</code> Agent's display name <code>wake_word</code> string <code>\"hey jarvis\"</code> Wake word phrase <code>model</code> string <code>\"claude-haiku-4-5-20251001\"</code> Claude model ID <code>max_tokens</code> int <code>512</code> Maximum response tokens"},{"location":"api/configuration/#perception-settings","title":"Perception Settings","text":"<p>Sensor and detection configuration:</p> <pre><code>perception:\n  wake_word_engine: openwakeword\n  wake_word_sensitivity: 0.5\n  spatial_audio_enabled: true\n  vision_enabled: true\n  face_detection_enabled: true\n</code></pre> Option Type Default Description <code>wake_word_engine</code> string <code>\"openwakeword\"</code> Wake word engine (<code>openwakeword</code>, <code>porcupine</code>) <code>wake_word_sensitivity</code> float <code>0.5</code> Detection sensitivity (0.0-1.0) <code>spatial_audio_enabled</code> bool <code>true</code> Enable spatial audio processing <code>vision_enabled</code> bool <code>true</code> Enable camera for vision <code>face_detection_enabled</code> bool <code>true</code> Enable face detection"},{"location":"api/configuration/#memory-settings","title":"Memory Settings","text":"<p>Semantic memory and profile storage:</p> <pre><code>memory:\n  chroma_path: ~/.reachy/memory/chroma\n  sqlite_path: ~/.reachy/memory/reachy.db\n  embedding_model: all-MiniLM-L6-v2\n  max_memories: 10000\n  retention_days: 90\n</code></pre> Option Type Default Description <code>chroma_path</code> path <code>~/.reachy/memory/chroma</code> ChromaDB storage directory <code>sqlite_path</code> path <code>~/.reachy/memory/reachy.db</code> SQLite database path <code>embedding_model</code> string <code>\"all-MiniLM-L6-v2\"</code> Sentence transformer model <code>max_memories</code> int <code>10000</code> Maximum stored memories <code>retention_days</code> int <code>90</code> Memory retention period"},{"location":"api/configuration/#attention-state-settings","title":"Attention State Settings","text":"<p>Attention state transitions:</p> <pre><code>attention:\n  passive_to_alert_motion_threshold: 0.3\n  alert_to_passive_timeout_minutes: 5\n  engaged_to_alert_silence_seconds: 30\n</code></pre> Option Type Default Description <code>passive_to_alert_motion_threshold</code> float <code>0.3</code> Motion detection threshold <code>alert_to_passive_timeout_minutes</code> int <code>5</code> Minutes before returning to passive <code>engaged_to_alert_silence_seconds</code> int <code>30</code> Silence before disengaging"},{"location":"api/configuration/#resilience-settings","title":"Resilience Settings","text":"<p>Error handling and thermal management:</p> <pre><code>resilience:\n  thermal_threshold_celsius: 80.0\n  api_timeout_seconds: 30.0\n  max_retries: 3\n  offline_llm_model: llama3.2:3b\n</code></pre> Option Type Default Description <code>thermal_threshold_celsius</code> float <code>80.0</code> CPU thermal throttle threshold <code>api_timeout_seconds</code> float <code>30.0</code> API request timeout <code>max_retries</code> int <code>3</code> Maximum retry attempts <code>offline_llm_model</code> string <code>\"llama3.2:3b\"</code> Offline LLM fallback"},{"location":"api/configuration/#privacy-settings","title":"Privacy Settings","text":"<p>Audit logging and data storage:</p> <pre><code>privacy:\n  audit_logging_enabled: true\n  audit_retention_days: 7\n  store_audio: false\n  store_images: false\n</code></pre> Option Type Default Description <code>audit_logging_enabled</code> bool <code>true</code> Enable audit logging <code>audit_retention_days</code> int <code>7</code> Audit log retention <code>store_audio</code> bool <code>false</code> Store audio recordings <code>store_images</code> bool <code>false</code> Store captured images"},{"location":"api/configuration/#motion-blend-settings","title":"Motion Blend Settings","text":"<p>Motion orchestration configuration:</p> <pre><code>motion_blend:\n  enabled: true\n  tick_rate_hz: 100.0\n  command_rate_hz: 20.0\n  smoothing_factor: 0.3\n  pose_limits:\n    pitch_range: [-45, 45]\n    yaw_range: [-45, 45]\n    roll_range: [-30, 30]\n    z_range: [-50, 50]\n    antenna_range: [0, 90]\n</code></pre> Option Type Default Description <code>enabled</code> bool <code>true</code> Enable motion blending <code>tick_rate_hz</code> float <code>100.0</code> Internal loop rate <code>command_rate_hz</code> float <code>20.0</code> Daemon command rate <code>smoothing_factor</code> float <code>0.3</code> Pose interpolation factor <code>pose_limits.pitch_range</code> [float, float] <code>[-45, 45]</code> Pitch safety limits (degrees) <code>pose_limits.yaw_range</code> [float, float] <code>[-45, 45]</code> Yaw safety limits (degrees) <code>pose_limits.roll_range</code> [float, float] <code>[-30, 30]</code> Roll safety limits (degrees) <code>pose_limits.z_range</code> [float, float] <code>[-50, 50]</code> Z offset limits (mm) <code>pose_limits.antenna_range</code> [float, float] <code>[0, 90]</code> Antenna limits (degrees)"},{"location":"api/configuration/#idle-behavior-settings","title":"Idle Behavior Settings","text":"<p>Autonomous look-around behavior:</p> <pre><code>idle_behavior:\n  enabled: true\n  min_look_interval: 3.0\n  max_look_interval: 8.0\n  movement_duration: 1.5\n  yaw_range: [-35, 35]\n  pitch_range: [-15, 20]\n  roll_range: [-8, 8]\n  curiosity_chance: 0.15\n  double_look_chance: 0.10\n  return_to_neutral_chance: 0.25\n  curiosity_intensity: 0.6\n  curiosity_emotions:\n    - curious\n    - thinking\n    - recognition\n  pause_on_interaction: true\n  interaction_cooldown: 2.0\n</code></pre> Option Type Default Description <code>enabled</code> bool <code>true</code> Enable idle behavior <code>min_look_interval</code> float <code>3.0</code> Minimum seconds between looks <code>max_look_interval</code> float <code>8.0</code> Maximum seconds between looks <code>movement_duration</code> float <code>1.5</code> Look movement duration <code>yaw_range</code> [float, float] <code>[-35, 35]</code> Left/right range (degrees) <code>pitch_range</code> [float, float] <code>[-15, 20]</code> Down/up range (degrees) <code>roll_range</code> [float, float] <code>[-8, 8]</code> Tilt range (degrees) <code>curiosity_chance</code> float <code>0.15</code> Probability of curiosity emotion <code>pause_on_interaction</code> bool <code>true</code> Pause during user interaction"},{"location":"api/configuration/#breathing-motion-settings","title":"Breathing Motion Settings","text":"<p>Subtle idle animation:</p> <pre><code>breathing:\n  enabled: false\n  z_amplitude_mm: 5.0\n  z_frequency_hz: 0.1\n  antenna_amplitude_deg: 15.0\n  antenna_frequency_hz: 0.5\n  antenna_base_angle: 45.0\n  pitch_amplitude_deg: 1.5\n  pitch_frequency_hz: 0.12\n  yaw_amplitude_deg: 0.8\n  yaw_frequency_hz: 0.07\n</code></pre> Option Type Default Description <code>enabled</code> bool <code>false</code> Enable breathing animation <code>z_amplitude_mm</code> float <code>5.0</code> Vertical oscillation amplitude <code>z_frequency_hz</code> float <code>0.1</code> Breathing cycle frequency <code>antenna_amplitude_deg</code> float <code>15.0</code> Antenna sway amplitude <code>antenna_frequency_hz</code> float <code>0.5</code> Antenna cycle frequency <code>antenna_base_angle</code> float <code>45.0</code> Neutral antenna position"},{"location":"api/configuration/#head-wobble-settings","title":"Head Wobble Settings","text":"<p>Speech-reactive animation:</p> <pre><code>wobble:\n  enabled: true\n  max_pitch_deg: 8.0\n  max_yaw_deg: 6.0\n  max_roll_deg: 4.0\n  latency_compensation_ms: 80.0\n  smoothing_factor: 0.3\n  noise_scale: 0.2\n</code></pre> Option Type Default Description <code>enabled</code> bool <code>true</code> Enable head wobble <code>max_pitch_deg</code> float <code>8.0</code> Maximum pitch displacement <code>max_yaw_deg</code> float <code>6.0</code> Maximum yaw displacement <code>max_roll_deg</code> float <code>4.0</code> Maximum roll displacement <code>latency_compensation_ms</code> float <code>80.0</code> Audio latency compensation <code>smoothing_factor</code> float <code>0.3</code> Motion smoothing factor"},{"location":"api/configuration/#voice-pipeline-settings","title":"Voice Pipeline Settings","text":"<p>Real-time voice interaction:</p> <pre><code>voice:\n  enabled: false\n  confirmation_beep: true\n  auto_restart: true\n</code></pre>"},{"location":"api/configuration/#persona-configuration","title":"Persona Configuration","text":"<p>Multi-persona wake word system:</p> <pre><code>voice:\n  personas:\n    hey_motoko:\n      name: motoko\n      display_name: Major Kusanagi\n      voice: nova\n      prompt_path: prompts/personas/motoko.md\n    hey_batou:\n      name: batou\n      display_name: Batou\n      voice: onyx\n      prompt_path: prompts/personas/batou.md\n\n  default_persona: hey_motoko\n</code></pre> Option Type Description <code>personas.&lt;key&gt;.name</code> string Internal persona identifier <code>personas.&lt;key&gt;.display_name</code> string Human-readable name <code>personas.&lt;key&gt;.voice</code> string OpenAI TTS voice (<code>alloy</code>, <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, <code>shimmer</code>) <code>personas.&lt;key&gt;.prompt_path</code> path Path to persona system prompt <code>default_persona</code> string Key of default persona"},{"location":"api/configuration/#wake-word-configuration","title":"Wake Word Configuration","text":"<pre><code>voice:\n  wake_word:\n    enabled: true\n    model: hey_jarvis\n    sensitivity: 0.5\n    cooldown_seconds: 2.0\n    custom_models_dir: data/wake_words\n</code></pre> Option Type Default Description <code>enabled</code> bool <code>true</code> Enable wake word detection <code>model</code> string <code>\"hey_jarvis\"</code> Fallback wake word model <code>sensitivity</code> float <code>0.5</code> Detection sensitivity (0.0-1.0) <code>cooldown_seconds</code> float <code>2.0</code> Ignore detections cooldown <code>custom_models_dir</code> path <code>\"data/wake_words\"</code> Custom .onnx models directory"},{"location":"api/configuration/#voice-activity-detection-vad","title":"Voice Activity Detection (VAD)","text":"<pre><code>voice:\n  vad:\n    silence_threshold_ms: 800\n    min_speech_duration_ms: 250\n    max_speech_duration_s: 30.0\n    speech_threshold: 0.5\n</code></pre> Option Type Default Description <code>silence_threshold_ms</code> int <code>800</code> Silence to trigger end-of-speech <code>min_speech_duration_ms</code> int <code>250</code> Minimum valid speech duration <code>max_speech_duration_s</code> float <code>30.0</code> Maximum before timeout <code>speech_threshold</code> float <code>0.5</code> VAD sensitivity"},{"location":"api/configuration/#openai-realtime-configuration","title":"OpenAI Realtime Configuration","text":"<pre><code>voice:\n  openai:\n    model: gpt-realtime-mini\n    voice: alloy\n    sample_rate: 24000\n    temperature: 0.8\n    max_response_tokens: 4096\n    turn_detection_threshold: 0.5\n    turn_detection_silence_ms: 500\n</code></pre> Option Type Default Description <code>model</code> string <code>\"gpt-realtime-mini\"</code> OpenAI Realtime model <code>voice</code> string <code>\"alloy\"</code> TTS voice <code>sample_rate</code> int <code>24000</code> Audio sample rate <code>temperature</code> float <code>0.8</code> Response temperature <code>max_response_tokens</code> int <code>4096</code> Max response length"},{"location":"api/configuration/#audio-hardware-configuration","title":"Audio Hardware Configuration","text":"<pre><code>voice:\n  audio:\n    sample_rate: 16000\n    channels: 1\n    chunk_size: 512\n    format_bits: 16\n    input_device_index: 4\n    output_device_index: 3\n    max_init_retries: 3\n    retry_delay_seconds: 1.0\n    retry_backoff_factor: 2.0\n    output_lead_in_ms: 200\n    input_warmup_chunks: 5\n    health_check_interval_seconds: 5.0\n    max_consecutive_errors: 3\n</code></pre> Option Type Default Description <code>sample_rate</code> int <code>16000</code> Microphone sample rate <code>channels</code> int <code>1</code> Audio channels (mono) <code>chunk_size</code> int <code>512</code> Samples per chunk <code>input_device_index</code> int <code>4</code> Input device (dsnoop) <code>output_device_index</code> int <code>3</code> Output device (dmix)"},{"location":"api/configuration/#degraded-mode-configuration","title":"Degraded Mode Configuration","text":"<pre><code>voice:\n  degraded_mode:\n    skip_wake_word_on_failure: true\n    use_energy_vad_fallback: true\n    log_response_on_tts_failure: true\n</code></pre> Option Type Default Description <code>skip_wake_word_on_failure</code> bool <code>true</code> Always-listening if wake word fails <code>use_energy_vad_fallback</code> bool <code>true</code> Energy-based VAD fallback <code>log_response_on_tts_failure</code> bool <code>true</code> Log response if TTS fails"},{"location":"api/configuration/#sdk-motion-control-settings","title":"SDK Motion Control Settings","text":"<p>Direct Python SDK for low-latency motion:</p> <pre><code>sdk:\n  enabled: true\n  robot_name: reachy_mini\n  connect_timeout_seconds: 10.0\n  fallback_to_http: true\n  max_workers: 1\n  localhost_only: true\n</code></pre> Option Type Default Description <code>enabled</code> bool <code>true</code> Enable SDK motion control <code>robot_name</code> string <code>\"reachy_mini\"</code> Robot name for Zenoh <code>connect_timeout_seconds</code> float <code>10.0</code> SDK connection timeout <code>fallback_to_http</code> bool <code>true</code> Fall back to HTTP on failure <code>max_workers</code> int <code>1</code> Thread pool size <code>localhost_only</code> bool <code>true</code> Only connect to localhost"},{"location":"api/configuration/#integration-settings","title":"Integration Settings","text":"<p>External service integrations:</p> <pre><code>integrations:\n  home_assistant:\n    enabled: false\n    url: null\n    token_env_var: HA_TOKEN\n\n  google_calendar:\n    enabled: false\n    credentials_path: null\n\n  github:\n    enabled: false\n    token_env_var: GITHUB_TOKEN\n    repos: []\n</code></pre> Integration Options <code>home_assistant</code> <code>enabled</code>, <code>url</code>, <code>token_env_var</code> <code>google_calendar</code> <code>enabled</code>, <code>credentials_path</code> <code>github</code> <code>enabled</code>, <code>token_env_var</code>, <code>repos</code>"},{"location":"api/configuration/#environment-variables","title":"Environment Variables","text":"<p>Key environment variables:</p> Variable Description <code>ANTHROPIC_API_KEY</code> Claude API key (required) <code>OPENAI_API_KEY</code> OpenAI API key (for voice pipeline) <code>GITHUB_TOKEN</code> GitHub personal access token <code>HA_TOKEN</code> Home Assistant long-lived access token <code>REACHY_DEBUG</code> Enable debug logging <code>REACHY_CONFIG_PATH</code> Override config file path"},{"location":"api/mcp-servers/","title":"MCP Servers API","text":"<p>The MCP servers expose robot control and memory capabilities to Claude via the Model Context Protocol.</p>"},{"location":"api/mcp-servers/#reachy-mcp-server","title":"Reachy MCP Server","text":"<p>The Reachy MCP server provides 23 tools for robot control.</p>"},{"location":"api/mcp-servers/#server-module","title":"Server Module","text":""},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.reachy_mcp","title":"<code>reachy_mcp</code>","text":"<p>Reachy MCP Server - Exposes robot body control as MCP tools.</p> <p>This server provides tools for controlling the Reachy Mini robot's head, antennas, camera, and audio output. It communicates with the Reachy Daemon running on localhost:8000.</p> <p>Motion tools use fire-and-forget execution to reduce latency - they return \"acknowledged\" immediately while the daemon executes asynchronously.</p>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.reachy_mcp.create_reachy_mcp_server","title":"<code>create_reachy_mcp_server(config=None, daemon_url='http://localhost:8000')</code>","text":"<p>Create and configure the Reachy MCP server.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ReachyConfig | None</code> <p>Optional Reachy configuration.</p> <code>None</code> <code>daemon_url</code> <code>str</code> <p>URL of the Reachy daemon API.</p> <code>'http://localhost:8000'</code> <p>Returns:</p> Type Description <code>FastMCP</code> <p>Configured FastMCP server instance.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/reachy_mcp.py</code> <pre><code>def create_reachy_mcp_server(\n    config: ReachyConfig | None = None,  # noqa: ARG001\n    daemon_url: str = \"http://localhost:8000\",\n) -&gt; FastMCP:\n    \"\"\"Create and configure the Reachy MCP server.\n\n    Args:\n        config: Optional Reachy configuration.\n        daemon_url: URL of the Reachy daemon API.\n\n    Returns:\n        Configured FastMCP server instance.\n    \"\"\"\n    mcp = FastMCP(\"Reachy Body Control\")\n\n    # Create daemon client for hardware communication\n    client = ReachyDaemonClient(base_url=daemon_url)\n\n    @mcp.tool()\n    async def move_head(\n        direction: str,\n        speed: str = \"normal\",\n        degrees: float | None = None,\n    ) -&gt; dict[str, str]:\n        \"\"\"Move Reachy's head to look in a direction.\n\n        Args:\n            direction: Direction to look (left, right, up, down, front).\n            speed: Movement speed (slow, normal, fast).\n            degrees: Optional specific angle in degrees (0-45).\n\n        Returns:\n            Status of the movement operation.\n        \"\"\"\n        valid_directions = [\"left\", \"right\", \"up\", \"down\", \"front\"]\n        valid_speeds = [\"slow\", \"normal\", \"fast\"]\n\n        if direction not in valid_directions:\n            return {\"error\": f\"Invalid direction. Must be one of: {valid_directions}\"}\n        if speed not in valid_speeds:\n            return {\"error\": f\"Invalid speed. Must be one of: {valid_speeds}\"}\n        if degrees is not None and not (0 &lt;= degrees &lt;= 45):\n            return {\"error\": \"Degrees must be between 0 and 45\"}\n\n        log.info(\"Moving head\", direction=direction, speed=speed, degrees=degrees)\n\n        # Fire-and-forget: return immediately while motion executes\n        return await _fire_and_forget(\n            client.move_head(direction=direction, speed=speed, degrees=degrees),\n            \"move_head\",\n        )\n\n    @mcp.tool()\n    async def play_emotion(\n        emotion: str,\n        intensity: float = 0.7,\n    ) -&gt; dict[str, str]:\n        \"\"\"Display an emotional expression through movement and antennas.\n\n        Args:\n            emotion: Emotion to express (happy, sad, curious, excited,\n                    confused, thinking, surprised, tired, alert).\n            intensity: Expression intensity from 0.1 to 1.0.\n\n        Returns:\n            Status of the expression operation.\n        \"\"\"\n        valid_emotions = [\n            \"happy\",\n            \"sad\",\n            \"curious\",\n            \"excited\",\n            \"confused\",\n            \"thinking\",\n            \"surprised\",\n            \"tired\",\n            \"alert\",\n        ]\n\n        if emotion not in valid_emotions:\n            return {\"error\": f\"Invalid emotion. Must be one of: {valid_emotions}\"}\n        if not (0.1 &lt;= intensity &lt;= 1.0):\n            return {\"error\": \"Intensity must be between 0.1 and 1.0\"}\n\n        log.info(\"Playing emotion\", emotion=emotion, intensity=intensity)\n\n        # Fire-and-forget: return immediately while emotion plays\n        return await _fire_and_forget(\n            client.play_emotion(emotion=emotion, intensity=intensity),\n            \"play_emotion\",\n        )\n\n    @mcp.tool()\n    async def speak(\n        text: str,\n        voice: str = \"default\",\n        speed: float = 1.0,\n    ) -&gt; dict[str, str]:\n        \"\"\"Speak text aloud through Reachy's speaker.\n\n        Args:\n            text: Text to speak (max 500 characters).\n            voice: Voice profile to use.\n            speed: Speech speed from 0.5 to 2.0.\n\n        Returns:\n            Status of the speech operation.\n        \"\"\"\n        if len(text) &gt; 500:\n            return {\"error\": \"Text exceeds 500 character limit\"}\n        if not (0.5 &lt;= speed &lt;= 2.0):\n            return {\"error\": \"Speed must be between 0.5 and 2.0\"}\n\n        log.info(\"Speaking\", text_length=len(text), voice=voice, speed=speed)\n\n        result = await client.speak(text=text, voice=voice, speed=speed)\n        return result\n\n    @mcp.tool()\n    async def capture_image(\n        analyze: bool = False,\n        save: bool = False,\n    ) -&gt; dict[str, str]:\n        \"\"\"Capture an image from Reachy's camera.\n\n        Args:\n            analyze: Whether to analyze the image content via vision model.\n            save: Whether to save the image to disk.\n\n        Returns:\n            Image capture result, optionally with analysis.\n        \"\"\"\n        log.info(\"Capturing image\", analyze=analyze, save=save)\n\n        result = await client.capture_image(analyze=analyze, save=save)\n        return result\n\n    @mcp.tool()\n    async def set_antenna_state(\n        left_angle: float | None = None,\n        right_angle: float | None = None,\n        wiggle: bool = False,\n        duration_ms: int = 500,\n    ) -&gt; dict[str, str]:\n        \"\"\"Control antenna positions for expression.\n\n        Args:\n            left_angle: Left antenna angle (0-90 degrees).\n            right_angle: Right antenna angle (0-90 degrees).\n            wiggle: Whether to wiggle the antennas.\n            duration_ms: Duration of the motion in milliseconds.\n\n        Returns:\n            Status of the antenna operation.\n        \"\"\"\n        if left_angle is not None and not (0 &lt;= left_angle &lt;= 90):\n            return {\"error\": \"Left angle must be between 0 and 90\"}\n        if right_angle is not None and not (0 &lt;= right_angle &lt;= 90):\n            return {\"error\": \"Right angle must be between 0 and 90\"}\n        if duration_ms &lt; 100:\n            return {\"error\": \"Duration must be at least 100ms\"}\n\n        log.info(\n            \"Setting antenna state\",\n            left_angle=left_angle,\n            right_angle=right_angle,\n            wiggle=wiggle,\n        )\n\n        # Fire-and-forget: return immediately while antennas move\n        return await _fire_and_forget(\n            client.set_antenna_state(\n                left_angle=left_angle,\n                right_angle=right_angle,\n                wiggle=wiggle,\n                duration_ms=duration_ms,\n            ),\n            \"set_antenna_state\",\n        )\n\n    @mcp.tool()\n    async def get_sensor_data(\n        sensors: list[str] | None = None,\n    ) -&gt; dict[str, dict[str, float] | str]:\n        \"\"\"Get current sensor readings.\n\n        Args:\n            sensors: List of sensors to read (imu, audio_level, temperature, all).\n                    Defaults to all sensors.\n\n        Returns:\n            Sensor readings dictionary.\n        \"\"\"\n        valid_sensors = [\"imu\", \"audio_level\", \"temperature\", \"all\"]\n        if sensors is None:\n            sensors = [\"all\"]\n\n        for sensor in sensors:\n            if sensor not in valid_sensors:\n                return {\"error\": f\"Invalid sensor. Must be one of: {valid_sensors}\"}\n\n        log.info(\"Getting sensor data\", sensors=sensors)\n\n        result = await client.get_sensor_data(sensors=sensors)\n        return result\n\n    @mcp.tool()\n    async def look_at_sound(\n        timeout_ms: int = 2000,\n    ) -&gt; dict[str, str]:\n        \"\"\"Turn to face the direction of detected sound.\n\n        Uses the 4-microphone array to detect sound direction\n        and turns the head to face it.\n\n        Args:\n            timeout_ms: Maximum time to wait for sound detection.\n\n        Returns:\n            Status of the operation and detected direction.\n        \"\"\"\n        if timeout_ms &lt; 500:\n            return {\"error\": \"Timeout must be at least 500ms\"}\n\n        log.info(\"Looking at sound\", timeout_ms=timeout_ms)\n\n        result = await client.look_at_sound(timeout_ms=timeout_ms)\n        return result\n\n    @mcp.tool()\n    async def dance(\n        routine: str,\n        duration_seconds: float = 5.0,\n    ) -&gt; dict[str, str]:\n        \"\"\"Perform a choreographed dance routine.\n\n        Args:\n            routine: Name of dance routine (celebrate, greeting, thinking, custom).\n            duration_seconds: Duration of the dance (1-30 seconds).\n\n        Returns:\n            Status of the dance operation.\n        \"\"\"\n        valid_routines = [\"celebrate\", \"greeting\", \"thinking\", \"custom\"]\n\n        if routine not in valid_routines:\n            return {\"error\": f\"Invalid routine. Must be one of: {valid_routines}\"}\n        if not (1 &lt;= duration_seconds &lt;= 30):\n            return {\"error\": \"Duration must be between 1 and 30 seconds\"}\n\n        log.info(\"Dancing\", routine=routine, duration_seconds=duration_seconds)\n\n        # Fire-and-forget: return immediately while dance executes\n        return await _fire_and_forget(\n            client.dance(routine=routine, duration_seconds=duration_seconds),\n            \"dance\",\n        )\n\n    # ========== NEW TOOLS FOR FULL SDK SUPPORT ==========\n\n    @mcp.tool()\n    async def rotate(\n        direction: str,\n        degrees: float = 90.0,\n        speed: str = \"normal\",\n    ) -&gt; dict[str, str]:\n        \"\"\"Rotate Reachy's body on its 360\u00b0 base.\n\n        Args:\n            direction: Rotation direction (left, right).\n            degrees: Rotation angle in degrees (0-360).\n            speed: Rotation speed (slow, normal, fast).\n\n        Returns:\n            Status of the rotation operation.\n        \"\"\"\n        valid_directions = [\"left\", \"right\"]\n        valid_speeds = [\"slow\", \"normal\", \"fast\"]\n\n        if direction not in valid_directions:\n            return {\"error\": f\"Invalid direction. Must be one of: {valid_directions}\"}\n        if not (0 &lt;= degrees &lt;= 360):\n            return {\"error\": \"Degrees must be between 0 and 360\"}\n        if speed not in valid_speeds:\n            return {\"error\": f\"Invalid speed. Must be one of: {valid_speeds}\"}\n\n        log.info(\"Rotating body\", direction=direction, degrees=degrees, speed=speed)\n\n        # Fire-and-forget: return immediately while body rotates\n        return await _fire_and_forget(\n            client.rotate(direction=direction, degrees=degrees, speed=speed),\n            \"rotate\",\n        )\n\n    @mcp.tool()\n    async def look_at(\n        roll: float = 0.0,\n        pitch: float = 0.0,\n        yaw: float = 0.0,\n        z: float = 0.0,\n        duration: float = 1.0,\n    ) -&gt; dict[str, str]:\n        \"\"\"Position Reachy's head with precise angles.\n\n        Use this for fine-grained head control with exact positioning.\n\n        Args:\n            roll: Roll angle in degrees (-45 to 45). Tilts head side to side.\n            pitch: Pitch angle in degrees (-45 to 45). Looks up/down.\n            yaw: Yaw angle in degrees (-45 to 45). Looks left/right.\n            z: Vertical offset in mm (-50 to 50). Raises/lowers head.\n            duration: Movement duration in seconds (0.1 to 5.0).\n\n        Returns:\n            Status with final head position.\n        \"\"\"\n        if not (-45 &lt;= roll &lt;= 45):\n            return {\"error\": \"Roll must be between -45 and 45 degrees\"}\n        if not (-45 &lt;= pitch &lt;= 45):\n            return {\"error\": \"Pitch must be between -45 and 45 degrees\"}\n        if not (-45 &lt;= yaw &lt;= 45):\n            return {\"error\": \"Yaw must be between -45 and 45 degrees\"}\n        if not (-50 &lt;= z &lt;= 50):\n            return {\"error\": \"Z offset must be between -50 and 50 mm\"}\n        if not (0.1 &lt;= duration &lt;= 5.0):\n            return {\"error\": \"Duration must be between 0.1 and 5.0 seconds\"}\n\n        log.info(\n            \"Looking at position\",\n            roll=roll,\n            pitch=pitch,\n            yaw=yaw,\n            z=z,\n            duration=duration,\n        )\n\n        # Fire-and-forget: return immediately while head moves\n        return await _fire_and_forget(\n            client.look_at(roll=roll, pitch=pitch, yaw=yaw, z=z, duration=duration),\n            \"look_at\",\n        )\n\n    @mcp.tool()\n    async def listen(\n        duration_seconds: float = 3.0,\n    ) -&gt; dict[str, str]:\n        \"\"\"Capture audio from Reachy's 4-microphone array.\n\n        Records audio that can be used for speech recognition or analysis.\n\n        Args:\n            duration_seconds: Recording duration (0.5 to 10.0 seconds).\n\n        Returns:\n            Audio data as base64-encoded string with format info.\n        \"\"\"\n        if not (0.5 &lt;= duration_seconds &lt;= 10.0):\n            return {\"error\": \"Duration must be between 0.5 and 10.0 seconds\"}\n\n        log.info(\"Listening\", duration_seconds=duration_seconds)\n\n        result = await client.listen(duration_seconds=duration_seconds)\n        return result\n\n    @mcp.tool()\n    async def wake_up() -&gt; dict[str, str]:\n        \"\"\"Initialize Reachy's motors and prepare for operation.\n\n        Call this before other motor commands after the robot has been sleeping.\n        Motors will be enabled and move to a neutral position.\n\n        Returns:\n            Status of the wake up operation.\n        \"\"\"\n        log.info(\"Waking up robot\")\n\n        # Fire-and-forget: return immediately while robot wakes\n        return await _fire_and_forget(client.wake_up(), \"wake_up\")\n\n    @mcp.tool()\n    async def sleep() -&gt; dict[str, str]:\n        \"\"\"Power down Reachy's motors and enter sleep mode.\n\n        Call this when done using the robot to conserve power and\n        reduce wear. The robot will safely power down motors.\n\n        Returns:\n            Status of the sleep operation.\n        \"\"\"\n        log.info(\"Putting robot to sleep\")\n\n        # Fire-and-forget: return immediately while robot sleeps\n        return await _fire_and_forget(client.sleep(), \"sleep\")\n\n    @mcp.tool()\n    async def nod(\n        times: int = 2,\n        speed: str = \"normal\",\n    ) -&gt; dict[str, str]:\n        \"\"\"Perform a nodding gesture to express agreement or acknowledgment.\n\n        Args:\n            times: Number of nods (1 to 5).\n            speed: Nod speed (slow, normal, fast).\n\n        Returns:\n            Status of the nod gesture.\n        \"\"\"\n        valid_speeds = [\"slow\", \"normal\", \"fast\"]\n\n        if not (1 &lt;= times &lt;= 5):\n            return {\"error\": \"Times must be between 1 and 5\"}\n        if speed not in valid_speeds:\n            return {\"error\": f\"Invalid speed. Must be one of: {valid_speeds}\"}\n\n        log.info(\"Nodding\", times=times, speed=speed)\n\n        # Fire-and-forget: return immediately while nodding\n        return await _fire_and_forget(\n            client.nod(times=times, speed=speed),\n            \"nod\",\n        )\n\n    @mcp.tool()\n    async def shake(\n        times: int = 2,\n        speed: str = \"normal\",\n    ) -&gt; dict[str, str]:\n        \"\"\"Perform a head shake gesture to express disagreement or negation.\n\n        Args:\n            times: Number of shakes (1 to 5).\n            speed: Shake speed (slow, normal, fast).\n\n        Returns:\n            Status of the shake gesture.\n        \"\"\"\n        valid_speeds = [\"slow\", \"normal\", \"fast\"]\n\n        if not (1 &lt;= times &lt;= 5):\n            return {\"error\": \"Times must be between 1 and 5\"}\n        if speed not in valid_speeds:\n            return {\"error\": f\"Invalid speed. Must be one of: {valid_speeds}\"}\n\n        log.info(\"Shaking head\", times=times, speed=speed)\n\n        # Fire-and-forget: return immediately while shaking\n        return await _fire_and_forget(\n            client.shake(times=times, speed=speed),\n            \"shake\",\n        )\n\n    @mcp.tool()\n    async def rest() -&gt; dict[str, str]:\n        \"\"\"Return Reachy to a neutral resting pose.\n\n        Moves head to center, antennas to neutral, and body to forward.\n        Useful after expressions or gestures to reset to a calm state.\n\n        Returns:\n            Status of the rest operation.\n        \"\"\"\n        log.info(\"Returning to rest pose\")\n\n        # Fire-and-forget: return immediately while moving to rest\n        return await _fire_and_forget(client.rest(), \"rest\")\n\n    @mcp.tool()\n    async def get_status() -&gt; dict[str, Any]:\n        \"\"\"Get comprehensive robot status.\n\n        Returns current state of all robot systems including:\n        - Motor positions and states\n        - Head orientation (roll, pitch, yaw)\n        - Body rotation angle\n        - Antenna positions\n        - Temperature readings\n        - Whether robot is awake or sleeping\n        - Any active actions in progress\n\n        Returns:\n            Comprehensive status dictionary.\n        \"\"\"\n        log.info(\"Getting robot status\")\n        result = await client.get_status()\n        return result\n\n    @mcp.tool()\n    async def cancel_action(\n        action_id: str | None = None,\n        all_actions: bool = False,\n    ) -&gt; dict[str, str]:\n        \"\"\"Cancel running actions.\n\n        Use this to stop any movement or action currently in progress.\n        Can cancel a specific action by ID or all running actions.\n\n        Args:\n            action_id: Optional ID of specific action to cancel.\n            all_actions: If True, cancels all running actions.\n\n        Returns:\n            Status with number of cancelled actions.\n        \"\"\"\n        if not action_id and not all_actions:\n            return {\"error\": \"Must specify action_id or set all_actions=True\"}\n\n        log.info(\"Cancelling actions\", action_id=action_id, all_actions=all_actions)\n\n        # Fire-and-forget: return immediately while cancelling\n        return await _fire_and_forget(\n            client.cancel_action(action_id=action_id, all_actions=all_actions),\n            \"cancel_action\",\n        )\n\n    @mcp.tool()\n    async def get_pose() -&gt; dict[str, Any]:\n        \"\"\"Get Reachy's current physical pose (proprioceptive feedback).\n\n        Use this to know your actual position before or after movements.\n        This is helpful for:\n        - Verifying a movement completed successfully\n        - Understanding your current state before planning next action\n        - Detecting if you are at a neutral or expressive pose\n\n        Returns:\n            Dictionary containing:\n            - head: {roll, pitch, yaw} in degrees\n              - roll: Head tilt side-to-side (-45 to 45)\n              - pitch: Looking up/down (-45 to 45)\n              - yaw: Looking left/right (-45 to 45)\n            - body_yaw: Body rotation in degrees\n            - antennas: {left, right} in degrees (0=droopy, 90=straight up)\n            - timestamp: When the reading was taken\n\n        Example response:\n            {\"head\": {\"roll\": 0.0, \"pitch\": 12.2, \"yaw\": -10.5},\n             \"body_yaw\": 0.0,\n             \"antennas\": {\"left\": 80.0, \"right\": 80.0}}\n        \"\"\"\n        log.info(\"Getting current pose\")\n        result = await client.get_current_pose()\n        return result\n\n    # ========== SDK COVERAGE EXPANSION TOOLS ==========\n\n    @mcp.tool()\n    async def look_at_world(\n        x: float,\n        y: float,\n        z: float,\n        duration: float = 1.0,\n    ) -&gt; dict[str, str]:\n        \"\"\"Look at a 3D point in world coordinates.\n\n        Orients Reachy's head to gaze at a specific point in 3D space.\n        Uses inverse kinematics to compute the required head orientation.\n\n        This is useful for spatial awareness tasks like:\n        - \"Look at where the sound came from\"\n        - \"Look at the detected object\"\n        - \"Focus on a point in the room\"\n\n        Args:\n            x: X coordinate in meters. Positive = right of robot.\n            y: Y coordinate in meters. Positive = forward/in front of robot.\n            z: Z coordinate in meters. Positive = up.\n            duration: Movement duration in seconds (0.1 to 5.0).\n\n        Returns:\n            Status of the operation.\n\n        Example:\n            # Look at a point 1 meter in front and 0.5m to the left\n            look_at_world(x=-0.5, y=1.0, z=0.3)\n        \"\"\"\n        if not (0.1 &lt;= duration &lt;= 5.0):\n            return {\"error\": \"Duration must be between 0.1 and 5.0 seconds\"}\n\n        log.info(\"Looking at world coordinates\", x=x, y=y, z=z, duration=duration)\n\n        # Fire-and-forget: return immediately while head moves\n        return await _fire_and_forget(\n            client.look_at_world(x=x, y=y, z=z, duration=duration),\n            \"look_at_world\",\n        )\n\n    @mcp.tool()\n    async def look_at_pixel(\n        u: int,\n        v: int,\n        duration: float = 1.0,\n    ) -&gt; dict[str, str]:\n        \"\"\"Look at a pixel coordinate in the camera image.\n\n        Orients Reachy's head to center the camera view on a specific pixel.\n        This is extremely useful for visual tracking and attention tasks.\n\n        Common use cases:\n        - \"Look at the detected face\" (center on face bounding box)\n        - \"Center on the object of interest\"\n        - \"Track the moving target\"\n\n        Args:\n            u: Horizontal pixel coordinate (0 = left edge of image).\n            v: Vertical pixel coordinate (0 = top edge of image).\n            duration: Movement duration in seconds (0.1 to 5.0).\n\n        Returns:\n            Status of the operation.\n\n        Example:\n            # Look at the center of a 640x480 image\n            look_at_pixel(u=320, v=240)\n        \"\"\"\n        if u &lt; 0:\n            return {\"error\": \"Pixel u coordinate must be non-negative\"}\n        if v &lt; 0:\n            return {\"error\": \"Pixel v coordinate must be non-negative\"}\n        if not (0.1 &lt;= duration &lt;= 5.0):\n            return {\"error\": \"Duration must be between 0.1 and 5.0 seconds\"}\n\n        log.info(\"Looking at pixel coordinates\", u=u, v=v, duration=duration)\n\n        # Fire-and-forget: return immediately while head moves\n        return await _fire_and_forget(\n            client.look_at_pixel(u=u, v=v, duration=duration),\n            \"look_at_pixel\",\n        )\n\n    @mcp.tool()\n    async def play_recorded_move(\n        dataset: str,\n        move_name: str,\n    ) -&gt; dict[str, str]:\n        \"\"\"Play a pre-recorded move from a HuggingFace dataset.\n\n        The Reachy Mini SDK includes professionally motion-captured animations\n        stored on HuggingFace. These include the official emotions library\n        with 19+ emotion animations that include synchronized audio.\n\n        Args:\n            dataset: HuggingFace dataset name.\n                Example: \"pollen-robotics/reachy-mini-emotions-library\"\n            move_name: Name of the move within the dataset.\n                Examples: \"curious1\", \"dance1\", \"cheerful1\", \"amazed1\"\n\n        Returns:\n            Status of the operation.\n\n        Available moves in the emotions library:\n        - Emotions: curious1, confused1, cheerful1, downcast1, amazed1,\n          exhausted1, attentive1, attentive2, contempt1, boredom1, fear1,\n          anxiety1, disgusted1, displeased1, calming1, dying1, electric1,\n          enthusiastic1, enthusiastic2, come1\n        - Dances: dance1, dance2, dance3\n\n        Example:\n            # Play the curious emotion\n            play_recorded_move(\n                dataset=\"pollen-robotics/reachy-mini-emotions-library\",\n                move_name=\"curious1\"\n            )\n        \"\"\"\n        if not dataset:\n            return {\"error\": \"Dataset name is required\"}\n        if not move_name:\n            return {\"error\": \"Move name is required\"}\n\n        log.info(\"Playing recorded move\", dataset=dataset, move_name=move_name)\n\n        # Fire-and-forget: return immediately while move plays\n        return await _fire_and_forget(\n            client.play_recorded_move(dataset=dataset, move_name=move_name),\n            \"play_recorded_move\",\n        )\n\n    @mcp.tool()\n    async def set_motor_mode(\n        mode: str,\n    ) -&gt; dict[str, str]:\n        \"\"\"Set the motor control mode.\n\n        Controls motor torque and behavior. Essential for safety, teaching mode,\n        and enabling manual physical interaction with the robot.\n\n        Args:\n            mode: Motor mode to set:\n                - \"enabled\": Motors powered and holding position (normal operation).\n                  Use this for autonomous movement.\n                - \"disabled\": Motors powered off, robot can be moved freely by hand.\n                  Use for transport or when not in use.\n                - \"gravity_compensation\": Motors compensate for gravity only.\n                  Allows smooth manual positioning while preventing collapse.\n                  Ideal for teaching poses or physical interaction.\n\n        Returns:\n            Status of the operation.\n\n        Example:\n            # Enable teaching mode for manual positioning\n            set_motor_mode(mode=\"gravity_compensation\")\n\n            # Return to normal autonomous operation\n            set_motor_mode(mode=\"enabled\")\n        \"\"\"\n        valid_modes = [\"enabled\", \"disabled\", \"gravity_compensation\"]\n\n        if mode not in valid_modes:\n            return {\"error\": f\"Invalid mode. Must be one of: {valid_modes}\"}\n\n        log.info(\"Setting motor mode\", mode=mode)\n\n        # Fire-and-forget: return immediately while mode changes\n        return await _fire_and_forget(\n            client.set_motor_mode(mode=mode),\n            \"set_motor_mode\",\n        )\n\n    return mcp\n</code></pre>"},{"location":"api/mcp-servers/#daemon-client","title":"Daemon Client","text":""},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient","title":"<code>ReachyDaemonClient(base_url='http://localhost:8000', timeout=10.0)</code>","text":"<p>HTTP client for communicating with the Reachy Daemon.</p> <p>The Reachy Daemon is a FastAPI server provided by Pollen Robotics that controls the physical robot hardware. This client supports both the official reachy-mini daemon and the mock daemon for development.</p> <p>The client auto-detects the backend type on first connection and adapts API calls accordingly.</p> <p>Initialize the daemon client.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>Base URL of the Reachy daemon API.</p> <code>'http://localhost:8000'</code> <code>timeout</code> <code>float</code> <p>Request timeout in seconds.</p> <code>10.0</code> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = \"http://localhost:8000\",\n    timeout: float = 10.0,\n) -&gt; None:\n    \"\"\"Initialize the daemon client.\n\n    Args:\n        base_url: Base URL of the Reachy daemon API.\n        timeout: Request timeout in seconds.\n    \"\"\"\n    self.base_url = base_url.rstrip(\"/\")\n    self.timeout = timeout\n    self._client: httpx.AsyncClient | None = None\n    self._backend: DaemonBackend = DaemonBackend.UNKNOWN\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.move_head","title":"<code>move_head(direction, speed='normal', degrees=None)</code>  <code>async</code>","text":"<p>Move the robot's head.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>str</code> <p>Direction to look (left, right, up, down, front).</p> required <code>speed</code> <code>str</code> <p>Movement speed (slow, normal, fast).</p> <code>'normal'</code> <code>degrees</code> <code>float | None</code> <p>Optional specific angle.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Operation status.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def move_head(\n    self,\n    direction: str,\n    speed: str = \"normal\",\n    degrees: float | None = None,\n) -&gt; dict[str, str]:\n    \"\"\"Move the robot's head.\n\n    Args:\n        direction: Direction to look (left, right, up, down, front).\n        speed: Movement speed (slow, normal, fast).\n        degrees: Optional specific angle.\n\n    Returns:\n        Operation status.\n    \"\"\"\n    backend = await self.detect_backend()\n\n    if backend == DaemonBackend.REAL:\n        # Real daemon uses /api/move/set_target for smooth movements\n        # Map direction to yaw/pitch angles (in degrees, will convert to radians)\n        angle = degrees if degrees is not None else 20.0  # Default 20 degrees\n        yaw_deg = 0.0\n        pitch_deg = 0.0\n\n        direction_lower = direction.lower()\n        if direction_lower == \"left\":\n            yaw_deg = angle\n        elif direction_lower == \"right\":\n            yaw_deg = -angle\n        elif direction_lower == \"up\":\n            # reachy-mini uses negative pitch for looking up\n            pitch_deg = -angle\n        elif direction_lower == \"down\":\n            # reachy-mini uses positive pitch for looking down\n            pitch_deg = angle\n        elif direction_lower == \"front\":\n            yaw_deg = 0.0\n            pitch_deg = 0.0\n\n        log.debug(\n            \"move_head: using set_target for smooth movement\",\n            direction=direction,\n            yaw_deg=yaw_deg,\n            pitch_deg=pitch_deg,\n            yaw_rad=deg_to_rad(yaw_deg),\n            pitch_rad=deg_to_rad(pitch_deg),\n        )\n\n        # Use set_target for smooth movements (no snapping to x/y/z=0)\n        head_pose = {\n            \"roll\": 0.0,\n            \"pitch\": deg_to_rad(pitch_deg),\n            \"yaw\": deg_to_rad(yaw_deg),\n        }\n\n        try:\n            result = await self._request(\n                \"POST\", \"/api/move/set_target\", json_data={\"target_head_pose\": head_pose}\n            )\n            return {\"status\": \"success\", \"result\": str(result.get(\"status\", \"ok\"))}\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n    else:\n        # Mock daemon uses /head/move\n        data_mock: dict[str, str | float] = {\n            \"direction\": direction,\n            \"speed\": speed,\n        }\n        if degrees is not None:\n            data_mock[\"degrees\"] = degrees\n\n        try:\n            return await self._request(\"POST\", \"/head/move\", json_data=data_mock)\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.look_at","title":"<code>look_at(roll=0.0, pitch=0.0, yaw=0.0, z=0.0, duration=1.0)</code>  <code>async</code>","text":"<p>Position head with precise angles.</p> <p>Parameters:</p> Name Type Description Default <code>roll</code> <code>float</code> <p>Roll angle in degrees (-45 to 45).</p> <code>0.0</code> <code>pitch</code> <code>float</code> <p>Pitch angle in degrees (-45 to 45). Positive = look up.</p> <code>0.0</code> <code>yaw</code> <code>float</code> <p>Yaw angle in degrees (-45 to 45). Positive = look left.</p> <code>0.0</code> <code>z</code> <code>float</code> <p>Vertical offset in mm (-50 to 50). Note: z is ignored with set_target.</p> <code>0.0</code> <code>duration</code> <code>float</code> <p>Movement duration in seconds. Note: set_target uses smooth interpolation.</p> <code>1.0</code> Note <p>The user-facing convention is positive pitch = look UP. Internally, reachy-mini uses negative pitch = look UP, so we invert pitch when sending to the daemon.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Operation status.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def look_at(\n    self,\n    roll: float = 0.0,\n    pitch: float = 0.0,\n    yaw: float = 0.0,\n    z: float = 0.0,\n    duration: float = 1.0,\n) -&gt; dict[str, str]:\n    \"\"\"Position head with precise angles.\n\n    Args:\n        roll: Roll angle in degrees (-45 to 45).\n        pitch: Pitch angle in degrees (-45 to 45). Positive = look up.\n        yaw: Yaw angle in degrees (-45 to 45). Positive = look left.\n        z: Vertical offset in mm (-50 to 50). Note: z is ignored with set_target.\n        duration: Movement duration in seconds. Note: set_target uses smooth interpolation.\n\n    Note:\n        The user-facing convention is positive pitch = look UP.\n        Internally, reachy-mini uses negative pitch = look UP,\n        so we invert pitch when sending to the daemon.\n\n    Returns:\n        Operation status.\n    \"\"\"\n    backend = await self.detect_backend()\n\n    # Convert user-facing pitch convention (positive = up) to\n    # reachy-mini convention (negative = up)\n    hardware_pitch = -pitch\n\n    if backend == DaemonBackend.REAL:\n        # Real daemon uses /api/move/set_target for smooth movements\n        # MuJoCo expects angles in RADIANS, convert from degrees\n        head_pose = {\n            \"roll\": deg_to_rad(roll),\n            \"pitch\": deg_to_rad(hardware_pitch),\n            \"yaw\": deg_to_rad(yaw),\n        }\n        log.debug(\n            \"look_at: using set_target for smooth movement\",\n            roll_deg=roll,\n            pitch_deg_user=pitch,\n            pitch_deg_hardware=hardware_pitch,\n            yaw_deg=yaw,\n            roll_rad=deg_to_rad(roll),\n            pitch_rad_hardware=deg_to_rad(hardware_pitch),\n            yaw_rad=deg_to_rad(yaw),\n        )\n        try:\n            result = await self._request(\n                \"POST\", \"/api/move/set_target\", json_data={\"target_head_pose\": head_pose}\n            )\n            return {\"status\": \"success\", \"result\": str(result.get(\"status\", \"ok\"))}\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n    else:\n        # Mock daemon uses /head/look_at\n        data = {\n            \"roll\": roll,\n            \"pitch\": pitch,\n            \"yaw\": yaw,\n            \"z\": z,\n            \"duration\": duration,\n        }\n        try:\n            return await self._request(\"POST\", \"/head/look_at\", json_data=data)\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.play_emotion","title":"<code>play_emotion(emotion, intensity=0.7)</code>  <code>async</code>","text":"<p>Play an emotional expression.</p> <p>Priority order: 1. Local bundled emotions (data/emotions/) - fastest, no network 2. HuggingFace SDK emotions - fallback if local fails 3. Custom EMOTION_MAPPINGS - for emotions not in the SDK</p> <p>Native emotions include synchronized audio + motion, providing higher quality animations tuned by Pollen Robotics.</p> <p>Parameters:</p> Name Type Description Default <code>emotion</code> <code>str</code> <p>Emotion to express (e.g., \"curious\", \"happy\", \"thinking\").</p> required <code>intensity</code> <code>float</code> <p>Expression intensity (0.0 to 1.0). Note: intensity is only applied for custom emotions, not native SDK ones.</p> <code>0.7</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Operation status.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def play_emotion(\n    self,\n    emotion: str,\n    intensity: float = 0.7,\n) -&gt; dict[str, str]:\n    \"\"\"Play an emotional expression.\n\n    Priority order:\n    1. Local bundled emotions (data/emotions/) - fastest, no network\n    2. HuggingFace SDK emotions - fallback if local fails\n    3. Custom EMOTION_MAPPINGS - for emotions not in the SDK\n\n    Native emotions include synchronized audio + motion, providing\n    higher quality animations tuned by Pollen Robotics.\n\n    Args:\n        emotion: Emotion to express (e.g., \"curious\", \"happy\", \"thinking\").\n        intensity: Expression intensity (0.0 to 1.0). Note: intensity\n            is only applied for custom emotions, not native SDK ones.\n\n    Returns:\n        Operation status.\n    \"\"\"\n    backend = await self.detect_backend()\n    emotion_lower = emotion.lower()\n\n    if backend == DaemonBackend.REAL:\n        # Check for native SDK emotion mapping\n        native_move = self.NATIVE_EMOTION_MAPPING.get(emotion_lower)\n        if native_move:\n            # Try local bundled emotion first (fastest)\n            loader = get_emotion_loader()\n            if loader.has_emotion(native_move):\n                log.info(\n                    \"Using local bundled emotion\",\n                    emotion=emotion,\n                    native_move=native_move,\n                )\n                result = await self.play_local_emotion(native_move, loader)\n                if result.get(\"status\") == \"success\":\n                    return result\n                log.warning(\n                    \"Local emotion playback failed, trying HuggingFace\",\n                    emotion=emotion,\n                    error=result.get(\"message\"),\n                )\n\n            # Fall back to HuggingFace SDK emotion\n            log.info(\n                \"Using HuggingFace SDK emotion\",\n                emotion=emotion,\n                native_move=native_move,\n            )\n            result = await self.play_recorded_move(self.EMOTIONS_DATASET, native_move)\n            if result.get(\"status\") == \"success\":\n                return result\n            # If HuggingFace also fails, fall back to custom composition\n            log.warning(\n                \"HuggingFace emotion failed, falling back to custom\",\n                emotion=emotion,\n                error=result.get(\"message\"),\n            )\n\n        # Fall back to custom EMOTION_MAPPINGS\n        # These are used for emotions not in the SDK: thinking, neutral, agreeing, disagreeing, etc.\n        emotion_data = self.EMOTION_MAPPINGS.get(emotion_lower)\n\n        if emotion_data is None:\n            log.warning(f\"Unknown emotion '{emotion}', defaulting to neutral\")\n            emotion_data = self.EMOTION_MAPPINGS[\"neutral\"]\n\n        head = emotion_data[\"head\"]\n        antennas = emotion_data[\"antennas\"]\n        duration_ms = emotion_data.get(\"duration_ms\", 1000)\n\n        # Apply intensity scaling to head movements\n        pitch_deg = head[\"pitch\"] * intensity\n        yaw_deg = head[\"yaw\"] * intensity\n        roll_deg = head[\"roll\"] * intensity\n\n        # Invert pitch: EMOTION_MAPPINGS uses positive = look up,\n        # but reachy-mini uses negative = look up\n        hardware_pitch_deg = -pitch_deg\n\n        # Convert antenna angles from our convention to daemon convention\n        # Our convention: 0\u00b0 = flat/back, 90\u00b0 = vertical (straight up)\n        # Daemon convention: 0 rad = vertical, \u03c0/2 rad = flat/back\n        def to_daemon_radians(deg: float) -&gt; float:\n            return deg_to_rad(90.0 - deg)\n\n        left_antenna_rad = to_daemon_radians(antennas[\"left\"])\n        right_antenna_rad = to_daemon_radians(antennas[\"right\"])\n\n        # Use set_target for smooth custom emotion (no snapping)\n        data = {\n            \"target_head_pose\": {\n                \"roll\": deg_to_rad(roll_deg),\n                \"pitch\": deg_to_rad(hardware_pitch_deg),\n                \"yaw\": deg_to_rad(yaw_deg),\n            },\n            \"target_antennas\": [left_antenna_rad, right_antenna_rad],\n        }\n        log.debug(\n            \"play_emotion: using custom composition with set_target\",\n            emotion=emotion,\n            user_pitch_deg=pitch_deg,\n            hardware_pitch_deg=hardware_pitch_deg,\n            yaw_deg=yaw_deg,\n            roll_deg=roll_deg,\n            left_antenna=antennas[\"left\"],\n            right_antenna=antennas[\"right\"],\n        )\n        try:\n            result = await self._request(\"POST\", \"/api/move/set_target\", json_data=data)\n            return {\"status\": \"success\", \"result\": str(result.get(\"status\", \"ok\"))}\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n    else:\n        # Mock daemon uses /expression/emotion\n        data = {\n            \"emotion\": emotion,\n            \"intensity\": intensity,\n        }\n        try:\n            return await self._request(\"POST\", \"/expression/emotion\", json_data=data)\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.speak","title":"<code>speak(text, voice='default', speed=1.0)</code>  <code>async</code>","text":"<p>Speak text through the robot's speaker.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to speak.</p> required <code>voice</code> <code>str</code> <p>Voice profile.</p> <code>'default'</code> <code>speed</code> <code>float</code> <p>Speech speed.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Operation status.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def speak(\n    self,\n    text: str,\n    voice: str = \"default\",\n    speed: float = 1.0,\n) -&gt; dict[str, str]:\n    \"\"\"Speak text through the robot's speaker.\n\n    Args:\n        text: Text to speak.\n        voice: Voice profile.\n        speed: Speech speed.\n\n    Returns:\n        Operation status.\n    \"\"\"\n    data = {\n        \"text\": text,\n        \"voice\": voice,\n        \"speed\": speed,\n    }\n\n    try:\n        return await self._request(\"POST\", \"/audio/speak\", json_data=data)\n    except ReachyDaemonError as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.nod","title":"<code>nod(times=2, speed='normal')</code>  <code>async</code>","text":"<p>Perform nodding gesture (agreement).</p> <p>Parameters:</p> Name Type Description Default <code>times</code> <code>int</code> <p>Number of nods.</p> <code>2</code> <code>speed</code> <code>str</code> <p>Nod speed (slow, normal, fast).</p> <code>'normal'</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Operation status.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def nod(\n    self,\n    times: int = 2,\n    speed: str = \"normal\",\n) -&gt; dict[str, str]:\n    \"\"\"Perform nodding gesture (agreement).\n\n    Args:\n        times: Number of nods.\n        speed: Nod speed (slow, normal, fast).\n\n    Returns:\n        Operation status.\n    \"\"\"\n    backend = await self.detect_backend()\n\n    if backend == DaemonBackend.REAL:\n        # Real daemon: implement nod using set_target for smooth movement\n        # Nod = pitch up, then down, back to center\n        speed_delays = {\"slow\": 0.4, \"normal\": 0.25, \"fast\": 0.15}\n        delay = speed_delays.get(speed, 0.25)\n        pitch_angle = 0.3  # ~17 degrees in radians\n\n        log.debug(\"nod: using set_target for smooth gesture\", times=times, speed=speed)\n        try:\n            for _ in range(times):\n                # Pitch down (look down = negative pitch for hardware)\n                await self._request(\n                    \"POST\",\n                    \"/api/move/set_target\",\n                    json_data={\"target_head_pose\": {\"pitch\": pitch_angle}},\n                )\n                await asyncio.sleep(delay)\n                # Pitch up (look up)\n                await self._request(\n                    \"POST\",\n                    \"/api/move/set_target\",\n                    json_data={\"target_head_pose\": {\"pitch\": -pitch_angle}},\n                )\n                await asyncio.sleep(delay)\n            # Return to center\n            await self._request(\n                \"POST\",\n                \"/api/move/set_target\",\n                json_data={\"target_head_pose\": {\"pitch\": 0}},\n            )\n            return {\"status\": \"success\", \"gesture\": \"nod\", \"times\": str(times)}\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n    else:\n        # Mock daemon uses /gesture/nod\n        data = {\"times\": times, \"speed\": speed}\n        try:\n            return await self._request(\"POST\", \"/gesture/nod\", json_data=data)\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.shake","title":"<code>shake(times=2, speed='normal')</code>  <code>async</code>","text":"<p>Perform head shake gesture (disagreement).</p> <p>Parameters:</p> Name Type Description Default <code>times</code> <code>int</code> <p>Number of shakes.</p> <code>2</code> <code>speed</code> <code>str</code> <p>Shake speed (slow, normal, fast).</p> <code>'normal'</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Operation status.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def shake(\n    self,\n    times: int = 2,\n    speed: str = \"normal\",\n) -&gt; dict[str, str]:\n    \"\"\"Perform head shake gesture (disagreement).\n\n    Args:\n        times: Number of shakes.\n        speed: Shake speed (slow, normal, fast).\n\n    Returns:\n        Operation status.\n    \"\"\"\n    backend = await self.detect_backend()\n\n    if backend == DaemonBackend.REAL:\n        # Real daemon: implement shake using set_target for smooth movement\n        # Shake = yaw left, then right, back to center\n        speed_delays = {\"slow\": 0.35, \"normal\": 0.2, \"fast\": 0.12}\n        delay = speed_delays.get(speed, 0.2)\n        yaw_angle = 0.35  # ~20 degrees in radians\n\n        log.debug(\"shake: using set_target for smooth gesture\", times=times, speed=speed)\n        try:\n            for _ in range(times):\n                # Turn left (positive yaw)\n                await self._request(\n                    \"POST\",\n                    \"/api/move/set_target\",\n                    json_data={\"target_head_pose\": {\"yaw\": yaw_angle}},\n                )\n                await asyncio.sleep(delay)\n                # Turn right (negative yaw)\n                await self._request(\n                    \"POST\",\n                    \"/api/move/set_target\",\n                    json_data={\"target_head_pose\": {\"yaw\": -yaw_angle}},\n                )\n                await asyncio.sleep(delay)\n            # Return to center\n            await self._request(\n                \"POST\",\n                \"/api/move/set_target\",\n                json_data={\"target_head_pose\": {\"yaw\": 0}},\n            )\n            return {\"status\": \"success\", \"gesture\": \"shake\", \"times\": str(times)}\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n    else:\n        # Mock daemon uses /gesture/shake\n        data = {\"times\": times, \"speed\": speed}\n        try:\n            return await self._request(\"POST\", \"/gesture/shake\", json_data=data)\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.wake_up","title":"<code>wake_up()</code>  <code>async</code>","text":"<p>Initialize robot motors and prepare for operation.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Operation status.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def wake_up(self) -&gt; dict[str, str]:\n    \"\"\"Initialize robot motors and prepare for operation.\n\n    Returns:\n        Operation status.\n    \"\"\"\n    backend = await self.detect_backend()\n\n    if backend == DaemonBackend.REAL:\n        # Real daemon uses /api/move/play/wake_up\n        try:\n            result = await self._request(\"POST\", \"/api/move/play/wake_up\")\n            return {\"status\": \"success\", \"uuid\": str(result.get(\"uuid\", \"\"))}\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n    else:\n        # Mock daemon uses /lifecycle/wake_up\n        try:\n            return await self._request(\"POST\", \"/lifecycle/wake_up\")\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.sleep","title":"<code>sleep()</code>  <code>async</code>","text":"<p>Power down motors and enter sleep mode.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Operation status.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def sleep(self) -&gt; dict[str, str]:\n    \"\"\"Power down motors and enter sleep mode.\n\n    Returns:\n        Operation status.\n    \"\"\"\n    backend = await self.detect_backend()\n\n    if backend == DaemonBackend.REAL:\n        # Real daemon uses /api/move/play/goto_sleep\n        try:\n            result = await self._request(\"POST\", \"/api/move/play/goto_sleep\")\n            return {\"status\": \"success\", \"uuid\": str(result.get(\"uuid\", \"\"))}\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n    else:\n        # Mock daemon uses /lifecycle/sleep\n        try:\n            return await self._request(\"POST\", \"/lifecycle/sleep\")\n        except ReachyDaemonError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.rest","title":"<code>rest()</code>  <code>async</code>","text":"<p>Return to neutral resting pose.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Operation status.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def rest(self) -&gt; dict[str, str]:\n    \"\"\"Return to neutral resting pose.\n\n    Returns:\n        Operation status.\n    \"\"\"\n    try:\n        return await self._request(\"POST\", \"/gesture/rest\")\n    except ReachyDaemonError as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.get_status","title":"<code>get_status()</code>  <code>async</code>","text":"<p>Get comprehensive robot status.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Robot status including position, motor states, temperature, etc.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def get_status(self) -&gt; dict[str, Any]:\n    \"\"\"Get comprehensive robot status.\n\n    Returns:\n        Robot status including position, motor states, temperature, etc.\n    \"\"\"\n    try:\n        return await self._request(\"GET\", \"/api/daemon/status\")\n    except ReachyDaemonError as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n</code></pre>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_client.ReachyDaemonClient.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the HTTP client.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the HTTP client.\"\"\"\n    if self._client is not None and not self._client.is_closed:\n        await self._client.aclose()\n        self._client = None\n</code></pre>"},{"location":"api/mcp-servers/#mock-daemon","title":"Mock Daemon","text":""},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_mock","title":"<code>daemon_mock</code>","text":"<p>Mock Reachy Daemon for development and testing.</p> <p>Provides a FastAPI server that mimics the Reachy Daemon API, allowing development without physical hardware.</p>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.reachy.daemon_mock.create_mock_daemon_app","title":"<code>create_mock_daemon_app()</code>","text":"<p>Create the mock daemon FastAPI application.</p> <p>Returns:</p> Type Description <code>Any</code> <p>FastAPI application instance.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If FastAPI is not installed.</p> Source code in <code>src/reachy_agent/mcp_servers/reachy/daemon_mock.py</code> <pre><code>def create_mock_daemon_app() -&gt; Any:\n    \"\"\"Create the mock daemon FastAPI application.\n\n    Returns:\n        FastAPI application instance.\n\n    Raises:\n        ImportError: If FastAPI is not installed.\n    \"\"\"\n    if not FASTAPI_AVAILABLE:\n        raise ImportError(\n            \"FastAPI is required for the mock daemon. \"\n            \"Install with: pip install fastapi uvicorn\"\n        )\n\n    @asynccontextmanager\n    async def lifespan(_app: FastAPI):\n        \"\"\"Reset state on startup.\"\"\"\n        global _mock_state\n        _mock_state = MockDaemonState()\n        yield\n\n    app = FastAPI(\n        title=\"Reachy Mock Daemon\",\n        description=\"Mock daemon for development without hardware\",\n        version=\"0.1.0\",\n        lifespan=lifespan,\n    )\n\n    # Add CORS middleware for cross-origin video feed requests\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    def generate_test_frame() -&gt; bytes:\n        \"\"\"Generate an enhanced test frame image for video streaming.\n\n        Creates a detailed robot visualization with:\n        - Body and neck below head\n        - Expressive face with emotion-based eyes/mouth\n        - Animated antennas with tips\n        - Activity indicators (speaking waves, dancing motion)\n        - Comprehensive status overlay panel\n        - Blink animation\n\n        Falls back to a minimal JPEG if PIL is not available.\n        \"\"\"\n        if PIL_AVAILABLE:\n            # Dashboard color scheme\n            BG_PRIMARY = (26, 26, 46)       # #1a1a2e\n            BG_SECONDARY = (22, 33, 62)     # #16213e\n            BG_CARD = (15, 52, 96)          # #0f3460\n            TEXT_PRIMARY = (234, 234, 234)  # #eaeaea\n            TEXT_SECONDARY = (160, 160, 160)  # #a0a0a0\n            ACCENT_CYAN = (0, 217, 255)     # #00d9ff\n            ACCENT_GREEN = (0, 255, 136)    # #00ff88\n            ACCENT_YELLOW = (255, 204, 0)   # #ffcc00\n            ACCENT_RED = (255, 71, 87)      # #ff4757\n            BORDER_COLOR = (45, 58, 74)     # #2d3a4a\n\n            # Create 800x600 image (increased resolution)\n            WIDTH, HEIGHT = 800, 600\n            img = Image.new(\"RGB\", (WIDTH, HEIGHT), color=BG_PRIMARY)\n            draw = ImageDraw.Draw(img)\n\n            # Update frame counter and handle blink animation\n            _mock_state.frame_counter += 1\n            current_time = time.time()\n\n            # Blink every 3-5 seconds for 0.15 seconds\n            if current_time - _mock_state.last_blink_time &gt; random.uniform(3, 5):\n                _mock_state.last_blink_time = current_time\n                _mock_state.blink_duration = 0.15\n\n            is_blinking = (current_time - _mock_state.last_blink_time) &lt; _mock_state.blink_duration\n\n            # Robot center position (moved up to make room for status panel)\n            center_x, center_y = WIDTH // 2, 220\n\n            # ===== BODY (no neck - small gap between head and body) =====\n            # Body is more vertically rectangular\n            body_width, body_height = 80, 110  # Taller than wide\n            body_y = center_y + 50  # Position with small gap from head (head bottom is at center_y + 35)\n            rotation_offset = int(_mock_state.body_rotation / 5)  # Visual hint of rotation\n\n            # Draw body shadow/depth\n            draw.rounded_rectangle(\n                [center_x - body_width // 2 + 3, body_y + 3,\n                 center_x + body_width // 2 + 3, body_y + body_height + 3],\n                radius=20,\n                fill=(10, 30, 50),\n            )\n            # Draw body\n            draw.rounded_rectangle(\n                [center_x - body_width // 2, body_y,\n                 center_x + body_width // 2, body_y + body_height],\n                radius=20,\n                fill=BG_CARD,\n                outline=ACCENT_CYAN,\n                width=2,\n            )\n\n            # Rotation indicator on body\n            if abs(_mock_state.body_rotation) &gt; 5:\n                rot_text = f\"{_mock_state.body_rotation:.0f}\u00b0\"\n                draw.text((center_x - 15, body_y + 45), rot_text, fill=ACCENT_YELLOW)\n\n            # NOTE: No neck - small gap left between head and body for Reachy Mini style\n\n            # ===== HEAD =====\n            # Apply head position offsets for visualization\n            yaw = _mock_state.head_position.get(\"yaw\", 0)\n            pitch = _mock_state.head_position.get(\"pitch\", 0)\n            roll = _mock_state.head_position.get(\"roll\", 0)\n\n            head_offset_x = int(yaw / 4)\n            head_offset_y = int(pitch / 4)\n\n            head_x = center_x + head_offset_x\n            head_y = center_y + head_offset_y\n\n            # Head shape - rounded rectangle matching body style\n            # Wider than tall, same corner radius style as body\n            head_width = 120   # Wide\n            head_height = 70   # Not too tall\n            head_radius_corner = 20  # Same corner radius as body\n\n            # Head shadow\n            draw.rounded_rectangle(\n                [head_x - head_width // 2 + 4, head_y - head_height // 2 + 4,\n                 head_x + head_width // 2 + 4, head_y + head_height // 2 + 4],\n                radius=head_radius_corner,\n                fill=(10, 30, 50),\n            )\n\n            # Head fill - matching body style\n            draw.rounded_rectangle(\n                [head_x - head_width // 2, head_y - head_height // 2,\n                 head_x + head_width // 2, head_y + head_height // 2],\n                fill=BG_CARD,\n                outline=ACCENT_CYAN,\n                width=2,\n            )\n\n            # Inner head highlight (top portion - subtle visor effect)\n            draw.rounded_rectangle(\n                [head_x - head_width // 2 + 8, head_y - head_height // 2 + 6,\n                 head_x + head_width // 2 - 8, head_y - 5],\n                radius=15,\n                fill=(20, 60, 100),\n            )\n\n            # ===== EYES (Reachy Mini goggle-style) =====\n            # Reachy has large circular goggle eyes with BLACK frames and dark lenses\n            # No mouth - expression is purely through eyes and antennas\n            eye_base_y = head_y\n            eye_spacing = 28  # Distance from center to each eye\n            eye_outer_radius = 24   # Black goggle frame (outer)\n            eye_inner_radius = 20   # Dark lens inside\n\n            # Eye position shifts with head movement\n            eye_shift_x = int(yaw / 6)\n            eye_shift_y = int(pitch / 6)\n\n            # Emotion affects eye appearance\n            emotion = _mock_state.current_emotion or \"neutral\"\n\n            # Draw both goggle eyes\n            for side in [-1, 1]:  # Left and right\n                ex = head_x + (side * eye_spacing) + eye_shift_x\n                ey = eye_base_y + eye_shift_y\n\n                if is_blinking:\n                    # Closed eyes - horizontal lines (black frames still visible)\n                    draw.ellipse(\n                        [ex - eye_outer_radius, ey - 4,\n                         ex + eye_outer_radius, ey + 4],\n                        fill=(20, 20, 30),  # Dark frame\n                        outline=ACCENT_CYAN,\n                        width=1,\n                    )\n                else:\n                    # Black goggle frame (like real Reachy)\n                    draw.ellipse(\n                        [ex - eye_outer_radius, ey - eye_outer_radius,\n                         ex + eye_outer_radius, ey + eye_outer_radius],\n                        fill=(20, 20, 30),  # Very dark frame\n                        outline=ACCENT_CYAN,\n                        width=2,\n                    )\n\n                    # Dark lens inside\n                    draw.ellipse(\n                        [ex - eye_inner_radius, ey - eye_inner_radius,\n                         ex + eye_inner_radius, ey + eye_inner_radius],\n                        fill=(8, 10, 18),  # Nearly black lens\n                    )\n\n                    # Lens reflection/highlight (subtle)\n                    highlight_x = ex - 6\n                    highlight_y = ey - 6\n                    draw.ellipse(\n                        [highlight_x - 4, highlight_y - 4,\n                         highlight_x + 4, highlight_y + 4],\n                        fill=(40, 50, 70),  # Subtle reflection\n                    )\n\n                    # Emotion-based eye glow/details inside lens\n                    if emotion == \"happy\":\n                        # Happy: upward arc like ^_^\n                        draw.arc(\n                            [ex - 12, ey - 8, ex + 12, ey + 12],\n                            start=200, end=340,\n                            fill=ACCENT_CYAN,\n                            width=2,\n                        )\n                    elif emotion == \"sad\":\n                        # Sad: downward droopy arc\n                        draw.arc(\n                            [ex - 12, ey - 4, ex + 12, ey + 14],\n                            start=20, end=160,\n                            fill=ACCENT_CYAN,\n                            width=2,\n                        )\n                    elif emotion == \"surprised\":\n                        # Surprised: bright ring inside\n                        draw.ellipse(\n                            [ex - 10, ey - 10, ex + 10, ey + 10],\n                            outline=ACCENT_CYAN,\n                            width=2,\n                        )\n                    elif emotion == \"angry\":\n                        # Angry: diagonal slash\n                        slant = -1 if side == -1 else 1\n                        draw.line(\n                            [(ex - 10, ey - 8 * slant),\n                             (ex + 10, ey + 8 * slant)],\n                            fill=ACCENT_RED,\n                            width=3,\n                        )\n                    elif emotion == \"curious\":\n                        # Curious: asymmetric - one brighter\n                        brightness = 10 if side == 1 else 6\n                        draw.ellipse(\n                            [ex - brightness, ey - brightness,\n                             ex + brightness, ey + brightness],\n                            outline=ACCENT_CYAN,\n                            width=2,\n                        )\n                    # Neutral: just the dark lens with highlight (no extra glow)\n\n            # Camera/sensor bar between eyes (like real Reachy - small dark strip)\n            sensor_width = 16\n            sensor_height = 5\n            draw.rounded_rectangle(\n                [head_x - sensor_width // 2, eye_base_y - sensor_height // 2 - 2,\n                 head_x + sensor_width // 2, eye_base_y + sensor_height // 2 - 2],\n                radius=2,\n                fill=(15, 15, 25),\n                outline=(40, 50, 70),\n                width=1,\n            )\n\n            # NOTE: Reachy Mini has NO mouth - expression is through eyes and antennas only\n\n            # ===== ANTENNAS (Reachy Mini style - thin wire with spring coil base) =====\n            left_ant = int(_mock_state.left_antenna_angle)\n            right_ant = int(_mock_state.right_antenna_angle)\n\n            # Antenna base points (on top of pill-shaped head)\n            ant_base_y = head_y - head_height // 2 - 5\n\n            for side, angle in [(-1, left_ant), (1, right_ant)]:\n                base_x = head_x + (side * 35)  # Closer together like real Reachy\n\n                # Calculate antenna tip position using angle\n                ant_length = 60  # Taller antennas\n                tip_x = base_x + int(side * 20 * math.cos(math.radians(90 - angle)))\n                tip_y = ant_base_y - int(ant_length * math.sin(math.radians(angle)))\n\n                # Spring coil at base (like real Reachy)\n                coil_height = 12\n                coil_segments = 4\n                for i in range(coil_segments):\n                    coil_y = ant_base_y - (i * coil_height // coil_segments)\n                    coil_offset = 4 * (1 if i % 2 == 0 else -1)\n                    draw.ellipse(\n                        [base_x + coil_offset - 3, coil_y - 2,\n                         base_x + coil_offset + 3, coil_y + 2],\n                        outline=ACCENT_GREEN,\n                        width=1,\n                    )\n\n                # Thin wire antenna line (thinner than before)\n                wire_start_y = ant_base_y - coil_height\n                draw.line(\n                    [(base_x, wire_start_y), (tip_x, tip_y)],\n                    fill=ACCENT_GREEN,\n                    width=2,  # Thinner wire\n                )\n\n                # Antenna tip (small ball)\n                draw.ellipse(\n                    [tip_x - 5, tip_y - 5, tip_x + 5, tip_y + 5],\n                    fill=ACCENT_GREEN,\n                )\n\n                # Antenna glow when awake\n                if _mock_state.is_awake:\n                    pulse = abs(math.sin(_mock_state.frame_counter * 0.1)) * 0.5 + 0.5\n                    glow_size = int(7 + pulse * 3)\n                    glow_color = (0, int(255 * pulse), int(136 * pulse))\n                    draw.ellipse(\n                        [tip_x - glow_size, tip_y - glow_size,\n                         tip_x + glow_size, tip_y + glow_size],\n                        outline=glow_color,\n                        width=2,\n                    )\n\n            # ===== ACTIVITY INDICATORS =====\n            # Speaking: sound waves near head (no mouth, so waves come from side)\n            if _mock_state.is_speaking:\n                wave_x = head_x + head_width // 2 + 15\n                wave_y = head_y + 10  # Near center of head\n                for i in range(3):\n                    offset = (_mock_state.frame_counter + i * 5) % 20\n                    alpha = 1.0 - (offset / 20)\n                    wave_color = (int(255 * alpha), int(204 * alpha), 0)\n                    draw.arc(\n                        [wave_x + offset, wave_y - 10 - offset,\n                         wave_x + offset + 15, wave_y + 10 + offset],\n                        start=-60, end=60,\n                        fill=wave_color,\n                        width=2,\n                    )\n\n            # Dancing: motion lines around body\n            if _mock_state.is_dancing:\n                dance_phase = _mock_state.frame_counter % 20\n                for i in range(4):\n                    angle = (i * 90 + dance_phase * 18) % 360\n                    mx = center_x + int(80 * math.cos(math.radians(angle)))\n                    my = body_y + 40 + int(30 * math.sin(math.radians(angle)))\n                    line_len = 15\n                    draw.line(\n                        [(mx, my), (mx + int(line_len * math.cos(math.radians(angle))),\n                                    my + int(line_len * math.sin(math.radians(angle))))],\n                        fill=ACCENT_YELLOW,\n                        width=3,\n                    )\n\n            # Sleeping: Zzz\n            if not _mock_state.is_awake:\n                z_x = head_x + head_width // 2 + 10\n                z_y = head_y - head_height // 2\n                for i, size in enumerate([12, 16, 20]):\n                    offset = (_mock_state.frame_counter // 10 + i) % 3\n                    draw.text(\n                        (z_x + i * 20, z_y - i * 15 - offset * 3),\n                        \"Z\",\n                        fill=TEXT_SECONDARY,\n                    )\n\n            # ===== STATUS OVERLAY PANEL =====\n            panel_height = 100\n            panel_y = HEIGHT - panel_height - 10\n            panel_margin = 20\n\n            # Panel background with border\n            draw.rounded_rectangle(\n                [panel_margin, panel_y,\n                 WIDTH - panel_margin, HEIGHT - 10],\n                radius=10,\n                fill=BG_SECONDARY,\n                outline=BORDER_COLOR,\n                width=2,\n            )\n\n            # Status text layout\n            timestamp = time.strftime(\"%H:%M:%S\")\n            line_y = panel_y + 12\n            line_height = 18\n\n            # Row 1: Title and timestamp\n            draw.text((panel_margin + 15, line_y), \"REACHY MOCK DAEMON\", fill=ACCENT_CYAN)\n            draw.text((WIDTH - panel_margin - 80, line_y), timestamp, fill=TEXT_SECONDARY)\n            line_y += line_height + 5\n\n            # Separator line\n            draw.line(\n                [(panel_margin + 10, line_y), (WIDTH - panel_margin - 10, line_y)],\n                fill=BORDER_COLOR,\n                width=1,\n            )\n            line_y += 8\n\n            # Row 2: Mode and status\n            mode_text = \"Mode: Simulator\"\n            status_text = \"AWAKE\" if _mock_state.is_awake else \"SLEEPING\"\n            status_color = ACCENT_GREEN if _mock_state.is_awake else TEXT_SECONDARY\n            draw.text((panel_margin + 15, line_y), mode_text, fill=TEXT_PRIMARY)\n            draw.text((panel_margin + 180, line_y), f\"Status: \", fill=TEXT_SECONDARY)\n            draw.text((panel_margin + 245, line_y), status_text, fill=status_color)\n            line_y += line_height\n\n            # Row 3: Head position and body rotation\n            roll = _mock_state.head_position.get('roll', 0)\n            pitch = _mock_state.head_position.get('pitch', 0)\n            yaw = _mock_state.head_position.get('yaw', 0)\n            head_text = f\"Head: R:{roll:.1f} P:{pitch:.1f} Y:{yaw:.1f}\"\n            body_text = f\"Body: {_mock_state.body_rotation:.1f}\u00b0\"\n            draw.text((panel_margin + 15, line_y), head_text, fill=TEXT_PRIMARY)\n            draw.text((panel_margin + 280, line_y), body_text, fill=TEXT_PRIMARY)\n            line_y += line_height\n\n            # Row 4: Antennas and emotion\n            ant_text = f\"Antennas: L:{_mock_state.left_antenna_angle:.0f}\u00b0 R:{_mock_state.right_antenna_angle:.0f}\u00b0\"\n            draw.text((panel_margin + 15, line_y), ant_text, fill=TEXT_PRIMARY)\n\n            if _mock_state.current_emotion:\n                draw.text((panel_margin + 280, line_y), f\"Emotion: {_mock_state.current_emotion}\", fill=ACCENT_GREEN)\n\n            # Activity badges on right side\n            badge_x = WIDTH - panel_margin - 100\n            badge_y = panel_y + 45\n            if _mock_state.is_speaking:\n                draw.rounded_rectangle(\n                    [badge_x, badge_y, badge_x + 80, badge_y + 22],\n                    radius=11,\n                    fill=ACCENT_YELLOW,\n                )\n                draw.text((badge_x + 8, badge_y + 3), \"Speaking\", fill=BG_PRIMARY)\n            elif _mock_state.is_dancing:\n                draw.rounded_rectangle(\n                    [badge_x, badge_y, badge_x + 80, badge_y + 22],\n                    radius=11,\n                    fill=ACCENT_YELLOW,\n                )\n                draw.text((badge_x + 12, badge_y + 3), \"Dancing\", fill=BG_PRIMARY)\n            elif _mock_state.is_listening:\n                draw.rounded_rectangle(\n                    [badge_x, badge_y, badge_x + 80, badge_y + 22],\n                    radius=11,\n                    fill=ACCENT_CYAN,\n                )\n                draw.text((badge_x + 10, badge_y + 3), \"Listening\", fill=BG_PRIMARY)\n\n            # ===== STATUS INDICATOR DOT =====\n            # Top right corner\n            dot_x, dot_y = WIDTH - 35, 25\n            if _mock_state.is_awake:\n                dot_color = ACCENT_GREEN\n            else:\n                dot_color = TEXT_SECONDARY\n\n            # Pulsing effect when active\n            if _mock_state.is_awake and (_mock_state.is_speaking or _mock_state.is_dancing):\n                pulse = abs(math.sin(_mock_state.frame_counter * 0.15))\n                dot_size = int(8 + pulse * 4)\n            else:\n                dot_size = 8\n\n            draw.ellipse(\n                [dot_x - dot_size, dot_y - dot_size,\n                 dot_x + dot_size, dot_y + dot_size],\n                fill=dot_color,\n            )\n\n            # Convert to JPEG\n            buffer = io.BytesIO()\n            img.save(buffer, format=\"JPEG\", quality=85)\n            return buffer.getvalue()\n\n        else:\n            # Minimal 1x1 JPEG if PIL not available\n            # This is a valid minimal JPEG\n            return bytes([\n                0xFF, 0xD8, 0xFF, 0xE0, 0x00, 0x10, 0x4A, 0x46, 0x49, 0x46, 0x00, 0x01,\n                0x01, 0x00, 0x00, 0x01, 0x00, 0x01, 0x00, 0x00, 0xFF, 0xDB, 0x00, 0x43,\n                0x00, 0x08, 0x06, 0x06, 0x07, 0x06, 0x05, 0x08, 0x07, 0x07, 0x07, 0x09,\n                0x09, 0x08, 0x0A, 0x0C, 0x14, 0x0D, 0x0C, 0x0B, 0x0B, 0x0C, 0x19, 0x12,\n                0x13, 0x0F, 0x14, 0x1D, 0x1A, 0x1F, 0x1E, 0x1D, 0x1A, 0x1C, 0x1C, 0x20,\n                0x24, 0x2E, 0x27, 0x20, 0x22, 0x2C, 0x23, 0x1C, 0x1C, 0x28, 0x37, 0x29,\n                0x2C, 0x30, 0x31, 0x34, 0x34, 0x34, 0x1F, 0x27, 0x39, 0x3D, 0x38, 0x32,\n                0x3C, 0x2E, 0x33, 0x34, 0x32, 0xFF, 0xC0, 0x00, 0x0B, 0x08, 0x00, 0x01,\n                0x00, 0x01, 0x01, 0x01, 0x11, 0x00, 0xFF, 0xC4, 0x00, 0x1F, 0x00, 0x00,\n                0x01, 0x05, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x00, 0x00, 0x00, 0x00,\n                0x00, 0x00, 0x00, 0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08,\n                0x09, 0x0A, 0x0B, 0xFF, 0xC4, 0x00, 0xB5, 0x10, 0x00, 0x02, 0x01, 0x03,\n                0x03, 0x02, 0x04, 0x03, 0x05, 0x05, 0x04, 0x04, 0x00, 0x00, 0x01, 0x7D,\n                0x01, 0x02, 0x03, 0x00, 0x04, 0x11, 0x05, 0x12, 0x21, 0x31, 0x41, 0x06,\n                0x13, 0x51, 0x61, 0x07, 0x22, 0x71, 0x14, 0x32, 0x81, 0x91, 0xA1, 0x08,\n                0x23, 0x42, 0xB1, 0xC1, 0x15, 0x52, 0xD1, 0xF0, 0x24, 0x33, 0x62, 0x72,\n                0x82, 0x09, 0x0A, 0x16, 0x17, 0x18, 0x19, 0x1A, 0x25, 0x26, 0x27, 0x28,\n                0x29, 0x2A, 0x34, 0x35, 0x36, 0x37, 0x38, 0x39, 0x3A, 0x43, 0x44, 0x45,\n                0x46, 0x47, 0x48, 0x49, 0x4A, 0x53, 0x54, 0x55, 0x56, 0x57, 0x58, 0x59,\n                0x5A, 0x63, 0x64, 0x65, 0x66, 0x67, 0x68, 0x69, 0x6A, 0x73, 0x74, 0x75,\n                0x76, 0x77, 0x78, 0x79, 0x7A, 0x83, 0x84, 0x85, 0x86, 0x87, 0x88, 0x89,\n                0x8A, 0x92, 0x93, 0x94, 0x95, 0x96, 0x97, 0x98, 0x99, 0x9A, 0xA2, 0xA3,\n                0xA4, 0xA5, 0xA6, 0xA7, 0xA8, 0xA9, 0xAA, 0xB2, 0xB3, 0xB4, 0xB5, 0xB6,\n                0xB7, 0xB8, 0xB9, 0xBA, 0xC2, 0xC3, 0xC4, 0xC5, 0xC6, 0xC7, 0xC8, 0xC9,\n                0xCA, 0xD2, 0xD3, 0xD4, 0xD5, 0xD6, 0xD7, 0xD8, 0xD9, 0xDA, 0xE1, 0xE2,\n                0xE3, 0xE4, 0xE5, 0xE6, 0xE7, 0xE8, 0xE9, 0xEA, 0xF1, 0xF2, 0xF3, 0xF4,\n                0xF5, 0xF6, 0xF7, 0xF8, 0xF9, 0xFA, 0xFF, 0xDA, 0x00, 0x08, 0x01, 0x01,\n                0x00, 0x00, 0x3F, 0x00, 0xFD, 0xFC, 0xA3, 0x1E, 0xB4, 0x00, 0xFF, 0xD9,\n            ])\n\n    @app.get(\"/camera/capture\")\n    async def get_camera_frame() -&gt; Response:\n        \"\"\"Return a camera frame as JPEG for video streaming.\"\"\"\n        frame_data = generate_test_frame()\n        return Response(\n            content=frame_data,\n            media_type=\"image/jpeg\",\n            headers={\n                \"Cache-Control\": \"no-cache, no-store, must-revalidate\",\n                \"Pragma\": \"no-cache\",\n                \"Expires\": \"0\",\n            },\n        )\n\n    @app.get(\"/health\")\n    async def health_check() -&gt; dict[str, str]:\n        \"\"\"Check daemon health.\"\"\"\n        return {\"status\": \"healthy\", \"mode\": \"mock\"}\n\n    @app.get(\"/api/daemon/status\")\n    async def daemon_status() -&gt; dict[str, Any]:\n        \"\"\"Get detailed daemon status for web dashboard.\"\"\"\n        return {\n            \"status\": \"connected\",\n            \"mode\": \"mock\",\n            \"connection_type\": \"Simulator\",\n            \"version\": \"1.0.0\",\n            \"head\": _mock_state.head_position,\n            \"body_rotation\": _mock_state.body_rotation,\n            \"left_antenna\": _mock_state.left_antenna_angle,\n            \"right_antenna\": _mock_state.right_antenna_angle,\n            \"current_emotion\": _mock_state.current_emotion,\n            \"is_awake\": _mock_state.is_awake,\n            \"is_speaking\": _mock_state.is_speaking,\n            \"is_dancing\": _mock_state.is_dancing,\n        }\n\n    @app.post(\"/head/move\")\n    async def move_head(request: HeadMoveRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate head movement.\"\"\"\n        # Simulate movement delay based on speed\n        delay_map = {\"slow\": 0.5, \"normal\": 0.3, \"fast\": 0.1}\n        await asyncio.sleep(delay_map.get(request.speed, 0.3))\n\n        # Update simulated position\n        degrees = request.degrees or 20.0\n        direction_map = {\n            \"left\": (\"yaw\", -degrees),\n            \"right\": (\"yaw\", degrees),\n            \"up\": (\"pitch\", -degrees),\n            \"down\": (\"pitch\", degrees),\n            \"front\": (\"yaw\", 0.0),\n        }\n\n        if request.direction in direction_map:\n            axis, value = direction_map[request.direction]\n            if request.direction == \"front\":\n                _mock_state.head_position = {\"pitch\": 0.0, \"yaw\": 0.0, \"roll\": 0.0}\n            else:\n                _mock_state.head_position[axis] = value\n\n        return {\n            \"status\": \"success\",\n            \"position\": _mock_state.head_position,\n            \"message\": f\"Head moved {request.direction}\",\n        }\n\n    @app.post(\"/expression/emotion\")\n    async def play_emotion(request: EmotionRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate emotional expression.\"\"\"\n        # Simulate expression duration\n        await asyncio.sleep(0.5 * request.intensity)\n\n        _mock_state.current_emotion = request.emotion\n\n        return {\n            \"status\": \"success\",\n            \"emotion\": request.emotion,\n            \"intensity\": request.intensity,\n            \"message\": f\"Expressing {request.emotion}\",\n        }\n\n    @app.post(\"/audio/speak\")\n    async def speak(request: SpeakRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate speech output.\"\"\"\n        # Estimate speech duration (rough: 5 chars per second)\n        base_duration = len(request.text) / 5.0\n        duration = base_duration / request.speed\n\n        _mock_state.is_speaking = True\n        await asyncio.sleep(min(duration, 2.0))  # Cap at 2s for testing\n        _mock_state.is_speaking = False\n\n        return {\n            \"status\": \"success\",\n            \"text\": request.text,\n            \"duration_seconds\": duration,\n            \"message\": \"Speech completed\",\n        }\n\n    @app.post(\"/camera/capture\")\n    async def capture_image(request: CaptureRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate image capture.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate capture delay\n\n        result: dict[str, Any] = {\n            \"status\": \"success\",\n            \"width\": 640,\n            \"height\": 480,\n            \"format\": \"jpeg\",\n        }\n\n        if request.analyze:\n            # Simulate vision analysis\n            await asyncio.sleep(0.5)\n            result[\"analysis\"] = {\n                \"objects_detected\": [\"desk\", \"computer\", \"person\"],\n                \"faces_detected\": 1,\n                \"description\": \"A person sitting at a desk with a computer\",\n            }\n\n        if request.save:\n            result[\"saved_path\"] = \"/tmp/reachy_capture_mock.jpg\"\n\n        return result\n\n    @app.post(\"/antenna/state\")\n    async def set_antenna_state(request: AntennaRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate antenna control.\"\"\"\n        await asyncio.sleep(request.duration_ms / 1000.0)\n\n        if request.left_angle is not None:\n            _mock_state.left_antenna_angle = request.left_angle\n        if request.right_angle is not None:\n            _mock_state.right_antenna_angle = request.right_angle\n\n        return {\n            \"status\": \"success\",\n            \"left_angle\": _mock_state.left_antenna_angle,\n            \"right_angle\": _mock_state.right_antenna_angle,\n            \"wiggle\": request.wiggle,\n        }\n\n    @app.get(\"/sensors\")\n    async def get_sensors(\n        sensors: str = Query(default=\"all\"),\n    ) -&gt; dict[str, Any]:\n        \"\"\"Simulate sensor readings.\"\"\"\n        sensor_list = sensors.split(\",\")\n\n        result: dict[str, Any] = {\"status\": \"success\"}\n\n        if \"all\" in sensor_list or \"imu\" in sensor_list:\n            result[\"imu\"] = {\n                \"acceleration\": {\n                    \"x\": random.uniform(-0.1, 0.1),\n                    \"y\": random.uniform(-0.1, 0.1),\n                    \"z\": 9.8 + random.uniform(-0.1, 0.1),\n                },\n                \"gyroscope\": {\n                    \"x\": random.uniform(-1, 1),\n                    \"y\": random.uniform(-1, 1),\n                    \"z\": random.uniform(-1, 1),\n                },\n            }\n\n        if \"all\" in sensor_list or \"audio_level\" in sensor_list:\n            result[\"audio_level\"] = {\n                \"level_db\": random.uniform(-60, -20),\n                \"is_speech_detected\": random.random() &gt; 0.7,\n            }\n\n        if \"all\" in sensor_list or \"temperature\" in sensor_list:\n            result[\"temperature\"] = {\n                \"cpu_celsius\": 45.0 + random.uniform(-5, 10),\n                \"ambient_celsius\": 22.0 + random.uniform(-2, 2),\n            }\n\n        return result\n\n    @app.post(\"/audio/look_at_sound\")\n    async def look_at_sound(request: LookAtSoundRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate sound localization.\"\"\"\n        # Simulate listening period\n        await asyncio.sleep(min(request.timeout_ms / 1000.0, 1.0))\n\n        # Randomly determine if sound was detected\n        if random.random() &gt; 0.3:\n            direction = random.choice([\"left\", \"right\", \"front\"])\n            angle = random.uniform(10, 45) * (1 if direction == \"right\" else -1)\n\n            return {\n                \"status\": \"success\",\n                \"sound_detected\": True,\n                \"direction\": direction,\n                \"angle_degrees\": angle,\n                \"confidence\": random.uniform(0.7, 0.95),\n            }\n        else:\n            return {\n                \"status\": \"success\",\n                \"sound_detected\": False,\n                \"message\": \"No significant sound detected\",\n            }\n\n    @app.post(\"/expression/dance\")\n    async def dance(request: DanceRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate dance routine.\"\"\"\n        _mock_state.is_dancing = True\n\n        # Simulate dance duration (capped for testing)\n        await asyncio.sleep(min(request.duration_seconds, 2.0))\n\n        _mock_state.is_dancing = False\n\n        return {\n            \"status\": \"success\",\n            \"routine\": request.routine,\n            \"duration_seconds\": request.duration_seconds,\n            \"message\": f\"Completed {request.routine} dance\",\n        }\n\n    # ========== NEW ENDPOINTS FOR FULL SDK SUPPORT ==========\n\n    @app.post(\"/body/rotate\")\n    async def rotate(request: RotateRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate body rotation.\"\"\"\n        delay_map = {\"slow\": 0.5, \"normal\": 0.3, \"fast\": 0.1}\n        await asyncio.sleep(delay_map.get(request.speed, 0.3))\n\n        # Update rotation (accumulate, wrap at 360)\n        delta = request.degrees if request.direction == \"right\" else -request.degrees\n        _mock_state.body_rotation = (_mock_state.body_rotation + delta) % 360\n\n        return {\n            \"status\": \"success\",\n            \"direction\": request.direction,\n            \"degrees\": request.degrees,\n            \"current_rotation\": _mock_state.body_rotation,\n            \"message\": f\"Rotated {request.direction} by {request.degrees} degrees\",\n        }\n\n    @app.post(\"/head/look_at\")\n    async def look_at(request: LookAtRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate precise head positioning.\"\"\"\n        await asyncio.sleep(request.duration)\n\n        _mock_state.head_position = {\n            \"pitch\": request.pitch,\n            \"yaw\": request.yaw,\n            \"roll\": request.roll,\n            \"z\": request.z,\n        }\n\n        return {\n            \"status\": \"success\",\n            \"position\": _mock_state.head_position,\n            \"duration\": request.duration,\n            \"message\": \"Head positioned\",\n        }\n\n    @app.post(\"/audio/listen\")\n    async def listen(request: ListenRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate audio capture.\"\"\"\n        _mock_state.is_listening = True\n\n        # Simulate recording (capped for testing)\n        await asyncio.sleep(min(request.duration_seconds, 1.0))\n\n        _mock_state.is_listening = False\n\n        # Return mock audio data (base64-encoded silence)\n        import base64\n\n        # Generate mock audio header (WAV format indicator)\n        mock_audio = base64.b64encode(b\"RIFF\" + b\"\\x00\" * 100).decode(\"utf-8\")\n\n        return {\n            \"status\": \"success\",\n            \"duration_seconds\": request.duration_seconds,\n            \"format\": \"wav\",\n            \"sample_rate\": 16000,\n            \"channels\": 4,\n            \"audio_base64\": mock_audio,\n            \"message\": f\"Recorded {request.duration_seconds}s of audio\",\n        }\n\n    @app.post(\"/lifecycle/wake_up\")\n    async def wake_up() -&gt; dict[str, Any]:\n        \"\"\"Simulate motor initialization.\"\"\"\n        await asyncio.sleep(0.5)  # Simulate startup time\n\n        _mock_state.is_awake = True\n        # Reset to neutral position\n        _mock_state.head_position = {\"pitch\": 0.0, \"yaw\": 0.0, \"roll\": 0.0, \"z\": 0.0}\n        _mock_state.left_antenna_angle = 45.0\n        _mock_state.right_antenna_angle = 45.0\n\n        return {\n            \"status\": \"success\",\n            \"is_awake\": True,\n            \"message\": \"Robot motors initialized and ready\",\n        }\n\n    @app.post(\"/lifecycle/sleep\")\n    async def sleep() -&gt; dict[str, Any]:\n        \"\"\"Simulate motor shutdown.\"\"\"\n        await asyncio.sleep(0.3)  # Simulate shutdown time\n\n        _mock_state.is_awake = False\n        _mock_state.is_dancing = False\n        _mock_state.is_speaking = False\n\n        return {\n            \"status\": \"success\",\n            \"is_awake\": False,\n            \"message\": \"Robot motors powered down\",\n        }\n\n    @app.post(\"/gesture/nod\")\n    async def nod(request: GestureRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate nodding gesture.\"\"\"\n        delay_map = {\"slow\": 0.4, \"normal\": 0.25, \"fast\": 0.15}\n        nod_delay = delay_map.get(request.speed, 0.25)\n\n        # Simulate nodding motion\n        for _ in range(request.times):\n            await asyncio.sleep(nod_delay)\n\n        return {\n            \"status\": \"success\",\n            \"gesture\": \"nod\",\n            \"times\": request.times,\n            \"speed\": request.speed,\n            \"message\": f\"Nodded {request.times} time(s)\",\n        }\n\n    @app.post(\"/gesture/shake\")\n    async def shake(request: GestureRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate head shake gesture.\"\"\"\n        delay_map = {\"slow\": 0.4, \"normal\": 0.25, \"fast\": 0.15}\n        shake_delay = delay_map.get(request.speed, 0.25)\n\n        # Simulate shaking motion\n        for _ in range(request.times):\n            await asyncio.sleep(shake_delay)\n\n        return {\n            \"status\": \"success\",\n            \"gesture\": \"shake\",\n            \"times\": request.times,\n            \"speed\": request.speed,\n            \"message\": f\"Shook head {request.times} time(s)\",\n        }\n\n    @app.post(\"/gesture/rest\")\n    async def rest() -&gt; dict[str, Any]:\n        \"\"\"Simulate returning to rest pose.\"\"\"\n        await asyncio.sleep(0.3)\n\n        # Reset to neutral\n        _mock_state.head_position = {\"pitch\": 0.0, \"yaw\": 0.0, \"roll\": 0.0, \"z\": 0.0}\n        _mock_state.left_antenna_angle = 45.0\n        _mock_state.right_antenna_angle = 45.0\n        _mock_state.current_emotion = None\n\n        return {\n            \"status\": \"success\",\n            \"position\": _mock_state.head_position,\n            \"left_antenna\": _mock_state.left_antenna_angle,\n            \"right_antenna\": _mock_state.right_antenna_angle,\n            \"message\": \"Returned to rest pose\",\n        }\n\n    @app.post(\"/actions/cancel\")\n    async def cancel_action(request: CancelActionRequest) -&gt; dict[str, Any]:\n        \"\"\"Simulate canceling an action.\"\"\"\n        return {\n            \"status\": \"success\",\n            \"action_id\": request.action_id or \"all\",\n            \"message\": \"Action cancelled (mock)\",\n        }\n\n    @app.get(\"/pose\")\n    async def get_pose() -&gt; dict[str, Any]:\n        \"\"\"Get current robot pose.\"\"\"\n        return {\n            \"status\": \"success\",\n            \"head\": {\n                \"roll\": _mock_state.head_position.get(\"roll\", 0.0),\n                \"pitch\": _mock_state.head_position.get(\"pitch\", 0.0),\n                \"yaw\": _mock_state.head_position.get(\"yaw\", 0.0),\n            },\n            \"body_yaw\": 0.0,\n            \"antennas\": {\n                \"left\": _mock_state.left_antenna_angle,\n                \"right\": _mock_state.right_antenna_angle,\n            },\n            \"timestamp\": \"mock\",\n        }\n\n    return app\n</code></pre>"},{"location":"api/mcp-servers/#memory-mcp-server","title":"Memory MCP Server","text":"<p>The Memory MCP server provides 4 tools for persistent memory and user profiles.</p>"},{"location":"api/mcp-servers/#server-module_1","title":"Server Module","text":""},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.memory.memory_mcp","title":"<code>memory_mcp</code>","text":"<p>Memory MCP Server - Exposes memory operations as MCP tools.</p> <p>Provides 4 tools for memory management: - search_memories: Semantic search over stored memories - store_memory: Save a new memory - get_user_profile: Retrieve user profile - update_user_profile: Update a user preference</p>"},{"location":"api/mcp-servers/#reachy_agent.mcp_servers.memory.memory_mcp.create_memory_mcp_server","title":"<code>create_memory_mcp_server(manager)</code>","text":"<p>Create and configure the Memory MCP server.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>MemoryManager</code> <p>Initialized MemoryManager instance.</p> required <p>Returns:</p> Type Description <code>FastMCP</code> <p>Configured FastMCP server instance.</p> Source code in <code>src/reachy_agent/mcp_servers/memory/memory_mcp.py</code> <pre><code>def create_memory_mcp_server(\n    manager: MemoryManager,\n) -&gt; FastMCP:\n    \"\"\"Create and configure the Memory MCP server.\n\n    Args:\n        manager: Initialized MemoryManager instance.\n\n    Returns:\n        Configured FastMCP server instance.\n    \"\"\"\n    mcp = FastMCP(\"Memory System\")\n\n    @mcp.tool()\n    async def search_memories(\n        query: str,\n        n_results: int = 5,\n        memory_type: str | None = None,\n    ) -&gt; dict:\n        \"\"\"Search memories by semantic similarity.\n\n        Use this to find relevant past memories, facts, preferences,\n        or observations that might help answer the current question.\n\n        Args:\n            query: Natural language search query describing what you're looking for.\n            n_results: Maximum number of results to return (default: 5).\n            memory_type: Optional filter by type (conversation, observation,\n                        fact, preference, event, task).\n\n        Returns:\n            Dictionary with list of matching memories and their relevance scores.\n\n        Example:\n            search_memories(\"user's coffee preferences\") -&gt; finds preferences about coffee\n            search_memories(\"previous conversations about travel\", memory_type=\"conversation\")\n        \"\"\"\n        log.info(f\"Searching memories: {query}\")\n\n        # Parse memory type if provided\n        type_filter = None\n        if memory_type:\n            try:\n                type_filter = MemoryType.from_string(memory_type)\n            except ValueError:\n                return {\n                    \"error\": f\"Invalid memory_type. Must be one of: {[t.value for t in MemoryType]}\"\n                }\n\n        # Clamp n_results to reasonable range\n        n_results = max(1, min(n_results, 20))\n\n        try:\n            results = await manager.search_memories(query, n_results, type_filter)\n\n            return {\n                \"status\": \"ok\",\n                \"count\": len(results),\n                \"memories\": [\n                    {\n                        \"content\": r.memory.content,\n                        \"type\": r.memory.memory_type.value,\n                        \"score\": round(r.score, 3),\n                        \"timestamp\": r.memory.timestamp.isoformat(),\n                    }\n                    for r in results\n                ],\n            }\n        except Exception as e:\n            log.error(f\"Memory search failed: {e}\")\n            return {\"error\": str(e)}\n\n    @mcp.tool()\n    async def store_memory(\n        content: str,\n        memory_type: str = \"fact\",\n    ) -&gt; dict:\n        \"\"\"Store a new memory for future retrieval.\n\n        Use this to save important information learned during the conversation\n        that should be remembered for future sessions.\n\n        Args:\n            content: The information to remember (be specific and concise).\n            memory_type: Category of memory:\n                - conversation: Dialog exchanges\n                - observation: Things seen or heard\n                - fact: Learned facts about the user or world\n                - preference: User preferences\n                - event: Calendar events or schedules\n                - task: Tasks or reminders\n\n        Returns:\n            Confirmation with the stored memory ID.\n\n        Example:\n            store_memory(\"User prefers morning meetings before 10am\", \"preference\")\n            store_memory(\"User mentioned they have a dog named Max\", \"fact\")\n        \"\"\"\n        log.info(f\"Storing memory: {content[:50]}...\")\n\n        # Validate memory type\n        try:\n            mem_type = MemoryType.from_string(memory_type)\n        except ValueError:\n            return {\n                \"error\": f\"Invalid memory_type. Must be one of: {[t.value for t in MemoryType]}\"\n            }\n\n        # Validate content length\n        if not content or len(content.strip()) &lt; 5:\n            return {\"error\": \"Content must be at least 5 characters\"}\n        if len(content) &gt; 2000:\n            return {\"error\": \"Content must be less than 2000 characters\"}\n\n        try:\n            memory = await manager.store_memory(content.strip(), mem_type)\n\n            return {\n                \"status\": \"ok\",\n                \"message\": f\"Memory stored successfully\",\n                \"memory_id\": memory.id,\n                \"type\": mem_type.value,\n            }\n        except Exception as e:\n            log.error(f\"Failed to store memory: {e}\")\n            return {\"error\": str(e)}\n\n    @mcp.tool()\n    async def get_user_profile() -&gt; dict:\n        \"\"\"Get the current user's profile and preferences.\n\n        Use this to retrieve stored information about the user including\n        their name, preferences, schedule patterns, and connected services.\n\n        Returns:\n            User profile with all stored preferences.\n        \"\"\"\n        log.info(\"Getting user profile\")\n\n        try:\n            profile = await manager.get_profile()\n\n            return {\n                \"status\": \"ok\",\n                \"profile\": {\n                    \"user_id\": profile.user_id,\n                    \"name\": profile.name,\n                    \"preferences\": profile.preferences,\n                    \"schedule_patterns\": profile.schedule_patterns,\n                    \"connected_services\": profile.connected_services,\n                },\n            }\n        except Exception as e:\n            log.error(f\"Failed to get profile: {e}\")\n            return {\"error\": str(e)}\n\n    @mcp.tool()\n    async def update_user_profile(\n        key: str,\n        value: str,\n    ) -&gt; dict:\n        \"\"\"Update a user preference.\n\n        Use this when the user explicitly states a preference that should\n        be remembered for future sessions.\n\n        Args:\n            key: The preference key (e.g., \"wake_time\", \"coffee_preference\",\n                \"nickname\", \"timezone\").\n            value: The preference value.\n\n        Returns:\n            Confirmation of the update.\n\n        Example:\n            update_user_profile(\"nickname\", \"John\")\n            update_user_profile(\"wake_time\", \"7:00 AM\")\n            update_user_profile(\"coffee_preference\", \"black, no sugar\")\n        \"\"\"\n        log.info(f\"Updating preference: {key}={value}\")\n\n        # Validate inputs\n        if not key or len(key.strip()) &lt; 2:\n            return {\"error\": \"Key must be at least 2 characters\"}\n        if len(key) &gt; 50:\n            return {\"error\": \"Key must be less than 50 characters\"}\n        if len(value) &gt; 500:\n            return {\"error\": \"Value must be less than 500 characters\"}\n\n        try:\n            profile = await manager.update_preference(key.strip(), value.strip())\n\n            return {\n                \"status\": \"ok\",\n                \"message\": f\"Updated preference '{key}'\",\n                \"preferences\": profile.preferences,\n            }\n        except Exception as e:\n            log.error(f\"Failed to update preference: {e}\")\n            return {\"error\": str(e)}\n\n    return mcp\n</code></pre>"},{"location":"api/mcp-servers/#running-mcp-servers","title":"Running MCP Servers","text":""},{"location":"api/mcp-servers/#as-subprocess-sdk-integration","title":"As Subprocess (SDK Integration)","text":"<p>The Claude Agent SDK automatically manages MCP servers as subprocesses:</p> <pre><code>mcp_servers = {\n    \"reachy\": {\n        \"type\": \"stdio\",\n        \"command\": sys.executable,\n        \"args\": [\"-m\", \"reachy_agent.mcp_servers.reachy\", daemon_url],\n    },\n    \"memory\": {\n        \"type\": \"stdio\",\n        \"command\": sys.executable,\n        \"args\": [\"-m\", \"reachy_agent.mcp_servers.memory\"],\n    },\n}\n</code></pre>"},{"location":"api/mcp-servers/#standalone-mcp-inspector","title":"Standalone (MCP Inspector)","text":"<p>For testing with MCP Inspector:</p> <pre><code># Start mock daemon\npython -m reachy_agent.mcp_servers.reachy.daemon_mock\n\n# Run MCP Inspector\nnpx @modelcontextprotocol/inspector \\\n  .venv/bin/python -m reachy_agent.mcp_servers.reachy\n</code></pre>"},{"location":"api/mcp-servers/#tool-reference","title":"Tool Reference","text":"<p>For complete tool documentation, see:</p> <ul> <li>MCP Tools Quick Reference</li> <li>Memory System</li> </ul>"},{"location":"api/mcp-tools/","title":"MCP Tools Reference","text":"<p>\u26a0\ufe0f DEPRECATED: This document is outdated. The Reachy MCP server now provides 23 tools including native SDK emotions from HuggingFace.</p> <p>Please see the current reference: ai_docs/mcp-tools-quick-ref.md</p> <p>The following content is preserved for historical reference only.</p> <p>The Reachy MCP server exposes 16 tools that Claude can use to control the robot. This document provides a complete reference for all available tools.</p>"},{"location":"api/mcp-tools/#tool-categories","title":"Tool Categories","text":"<pre><code>mindmap\n  root((Reachy MCP))\n    Movement\n      move_head\n      look_at\n      rotate\n    Expression\n      play_emotion\n      set_antenna_state\n      nod\n      shake\n    Audio\n      speak\n      play_sound\n    Perception\n      capture_image\n      get_sensor_data\n      look_at_sound\n    Lifecycle\n      wake_up\n      sleep\n      rest\n    Actions\n      dance</code></pre>"},{"location":"api/mcp-tools/#movement-tools","title":"Movement Tools","text":""},{"location":"api/mcp-tools/#move_head","title":"move_head","text":"<p>Move the robot's head in a cardinal direction.</p> <pre><code>tool: move_head\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>direction</code> string Yes - One of: <code>left</code>, <code>right</code>, <code>up</code>, <code>down</code>, <code>front</code> <code>speed</code> string No <code>normal</code> One of: <code>slow</code>, <code>normal</code>, <code>fast</code> <code>degrees</code> number No 30 Angle in degrees (0-90) <p>Example: <pre><code>{\n  \"tool\": \"move_head\",\n  \"input\": {\n    \"direction\": \"left\",\n    \"speed\": \"fast\",\n    \"degrees\": 45\n  }\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"status\": \"ok\",\n  \"message\": \"Head moved left by 45 degrees\",\n  \"uuid\": \"a1b2c3d4-...\"\n}\n</code></pre></p>"},{"location":"api/mcp-tools/#look_at","title":"look_at","text":"<p>Position the head with precise Euler angles.</p> <pre><code>tool: look_at\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>roll</code> number No 0 Tilt left/right in degrees <code>pitch</code> number No 0 Up (negative) / Down (positive) <code>yaw</code> number No 0 Left (positive) / Right (negative) <code>z</code> number No 0 Vertical offset (limited use) <p>Example: <pre><code>{\n  \"tool\": \"look_at\",\n  \"input\": {\n    \"roll\": 0,\n    \"pitch\": -15,\n    \"yaw\": 30\n  }\n}\n</code></pre></p>"},{"location":"api/mcp-tools/#rotate","title":"rotate","text":"<p>Rotate the robot's body.</p> <pre><code>tool: rotate\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>direction</code> string Yes - One of: <code>left</code>, <code>right</code> <code>degrees</code> number No 90 Rotation amount (0-360) <code>speed</code> string No <code>normal</code> One of: <code>slow</code>, <code>normal</code>, <code>fast</code>"},{"location":"api/mcp-tools/#expression-tools","title":"Expression Tools","text":""},{"location":"api/mcp-tools/#play_emotion","title":"play_emotion","text":"<p>Trigger a predefined emotion sequence.</p> <pre><code>tool: play_emotion\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>emotion</code> string Yes - See emotion list below <code>intensity</code> number No 0.7 Intensity from 0.0 to 1.0 <p>Available Emotions:</p> Emotion Description <code>happy</code> Antennas up, slight nod <code>sad</code> Antennas down, head droops <code>curious</code> Head tilt, one antenna up <code>surprised</code> Quick antenna movement <code>thinking</code> Slow side-to-side <code>confused</code> Head tilt, antenna wiggle <code>excited</code> Rapid antenna movement <code>sleepy</code> Slow droop, eyes close <code>greeting</code> Head nod, antennas wave <code>acknowledge</code> Quick nod <p>Example: <pre><code>{\n  \"tool\": \"play_emotion\",\n  \"input\": {\n    \"emotion\": \"curious\",\n    \"intensity\": 0.8\n  }\n}\n</code></pre></p>"},{"location":"api/mcp-tools/#set_antenna_state","title":"set_antenna_state","text":"<p>Fine-grained control of antenna positions.</p> <pre><code>tool: set_antenna_state\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>left_angle</code> number No 45 Left antenna angle (0-90 degrees) <code>right_angle</code> number No 45 Right antenna angle (0-90 degrees) <code>wiggle</code> boolean No false Add subtle movement <code>duration_ms</code> number No 500 Movement duration in ms <p>Antenna Semantics:</p> Position Meaning Both at 0\u00b0 Passive/sleeping Both at 45\u00b0 Alert/neutral Both at 90\u00b0 Engaged/listening Asymmetric Curious/confused Wiggling Processing/thinking"},{"location":"api/mcp-tools/#nod","title":"nod","text":"<p>Perform a nodding gesture (affirmative).</p> <pre><code>tool: nod\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>times</code> number No 2 Number of nods (1-5) <code>speed</code> string No <code>normal</code> One of: <code>slow</code>, <code>normal</code>, <code>fast</code>"},{"location":"api/mcp-tools/#shake","title":"shake","text":"<p>Perform a head shake gesture (negative).</p> <pre><code>tool: shake\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>times</code> number No 2 Number of shakes (1-5) <code>speed</code> string No <code>normal</code> One of: <code>slow</code>, <code>normal</code>, <code>fast</code>"},{"location":"api/mcp-tools/#audio-tools","title":"Audio Tools","text":""},{"location":"api/mcp-tools/#speak","title":"speak","text":"<p>Output speech through the robot's speaker.</p> <pre><code>tool: speak\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>text</code> string Yes - Text to speak (max 500 chars) <code>voice</code> string No <code>default</code> Voice preset <code>speed</code> number No 1.0 Speech rate (0.5-2.0) <code>pitch</code> number No 1.0 Voice pitch (0.5-2.0) <p>Example: <pre><code>{\n  \"tool\": \"speak\",\n  \"input\": {\n    \"text\": \"Hello! I'm Reachy, your robot assistant.\",\n    \"speed\": 1.1\n  }\n}\n</code></pre></p>"},{"location":"api/mcp-tools/#play_sound","title":"play_sound","text":"<p>Play a sound effect.</p> <pre><code>tool: play_sound\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>sound</code> string Yes - Sound name or path <code>volume</code> number No 0.8 Volume (0.0-1.0) <p>Built-in Sounds:</p> Sound Description <code>beep</code> Simple beep <code>chime</code> Pleasant chime <code>alert</code> Attention sound <code>confirm</code> Confirmation tone <code>error</code> Error indicator"},{"location":"api/mcp-tools/#perception-tools","title":"Perception Tools","text":""},{"location":"api/mcp-tools/#capture_image","title":"capture_image","text":"<p>Capture a frame from the robot's camera.</p> <pre><code>tool: capture_image\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>format</code> string No <code>jpeg</code> One of: <code>jpeg</code>, <code>png</code> <code>quality</code> number No 85 JPEG quality (1-100) <code>resize</code> object No - Resize options <p>Response: <pre><code>{\n  \"status\": \"ok\",\n  \"image\": \"base64-encoded-data...\",\n  \"width\": 640,\n  \"height\": 480,\n  \"timestamp\": \"2024-12-20T12:00:00Z\"\n}\n</code></pre></p>"},{"location":"api/mcp-tools/#get_sensor_data","title":"get_sensor_data","text":"<p>Read sensor values.</p> <pre><code>tool: get_sensor_data\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>sensors</code> array No all Sensors to read <p>Available Sensors:</p> Sensor Data <code>imu</code> Accelerometer, gyroscope <code>audio_level</code> Ambient audio level (dB) <code>temperature</code> Internal temperature <code>battery</code> Battery level (if wireless) <code>position</code> Current motor positions <p>Example Response: <pre><code>{\n  \"status\": \"ok\",\n  \"imu\": {\n    \"accel\": [0.01, 0.02, 9.81],\n    \"gyro\": [0.0, 0.0, 0.0]\n  },\n  \"audio_level\": 42.5,\n  \"temperature\": 38.2\n}\n</code></pre></p>"},{"location":"api/mcp-tools/#look_at_sound","title":"look_at_sound","text":"<p>Turn the head toward a detected sound source.</p> <pre><code>tool: look_at_sound\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>threshold_db</code> number No 50 Minimum sound level to trigger <code>timeout_ms</code> number No 3000 Max wait time for sound <p>Response: <pre><code>{\n  \"status\": \"ok\",\n  \"detected\": true,\n  \"direction\": 45,\n  \"confidence\": 0.85\n}\n</code></pre></p>"},{"location":"api/mcp-tools/#lifecycle-tools","title":"Lifecycle Tools","text":""},{"location":"api/mcp-tools/#wake_up","title":"wake_up","text":"<p>Activate the robot (enable motors, ready position).</p> <pre><code>tool: wake_up\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters: None</p> <p>Response: <pre><code>{\n  \"status\": \"ok\",\n  \"message\": \"Robot is now awake\",\n  \"uuid\": \"...\"\n}\n</code></pre></p>"},{"location":"api/mcp-tools/#sleep","title":"sleep","text":"<p>Deactivate the robot (safe position, disable motors).</p> <pre><code>tool: sleep\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters: None</p>"},{"location":"api/mcp-tools/#rest","title":"rest","text":"<p>Return to neutral resting pose without disabling motors.</p> <pre><code>tool: rest\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters: None</p>"},{"location":"api/mcp-tools/#action-tools","title":"Action Tools","text":""},{"location":"api/mcp-tools/#dance","title":"dance","text":"<p>Execute a choreographed movement routine.</p> <pre><code>tool: dance\npermission_tier: 1 (Autonomous)\n</code></pre> <p>Parameters:</p> Parameter Type Required Default Description <code>routine</code> string Yes - Dance routine name <code>duration</code> number No 10 Max duration in seconds <p>Available Routines:</p> Routine Description <code>wiggle</code> Side-to-side wiggle <code>happy_dance</code> Excited movement <code>groove</code> Rhythmic movement <code>wave</code> Antenna wave"},{"location":"api/mcp-tools/#error-handling","title":"Error Handling","text":"<p>All tools return errors in a consistent format:</p> <pre><code>{\n  \"status\": \"error\",\n  \"error\": \"Error message\",\n  \"code\": \"ERROR_CODE\"\n}\n</code></pre> <p>Common Error Codes:</p> Code Description <code>INVALID_PARAMETER</code> Invalid parameter value <code>HARDWARE_ERROR</code> Hardware communication failed <code>TIMEOUT</code> Operation timed out <code>NOT_READY</code> Robot not in ready state <code>PERMISSION_DENIED</code> Action not permitted"},{"location":"api/mcp-tools/#permission-integration","title":"Permission Integration","text":"<pre><code>sequenceDiagram\n    participant C as Claude\n    participant A as Agent Loop\n    participant P as Permission Evaluator\n    participant M as MCP Server\n    participant D as Reachy Daemon\n\n    C-&gt;&gt;A: Tool call: move_head\n    A-&gt;&gt;P: Evaluate permission\n    P--&gt;&gt;A: Tier 1: Autonomous\n    A-&gt;&gt;M: Execute tool\n    M-&gt;&gt;D: POST /api/move/set_target\n    D--&gt;&gt;M: Success\n    M--&gt;&gt;A: Result\n    A--&gt;&gt;C: Tool result</code></pre> <p>Note: Real hardware uses <code>/api/move/set_target</code> for smooth movements. The client auto-detects the backend.</p> <p>All body control tools are Tier 1 (Autonomous) by default. See Permission System for customization.</p>"},{"location":"api/memory/","title":"Memory Module API","text":"<p>The memory module provides semantic memory storage and user profile management.</p>"},{"location":"api/memory/#memory-manager","title":"Memory Manager","text":""},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager","title":"<code>MemoryManager(chroma_path, sqlite_path, embedding_model='all-MiniLM-L6-v2', retention_days=90)</code>","text":"<p>Unified interface for the memory system.</p> <p>Coordinates between ChromaDB for semantic memories and SQLite for user profiles and session summaries.</p> <p>Parameters:</p> Name Type Description Default <code>chroma_path</code> <code>str | Path</code> <p>Path to ChromaDB persistence directory.</p> required <code>sqlite_path</code> <code>str | Path</code> <p>Path to SQLite database file.</p> required <code>embedding_model</code> <code>str</code> <p>Name of sentence-transformers model.</p> <code>'all-MiniLM-L6-v2'</code> <code>retention_days</code> <code>int</code> <p>Days to retain memories before cleanup.</p> <code>90</code> Example <p>manager = MemoryManager( ...     chroma_path=\"~/.reachy/memory/chroma\", ...     sqlite_path=\"~/.reachy/memory/reachy.db\", ... ) await manager.initialize() await manager.store_memory(\"User likes coffee\", MemoryType.PREFERENCE) results = await manager.search_memories(\"coffee preferences\")</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>def __init__(\n    self,\n    chroma_path: str | Path,\n    sqlite_path: str | Path,\n    embedding_model: str = \"all-MiniLM-L6-v2\",\n    retention_days: int = 90,\n) -&gt; None:\n    self.chroma_store = ChromaMemoryStore(chroma_path, embedding_model)\n    self.sqlite_store = SQLiteProfileStore(sqlite_path)\n    self.retention_days = retention_days\n\n    self._current_session: SessionSummary | None = None\n    self._current_user_id: str = \"default\"\n    self._initialized = False\n    self._session_lock = asyncio.Lock()  # Thread safety for session operations\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.from_config","title":"<code>from_config(chroma_path, sqlite_path, embedding_model='all-MiniLM-L6-v2', max_memories=10000, retention_days=90)</code>  <code>classmethod</code>","text":"<p>Create MemoryManager from configuration values.</p> <p>This mirrors the MemoryConfig structure from utils/config.py.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    chroma_path: str,\n    sqlite_path: str,\n    embedding_model: str = \"all-MiniLM-L6-v2\",\n    max_memories: int = 10000,\n    retention_days: int = 90,\n) -&gt; MemoryManager:\n    \"\"\"Create MemoryManager from configuration values.\n\n    This mirrors the MemoryConfig structure from utils/config.py.\n    \"\"\"\n    return cls(\n        chroma_path=chroma_path,\n        sqlite_path=sqlite_path,\n        embedding_model=embedding_model,\n        retention_days=retention_days,\n    )\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.initialize","title":"<code>initialize(user_id='default')</code>  <code>async</code>","text":"<p>Initialize both storage backends.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>The user to load profile for.</p> <code>'default'</code> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def initialize(self, user_id: str = \"default\") -&gt; None:\n    \"\"\"Initialize both storage backends.\n\n    Args:\n        user_id: The user to load profile for.\n    \"\"\"\n    log.info(\"Initializing memory system...\")\n    await self.chroma_store.initialize()\n    await self.sqlite_store.initialize()\n    self._current_user_id = user_id\n    self._initialized = True\n    log.info(\"Memory system initialized\")\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close both storage backends.</p> Note <p>Idempotent: safe to call multiple times. Thread-safe: end_session() uses asyncio.Lock internally.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close both storage backends.\n\n    Note:\n        Idempotent: safe to call multiple times.\n        Thread-safe: end_session() uses asyncio.Lock internally.\n    \"\"\"\n    # Always call end_session() - it's idempotent and handles None case\n    # This avoids TOCTOU race condition from checking _current_session first\n    await self.end_session()\n    await self.chroma_store.close()\n    await self.sqlite_store.close()\n    self._initialized = False\n    log.info(\"Memory system closed\")\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.start_session","title":"<code>start_session(user_id=None)</code>  <code>async</code>","text":"<p>Start a new conversation session.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str | None</code> <p>Optional user ID override.</p> <code>None</code> <p>Returns:</p> Type Description <code>SessionSummary</code> <p>The new SessionSummary.</p> Note <p>Thread-safe: uses asyncio.Lock to prevent concurrent session creation.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def start_session(self, user_id: str | None = None) -&gt; SessionSummary:\n    \"\"\"Start a new conversation session.\n\n    Args:\n        user_id: Optional user ID override.\n\n    Returns:\n        The new SessionSummary.\n\n    Note:\n        Thread-safe: uses asyncio.Lock to prevent concurrent session creation.\n    \"\"\"\n    async with self._session_lock:\n        if user_id:\n            self._current_user_id = user_id\n\n        session_id = str(uuid.uuid4())\n        self._current_session = SessionSummary(\n            session_id=session_id,\n            user_id=self._current_user_id,\n            start_time=datetime.now(),\n        )\n\n        await self.sqlite_store.save_session(self._current_session)\n        log.info(f\"Started session {session_id}\")\n        return self._current_session\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.end_session","title":"<code>end_session(summary_text='', key_topics=None)</code>  <code>async</code>","text":"<p>End the current session with an optional summary.</p> <p>Parameters:</p> Name Type Description Default <code>summary_text</code> <code>str</code> <p>Summary of the session (can be generated by Claude).</p> <code>''</code> <code>key_topics</code> <code>list[str] | None</code> <p>List of key topics discussed.</p> <code>None</code> <p>Returns:</p> Type Description <code>SessionSummary | None</code> <p>The ended SessionSummary, or None if no active session.</p> Note <p>Thread-safe: uses asyncio.Lock to prevent concurrent session modification.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def end_session(\n    self,\n    summary_text: str = \"\",\n    key_topics: list[str] | None = None,\n) -&gt; SessionSummary | None:\n    \"\"\"End the current session with an optional summary.\n\n    Args:\n        summary_text: Summary of the session (can be generated by Claude).\n        key_topics: List of key topics discussed.\n\n    Returns:\n        The ended SessionSummary, or None if no active session.\n\n    Note:\n        Thread-safe: uses asyncio.Lock to prevent concurrent session modification.\n    \"\"\"\n    async with self._session_lock:\n        if not self._current_session:\n            log.warning(\"No active session to end\")\n            return None\n\n        self._current_session.end_time = datetime.now()\n        self._current_session.summary_text = summary_text\n        self._current_session.key_topics = key_topics or []\n        self._current_session.memory_count = await self.chroma_store.count()\n\n        await self.sqlite_store.save_session(self._current_session)\n        log.info(f\"Ended session {self._current_session.session_id}\")\n\n        session = self._current_session\n        self._current_session = None\n        return session\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.store_memory","title":"<code>store_memory(content, memory_type, metadata=None)</code>  <code>async</code>","text":"<p>Store a new memory.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The text content to store.</p> required <code>memory_type</code> <code>MemoryType</code> <p>Category of this memory.</p> required <code>metadata</code> <code>dict | None</code> <p>Optional additional metadata.</p> <code>None</code> <p>Returns:</p> Type Description <code>Memory</code> <p>The created Memory object.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def store_memory(\n    self,\n    content: str,\n    memory_type: MemoryType,\n    metadata: dict | None = None,\n) -&gt; Memory:\n    \"\"\"Store a new memory.\n\n    Args:\n        content: The text content to store.\n        memory_type: Category of this memory.\n        metadata: Optional additional metadata.\n\n    Returns:\n        The created Memory object.\n    \"\"\"\n    # Add session context to metadata\n    full_metadata = metadata or {}\n    if self._current_session:\n        full_metadata[\"session_id\"] = self._current_session.session_id\n    full_metadata[\"user_id\"] = self._current_user_id\n\n    return await self.chroma_store.store(content, memory_type, full_metadata)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.search_memories","title":"<code>search_memories(query, n_results=5, memory_type=None)</code>  <code>async</code>","text":"<p>Search memories by semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query text.</p> required <code>n_results</code> <code>int</code> <p>Maximum number of results.</p> <code>5</code> <code>memory_type</code> <code>MemoryType | None</code> <p>Optional filter by memory type.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[SearchResult]</code> <p>List of SearchResult objects.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def search_memories(\n    self,\n    query: str,\n    n_results: int = 5,\n    memory_type: MemoryType | None = None,\n) -&gt; list[SearchResult]:\n    \"\"\"Search memories by semantic similarity.\n\n    Args:\n        query: The search query text.\n        n_results: Maximum number of results.\n        memory_type: Optional filter by memory type.\n\n    Returns:\n        List of SearchResult objects.\n    \"\"\"\n    return await self.chroma_store.search(query, n_results, memory_type)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.get_memory","title":"<code>get_memory(memory_id)</code>  <code>async</code>","text":"<p>Get a specific memory by ID.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def get_memory(self, memory_id: str) -&gt; Memory | None:\n    \"\"\"Get a specific memory by ID.\"\"\"\n    return await self.chroma_store.get(memory_id)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.delete_memory","title":"<code>delete_memory(memory_id)</code>  <code>async</code>","text":"<p>Delete a memory by ID.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def delete_memory(self, memory_id: str) -&gt; bool:\n    \"\"\"Delete a memory by ID.\"\"\"\n    return await self.chroma_store.delete(memory_id)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.memory_count","title":"<code>memory_count()</code>  <code>async</code>","text":"<p>Get total number of memories stored.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def memory_count(self) -&gt; int:\n    \"\"\"Get total number of memories stored.\"\"\"\n    return await self.chroma_store.count()\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.get_profile","title":"<code>get_profile(user_id=None)</code>  <code>async</code>","text":"<p>Get the user profile.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str | None</code> <p>Optional user ID override.</p> <code>None</code> <p>Returns:</p> Type Description <code>UserProfile</code> <p>The UserProfile for the user.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def get_profile(self, user_id: str | None = None) -&gt; UserProfile:\n    \"\"\"Get the user profile.\n\n    Args:\n        user_id: Optional user ID override.\n\n    Returns:\n        The UserProfile for the user.\n    \"\"\"\n    return await self.sqlite_store.get_profile(user_id or self._current_user_id)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.save_profile","title":"<code>save_profile(profile)</code>  <code>async</code>","text":"<p>Save a user profile.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def save_profile(self, profile: UserProfile) -&gt; None:\n    \"\"\"Save a user profile.\"\"\"\n    await self.sqlite_store.save_profile(profile)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.update_preference","title":"<code>update_preference(key, value)</code>  <code>async</code>","text":"<p>Update a user preference.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The preference key.</p> required <code>value</code> <code>str</code> <p>The preference value.</p> required <p>Returns:</p> Type Description <code>UserProfile</code> <p>The updated UserProfile.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def update_preference(self, key: str, value: str) -&gt; UserProfile:\n    \"\"\"Update a user preference.\n\n    Args:\n        key: The preference key.\n        value: The preference value.\n\n    Returns:\n        The updated UserProfile.\n    \"\"\"\n    return await self.sqlite_store.update_preference(\n        key, value, self._current_user_id\n    )\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.get_last_session","title":"<code>get_last_session()</code>  <code>async</code>","text":"<p>Get the most recent completed session.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def get_last_session(self) -&gt; SessionSummary | None:\n    \"\"\"Get the most recent completed session.\"\"\"\n    return await self.sqlite_store.get_last_session(self._current_user_id)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.manager.MemoryManager.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Run cleanup on old memories and sessions.</p> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dict with counts of deleted items.</p> Source code in <code>src/reachy_agent/memory/manager.py</code> <pre><code>async def cleanup(self) -&gt; dict[str, int]:\n    \"\"\"Run cleanup on old memories and sessions.\n\n    Returns:\n        Dict with counts of deleted items.\n    \"\"\"\n    memories_deleted = await self.chroma_store.cleanup(self.retention_days)\n    sessions_deleted = await self.sqlite_store.cleanup_old_sessions(\n        self.retention_days\n    )\n\n    return {\n        \"memories_deleted\": memories_deleted,\n        \"sessions_deleted\": sessions_deleted,\n    }\n</code></pre>"},{"location":"api/memory/#types","title":"Types","text":""},{"location":"api/memory/#memory","title":"Memory","text":""},{"location":"api/memory/#reachy_agent.memory.types.Memory","title":"<code>Memory(id, content, memory_type, timestamp=datetime.now(), metadata=dict(), embedding=None)</code>  <code>dataclass</code>","text":"<p>A single memory item stored in ChromaDB.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for this memory</p> <code>content</code> <code>str</code> <p>The text content of the memory</p> <code>memory_type</code> <code>MemoryType</code> <p>Category of this memory</p> <code>timestamp</code> <code>datetime</code> <p>When this memory was created</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional key-value metadata</p> <code>embedding</code> <code>list[float] | None</code> <p>Optional pre-computed embedding vector</p>"},{"location":"api/memory/#reachy_agent.memory.types.Memory.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create Memory from dictionary.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; Memory:\n    \"\"\"Create Memory from dictionary.\"\"\"\n    return cls(\n        id=data[\"id\"],\n        content=data[\"content\"],\n        memory_type=MemoryType.from_string(data[\"memory_type\"]),\n        timestamp=datetime.fromisoformat(data[\"timestamp\"]),\n        metadata=data.get(\"metadata\", {}),\n    )\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.types.Memory.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary for storage.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary for storage.\"\"\"\n    return {\n        \"id\": self.id,\n        \"content\": self.content,\n        \"memory_type\": self.memory_type.value,\n        \"timestamp\": self.timestamp.isoformat(),\n        \"metadata\": self.metadata,\n    }\n</code></pre>"},{"location":"api/memory/#memorytype","title":"MemoryType","text":""},{"location":"api/memory/#reachy_agent.memory.types.MemoryType","title":"<code>MemoryType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Categories of memories stored in the system.</p> <p>Each type has different retention and retrieval characteristics: - conversation: Dialog exchanges, moderate retention - observation: Things seen/heard, short retention - fact: Learned facts, long retention - preference: User preferences, permanent - event: Calendar events, permanent until past - task: Tasks and reminders, permanent until completed</p>"},{"location":"api/memory/#reachy_agent.memory.types.MemoryType.from_string","title":"<code>from_string(value)</code>  <code>classmethod</code>","text":"<p>Parse a string into a MemoryType, case-insensitive.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; MemoryType:\n    \"\"\"Parse a string into a MemoryType, case-insensitive.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        # Default to fact for unknown types\n        return cls.FACT\n</code></pre>"},{"location":"api/memory/#userprofile","title":"UserProfile","text":""},{"location":"api/memory/#reachy_agent.memory.types.UserProfile","title":"<code>UserProfile</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>User profile stored in SQLite.</p> <p>Contains personalization data that persists across sessions: - Name and basic info - Preferences (key-value pairs) - Schedule patterns (natural language) - Connected services list</p>"},{"location":"api/memory/#reachy_agent.memory.types.UserProfile.from_db_dict","title":"<code>from_db_dict(data)</code>  <code>classmethod</code>","text":"<p>Create UserProfile from SQLite row.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>@classmethod\ndef from_db_dict(cls, data: dict[str, Any]) -&gt; UserProfile:\n    \"\"\"Create UserProfile from SQLite row.\"\"\"\n    return cls(\n        user_id=data[\"user_id\"],\n        name=data[\"name\"],\n        preferences=json.loads(data.get(\"preferences\", \"{}\")),\n        schedule_patterns=data.get(\"schedule_patterns\", \"\"),\n        connected_services=json.loads(data.get(\"connected_services\", \"[]\")),\n        created_at=datetime.fromisoformat(data[\"created_at\"]),\n        updated_at=datetime.fromisoformat(data[\"updated_at\"]),\n    )\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.types.UserProfile.get_preference","title":"<code>get_preference(key, default='')</code>","text":"<p>Get a preference value by key.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>def get_preference(self, key: str, default: str = \"\") -&gt; str:\n    \"\"\"Get a preference value by key.\"\"\"\n    return self.preferences.get(key, default)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.types.UserProfile.set_preference","title":"<code>set_preference(key, value)</code>","text":"<p>Set a preference value and update timestamp.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>def set_preference(self, key: str, value: str) -&gt; None:\n    \"\"\"Set a preference value and update timestamp.\"\"\"\n    self.preferences[key] = value\n    self.updated_at = datetime.now()\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.types.UserProfile.to_context_string","title":"<code>to_context_string()</code>","text":"<p>Format profile for injection into system prompt.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>def to_context_string(self) -&gt; str:\n    \"\"\"Format profile for injection into system prompt.\"\"\"\n    lines = [f\"- **Name**: {self.name}\"]\n\n    if self.preferences:\n        lines.append(\"- **Preferences**:\")\n        for key, value in self.preferences.items():\n            lines.append(f\"  - {key}: {value}\")\n\n    if self.schedule_patterns:\n        lines.append(f\"- **Schedule**: {self.schedule_patterns}\")\n\n    if self.connected_services:\n        services = \", \".join(self.connected_services)\n        lines.append(f\"- **Connected services**: {services}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.types.UserProfile.to_db_dict","title":"<code>to_db_dict()</code>","text":"<p>Convert to dictionary for SQLite storage.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>def to_db_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary for SQLite storage.\"\"\"\n    return {\n        \"user_id\": self.user_id,\n        \"name\": self.name,\n        \"preferences\": json.dumps(self.preferences),\n        \"schedule_patterns\": self.schedule_patterns,\n        \"connected_services\": json.dumps(self.connected_services),\n        \"created_at\": self.created_at.isoformat(),\n        \"updated_at\": self.updated_at.isoformat(),\n    }\n</code></pre>"},{"location":"api/memory/#sessionsummary","title":"SessionSummary","text":""},{"location":"api/memory/#reachy_agent.memory.types.SessionSummary","title":"<code>SessionSummary</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Summary of a conversation session.</p> <p>Stored in SQLite, one per session. Used for: - Injecting last session context - Continuity across conversations</p>"},{"location":"api/memory/#reachy_agent.memory.types.SessionSummary.from_db_dict","title":"<code>from_db_dict(data)</code>  <code>classmethod</code>","text":"<p>Create SessionSummary from SQLite row.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>@classmethod\ndef from_db_dict(cls, data: dict[str, Any]) -&gt; SessionSummary:\n    \"\"\"Create SessionSummary from SQLite row.\"\"\"\n    return cls(\n        session_id=data[\"session_id\"],\n        user_id=data[\"user_id\"],\n        start_time=datetime.fromisoformat(data[\"start_time\"]),\n        end_time=(\n            datetime.fromisoformat(data[\"end_time\"])\n            if data.get(\"end_time\")\n            else None\n        ),\n        summary_text=data.get(\"summary_text\", \"\"),\n        key_topics=json.loads(data.get(\"key_topics\", \"[]\")),\n        memory_count=data.get(\"memory_count\", 0),\n    )\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.types.SessionSummary.to_context_string","title":"<code>to_context_string()</code>","text":"<p>Format session for injection into system prompt.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>def to_context_string(self) -&gt; str:\n    \"\"\"Format session for injection into system prompt.\"\"\"\n    lines = []\n\n    if self.summary_text:\n        lines.append(f\"- **Summary**: {self.summary_text}\")\n\n    if self.end_time:\n        end_str = self.end_time.strftime(\"%Y-%m-%d %H:%M\")\n        lines.append(f\"- **Ended**: {end_str}\")\n\n    if self.key_topics:\n        topics = \", \".join(self.key_topics)\n        lines.append(f\"- **Key topics**: {topics}\")\n\n    return \"\\n\".join(lines) if lines else \"No previous session\"\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.types.SessionSummary.to_db_dict","title":"<code>to_db_dict()</code>","text":"<p>Convert to dictionary for SQLite storage.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>def to_db_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary for SQLite storage.\"\"\"\n    return {\n        \"session_id\": self.session_id,\n        \"user_id\": self.user_id,\n        \"start_time\": self.start_time.isoformat(),\n        \"end_time\": self.end_time.isoformat() if self.end_time else None,\n        \"summary_text\": self.summary_text,\n        \"key_topics\": json.dumps(self.key_topics),\n        \"memory_count\": self.memory_count,\n    }\n</code></pre>"},{"location":"api/memory/#searchresult","title":"SearchResult","text":""},{"location":"api/memory/#reachy_agent.memory.types.SearchResult","title":"<code>SearchResult(memory, score)</code>  <code>dataclass</code>","text":"<p>Result from a memory search query.</p> <p>Includes the memory and its similarity score for ranking.</p>"},{"location":"api/memory/#reachy_agent.memory.types.SearchResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary for API responses.</p> Source code in <code>src/reachy_agent/memory/types.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary for API responses.\"\"\"\n    return {\n        \"memory\": self.memory.to_dict(),\n        \"score\": self.score,\n    }\n</code></pre>"},{"location":"api/memory/#storage-backends","title":"Storage Backends","text":""},{"location":"api/memory/#chromadb-store","title":"ChromaDB Store","text":""},{"location":"api/memory/#reachy_agent.memory.storage.chroma_store.ChromaMemoryStore","title":"<code>ChromaMemoryStore(path, embedding_model='all-MiniLM-L6-v2')</code>","text":"<p>ChromaDB-backed storage for semantic memories.</p> <p>Provides storage and retrieval of memories using vector embeddings for semantic similarity search.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to ChromaDB persistence directory.</p> required <code>embedding_model</code> <code>str</code> <p>Name of sentence-transformers model to use.</p> <code>'all-MiniLM-L6-v2'</code> Example <p>store = ChromaMemoryStore(\"~/.reachy/memory/chroma\") await store.initialize() memory = await store.store(\"User prefers morning meetings\", MemoryType.PREFERENCE) results = await store.search(\"when does user like meetings\", n_results=5)</p> Source code in <code>src/reachy_agent/memory/storage/chroma_store.py</code> <pre><code>def __init__(\n    self,\n    path: str | Path,\n    embedding_model: str = \"all-MiniLM-L6-v2\",\n) -&gt; None:\n    self.path = Path(path).expanduser()\n    self.embedding_model = embedding_model\n    self._client: chromadb.PersistentClient | None = None\n    self._collection: Collection | None = None\n    self._embedding_service: EmbeddingService | None = None\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.chroma_store.ChromaMemoryStore.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize ChromaDB client and collection.</p> <p>Creates the persistence directory if it doesn't exist.</p> Source code in <code>src/reachy_agent/memory/storage/chroma_store.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Initialize ChromaDB client and collection.\n\n    Creates the persistence directory if it doesn't exist.\n    \"\"\"\n    import chromadb\n    from chromadb.config import Settings\n\n    self.path.mkdir(parents=True, exist_ok=True)\n\n    log.info(f\"Initializing ChromaDB at {self.path}\")\n    self._client = chromadb.PersistentClient(\n        path=str(self.path),\n        settings=Settings(\n            anonymized_telemetry=False,\n            allow_reset=True,\n        ),\n    )\n\n    self._collection = self._client.get_or_create_collection(\n        name=COLLECTION_NAME,\n        metadata={\"hnsw:space\": \"cosine\"},\n    )\n    log.info(f\"ChromaDB collection '{COLLECTION_NAME}' ready\")\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.chroma_store.ChromaMemoryStore.store","title":"<code>store(content, memory_type, metadata=None)</code>  <code>async</code>","text":"<p>Store a new memory with its embedding.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The text content to store.</p> required <code>memory_type</code> <code>MemoryType</code> <p>Category of this memory.</p> required <code>metadata</code> <code>dict | None</code> <p>Optional additional metadata.</p> <code>None</code> <p>Returns:</p> Type Description <code>Memory</code> <p>The created Memory object.</p> Source code in <code>src/reachy_agent/memory/storage/chroma_store.py</code> <pre><code>async def store(\n    self,\n    content: str,\n    memory_type: MemoryType,\n    metadata: dict | None = None,\n) -&gt; Memory:\n    \"\"\"Store a new memory with its embedding.\n\n    Args:\n        content: The text content to store.\n        memory_type: Category of this memory.\n        metadata: Optional additional metadata.\n\n    Returns:\n        The created Memory object.\n    \"\"\"\n    memory_id = str(uuid.uuid4())\n    timestamp = datetime.now()\n\n    # Generate embedding\n    embedding = self.embedding_service.embed(content)\n\n    # Prepare metadata for ChromaDB (must be flat key-value)\n    chroma_metadata = {\n        \"memory_type\": memory_type.value,\n        \"timestamp\": timestamp.isoformat(),\n        **(metadata or {}),\n    }\n\n    # Store in ChromaDB\n    self.collection.add(\n        ids=[memory_id],\n        embeddings=[embedding],\n        documents=[content],\n        metadatas=[chroma_metadata],\n    )\n\n    memory = Memory(\n        id=memory_id,\n        content=content,\n        memory_type=memory_type,\n        timestamp=timestamp,\n        metadata=metadata or {},\n        embedding=embedding,\n    )\n\n    log.debug(f\"Stored memory {memory_id}: {content[:50]}...\")\n    return memory\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.chroma_store.ChromaMemoryStore.search","title":"<code>search(query, n_results=5, memory_type=None)</code>  <code>async</code>","text":"<p>Search memories by semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query text.</p> required <code>n_results</code> <code>int</code> <p>Maximum number of results to return.</p> <code>5</code> <code>memory_type</code> <code>MemoryType | None</code> <p>Optional filter by memory type.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[SearchResult]</code> <p>List of SearchResult objects sorted by similarity.</p> Source code in <code>src/reachy_agent/memory/storage/chroma_store.py</code> <pre><code>async def search(\n    self,\n    query: str,\n    n_results: int = 5,\n    memory_type: MemoryType | None = None,\n) -&gt; list[SearchResult]:\n    \"\"\"Search memories by semantic similarity.\n\n    Args:\n        query: The search query text.\n        n_results: Maximum number of results to return.\n        memory_type: Optional filter by memory type.\n\n    Returns:\n        List of SearchResult objects sorted by similarity.\n    \"\"\"\n    # Generate query embedding\n    query_embedding = self.embedding_service.embed(query)\n\n    # Build where filter if memory_type specified\n    where_filter = None\n    if memory_type is not None:\n        where_filter = {\"memory_type\": memory_type.value}\n\n    # Search ChromaDB\n    results = self.collection.query(\n        query_embeddings=[query_embedding],\n        n_results=n_results,\n        where=where_filter,\n        include=[\"documents\", \"metadatas\", \"distances\"],\n    )\n\n    # Convert to SearchResult objects\n    search_results = []\n    if results[\"ids\"] and results[\"ids\"][0]:\n        for i, memory_id in enumerate(results[\"ids\"][0]):\n            content = results[\"documents\"][0][i] if results[\"documents\"] else \"\"\n            metadata = results[\"metadatas\"][0][i] if results[\"metadatas\"] else {}\n            distance = results[\"distances\"][0][i] if results[\"distances\"] else 1.0\n\n            # Convert distance to similarity score (0.0 to 1.0)\n            # ChromaDB cosine distance ranges from 0 (identical) to 2 (opposite)\n            # Formula: similarity = 1 - (distance / 2), clamped to [0, 1]\n            # This gives: distance=0 -&gt; similarity=1.0, distance=2 -&gt; similarity=0.0\n            similarity = max(0.0, min(1.0, 1.0 - (distance / 2.0)))\n\n            memory = Memory(\n                id=memory_id,\n                content=content,\n                memory_type=MemoryType.from_string(\n                    metadata.get(\"memory_type\", \"fact\")\n                ),\n                timestamp=datetime.fromisoformat(\n                    metadata.get(\"timestamp\", datetime.now().isoformat())\n                ),\n                metadata={\n                    k: v for k, v in metadata.items() if k not in (\"memory_type\", \"timestamp\")\n                },\n            )\n\n            search_results.append(SearchResult(memory=memory, score=similarity))\n\n    return search_results\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.chroma_store.ChromaMemoryStore.get","title":"<code>get(memory_id)</code>  <code>async</code>","text":"<p>Retrieve a specific memory by ID.</p> <p>Parameters:</p> Name Type Description Default <code>memory_id</code> <code>str</code> <p>The unique identifier of the memory.</p> required <p>Returns:</p> Type Description <code>Memory | None</code> <p>The Memory object if found, None otherwise.</p> Source code in <code>src/reachy_agent/memory/storage/chroma_store.py</code> <pre><code>async def get(self, memory_id: str) -&gt; Memory | None:\n    \"\"\"Retrieve a specific memory by ID.\n\n    Args:\n        memory_id: The unique identifier of the memory.\n\n    Returns:\n        The Memory object if found, None otherwise.\n    \"\"\"\n    result = self.collection.get(\n        ids=[memory_id],\n        include=[\"documents\", \"metadatas\"],\n    )\n\n    if not result[\"ids\"]:\n        return None\n\n    content = result[\"documents\"][0] if result[\"documents\"] else \"\"\n    metadata = result[\"metadatas\"][0] if result[\"metadatas\"] else {}\n\n    return Memory(\n        id=memory_id,\n        content=content,\n        memory_type=MemoryType.from_string(metadata.get(\"memory_type\", \"fact\")),\n        timestamp=datetime.fromisoformat(\n            metadata.get(\"timestamp\", datetime.now().isoformat())\n        ),\n        metadata={\n            k: v for k, v in metadata.items() if k not in (\"memory_type\", \"timestamp\")\n        },\n    )\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.chroma_store.ChromaMemoryStore.delete","title":"<code>delete(memory_id)</code>  <code>async</code>","text":"<p>Delete a memory by ID.</p> <p>Parameters:</p> Name Type Description Default <code>memory_id</code> <code>str</code> <p>The unique identifier of the memory to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False if not found.</p> Source code in <code>src/reachy_agent/memory/storage/chroma_store.py</code> <pre><code>async def delete(self, memory_id: str) -&gt; bool:\n    \"\"\"Delete a memory by ID.\n\n    Args:\n        memory_id: The unique identifier of the memory to delete.\n\n    Returns:\n        True if deleted, False if not found.\n    \"\"\"\n    try:\n        self.collection.delete(ids=[memory_id])\n        log.debug(f\"Deleted memory {memory_id}\")\n        return True\n    except Exception as e:\n        log.warning(f\"Failed to delete memory {memory_id}: {e}\")\n        return False\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.chroma_store.ChromaMemoryStore.count","title":"<code>count()</code>  <code>async</code>","text":"<p>Get total number of memories stored.</p> Source code in <code>src/reachy_agent/memory/storage/chroma_store.py</code> <pre><code>async def count(self) -&gt; int:\n    \"\"\"Get total number of memories stored.\"\"\"\n    return self.collection.count()\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.chroma_store.ChromaMemoryStore.cleanup","title":"<code>cleanup(retention_days)</code>  <code>async</code>","text":"<p>Remove memories older than retention period.</p> <p>Parameters:</p> Name Type Description Default <code>retention_days</code> <code>int</code> <p>Number of days to retain memories.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of memories deleted.</p> Source code in <code>src/reachy_agent/memory/storage/chroma_store.py</code> <pre><code>async def cleanup(self, retention_days: int) -&gt; int:\n    \"\"\"Remove memories older than retention period.\n\n    Args:\n        retention_days: Number of days to retain memories.\n\n    Returns:\n        Number of memories deleted.\n    \"\"\"\n    cutoff = datetime.now() - timedelta(days=retention_days)\n    cutoff_str = cutoff.isoformat()\n\n    # Get all memories older than cutoff\n    results = self.collection.get(\n        where={\"timestamp\": {\"$lt\": cutoff_str}},\n        include=[\"metadatas\"],\n    )\n\n    if not results[\"ids\"]:\n        return 0\n\n    # Delete old memories\n    self.collection.delete(ids=results[\"ids\"])\n    count = len(results[\"ids\"])\n    log.info(f\"Cleaned up {count} memories older than {retention_days} days\")\n    return count\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.chroma_store.ChromaMemoryStore.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the ChromaDB client.</p> <p>ChromaDB PersistentClient handles cleanup automatically, so this method clears references for consistency.</p> Source code in <code>src/reachy_agent/memory/storage/chroma_store.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the ChromaDB client.\n\n    ChromaDB PersistentClient handles cleanup automatically,\n    so this method clears references for consistency.\n    \"\"\"\n    if self._client is None and self._collection is None:\n        log.debug(\"ChromaDB store already closed\")\n        return\n\n    self._client = None\n    self._collection = None\n    self._embedding_service = None\n    log.info(\"ChromaDB store closed\")\n</code></pre>"},{"location":"api/memory/#sqlite-store","title":"SQLite Store","text":""},{"location":"api/memory/#reachy_agent.memory.storage.sqlite_store.SQLiteProfileStore","title":"<code>SQLiteProfileStore(path)</code>","text":"<p>SQLite-backed storage for user profiles and sessions.</p> <p>Provides CRUD operations for user profiles and session summaries. Data is stored in a local SQLite database for persistence.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the SQLite database file.</p> required Example <p>store = SQLiteProfileStore(\"~/.reachy/memory/reachy.db\") await store.initialize() profile = await store.get_profile(\"default\") profile.name = \"John\" await store.save_profile(profile)</p> Source code in <code>src/reachy_agent/memory/storage/sqlite_store.py</code> <pre><code>def __init__(self, path: str | Path) -&gt; None:\n    self.path = Path(path).expanduser()\n    self._initialized = False\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.sqlite_store.SQLiteProfileStore.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize the database schema.</p> <p>Creates the database file and tables if they don't exist.</p> Source code in <code>src/reachy_agent/memory/storage/sqlite_store.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Initialize the database schema.\n\n    Creates the database file and tables if they don't exist.\n    \"\"\"\n    self.path.parent.mkdir(parents=True, exist_ok=True)\n\n    def _init_db() -&gt; None:\n        with self._get_connection() as conn:\n            conn.executescript(SCHEMA)\n            conn.commit()\n\n    log.info(f\"Initializing SQLite database at {self.path}\")\n    await self._run_sync(_init_db)\n\n    self._initialized = True\n    log.info(\"SQLite profile store ready\")\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.sqlite_store.SQLiteProfileStore.get_profile","title":"<code>get_profile(user_id='default')</code>  <code>async</code>","text":"<p>Get a user profile, creating default if not exists.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>The user identifier.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>UserProfile</code> <p>The UserProfile for the given user.</p> Source code in <code>src/reachy_agent/memory/storage/sqlite_store.py</code> <pre><code>async def get_profile(self, user_id: str = \"default\") -&gt; UserProfile:\n    \"\"\"Get a user profile, creating default if not exists.\n\n    Args:\n        user_id: The user identifier.\n\n    Returns:\n        The UserProfile for the given user.\n    \"\"\"\n\n    def _fetch() -&gt; dict | None:\n        with self._get_connection() as conn:\n            cursor = conn.execute(\n                \"SELECT * FROM user_profiles WHERE user_id = ?\",\n                (user_id,),\n            )\n            row = cursor.fetchone()\n            return dict(row) if row else None\n\n    row_dict = await self._run_sync(_fetch)\n\n    if row_dict is None:\n        # Create default profile\n        profile = UserProfile(user_id=user_id)\n        await self.save_profile(profile)\n        return profile\n\n    return UserProfile.from_db_dict(row_dict)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.sqlite_store.SQLiteProfileStore.save_profile","title":"<code>save_profile(profile)</code>  <code>async</code>","text":"<p>Save or update a user profile.</p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>UserProfile</code> <p>The UserProfile to save.</p> required Source code in <code>src/reachy_agent/memory/storage/sqlite_store.py</code> <pre><code>async def save_profile(self, profile: UserProfile) -&gt; None:\n    \"\"\"Save or update a user profile.\n\n    Args:\n        profile: The UserProfile to save.\n    \"\"\"\n    profile.updated_at = datetime.now()\n    data = profile.to_db_dict()\n\n    def _save() -&gt; None:\n        with self._get_connection() as conn:\n            conn.execute(\n                \"\"\"\n                INSERT INTO user_profiles\n                    (user_id, name, preferences, schedule_patterns,\n                     connected_services, created_at, updated_at)\n                VALUES\n                    (:user_id, :name, :preferences, :schedule_patterns,\n                     :connected_services, :created_at, :updated_at)\n                ON CONFLICT(user_id) DO UPDATE SET\n                    name = excluded.name,\n                    preferences = excluded.preferences,\n                    schedule_patterns = excluded.schedule_patterns,\n                    connected_services = excluded.connected_services,\n                    updated_at = excluded.updated_at\n                \"\"\",\n                data,\n            )\n            conn.commit()\n\n    await self._run_sync(_save)\n    log.debug(f\"Saved profile for user {profile.user_id}\")\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.sqlite_store.SQLiteProfileStore.update_preference","title":"<code>update_preference(key, value, user_id='default')</code>  <code>async</code>","text":"<p>Update a single preference for a user.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The preference key.</p> required <code>value</code> <code>str</code> <p>The preference value.</p> required <code>user_id</code> <code>str</code> <p>The user identifier.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>UserProfile</code> <p>The updated UserProfile.</p> Source code in <code>src/reachy_agent/memory/storage/sqlite_store.py</code> <pre><code>async def update_preference(\n    self,\n    key: str,\n    value: str,\n    user_id: str = \"default\",\n) -&gt; UserProfile:\n    \"\"\"Update a single preference for a user.\n\n    Args:\n        key: The preference key.\n        value: The preference value.\n        user_id: The user identifier.\n\n    Returns:\n        The updated UserProfile.\n    \"\"\"\n    profile = await self.get_profile(user_id)\n    profile.set_preference(key, value)\n    await self.save_profile(profile)\n    return profile\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.sqlite_store.SQLiteProfileStore.save_session","title":"<code>save_session(session)</code>  <code>async</code>","text":"<p>Save or update a session summary.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionSummary</code> <p>The SessionSummary to save.</p> required Source code in <code>src/reachy_agent/memory/storage/sqlite_store.py</code> <pre><code>async def save_session(self, session: SessionSummary) -&gt; None:\n    \"\"\"Save or update a session summary.\n\n    Args:\n        session: The SessionSummary to save.\n    \"\"\"\n    data = session.to_db_dict()\n\n    def _save() -&gt; None:\n        with self._get_connection() as conn:\n            conn.execute(\n                \"\"\"\n                INSERT INTO sessions\n                    (session_id, user_id, start_time, end_time,\n                     summary_text, key_topics, memory_count)\n                VALUES\n                    (:session_id, :user_id, :start_time, :end_time,\n                     :summary_text, :key_topics, :memory_count)\n                ON CONFLICT(session_id) DO UPDATE SET\n                    end_time = excluded.end_time,\n                    summary_text = excluded.summary_text,\n                    key_topics = excluded.key_topics,\n                    memory_count = excluded.memory_count\n                \"\"\",\n                data,\n            )\n            conn.commit()\n\n    await self._run_sync(_save)\n    log.debug(f\"Saved session {session.session_id}\")\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.sqlite_store.SQLiteProfileStore.get_last_session","title":"<code>get_last_session(user_id='default')</code>  <code>async</code>","text":"<p>Get the most recent completed session for a user.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>The user identifier.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>SessionSummary | None</code> <p>The most recent SessionSummary with an end_time, or None.</p> Source code in <code>src/reachy_agent/memory/storage/sqlite_store.py</code> <pre><code>async def get_last_session(self, user_id: str = \"default\") -&gt; SessionSummary | None:\n    \"\"\"Get the most recent completed session for a user.\n\n    Args:\n        user_id: The user identifier.\n\n    Returns:\n        The most recent SessionSummary with an end_time, or None.\n    \"\"\"\n\n    def _fetch() -&gt; dict | None:\n        with self._get_connection() as conn:\n            cursor = conn.execute(\n                \"\"\"\n                SELECT * FROM sessions\n                WHERE user_id = ? AND end_time IS NOT NULL\n                ORDER BY end_time DESC\n                LIMIT 1\n                \"\"\",\n                (user_id,),\n            )\n            row = cursor.fetchone()\n            return dict(row) if row else None\n\n    row_dict = await self._run_sync(_fetch)\n    if row_dict is None:\n        return None\n    return SessionSummary.from_db_dict(row_dict)\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.storage.sqlite_store.SQLiteProfileStore.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the store.</p> <p>SQLite connections are managed per-operation via context manager, so this method primarily updates state for consistency with other stores.</p> Source code in <code>src/reachy_agent/memory/storage/sqlite_store.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the store.\n\n    SQLite connections are managed per-operation via context manager,\n    so this method primarily updates state for consistency with other stores.\n    \"\"\"\n    if not self._initialized:\n        log.debug(\"SQLite profile store already closed\")\n        return\n\n    self._initialized = False\n    log.info(\"SQLite profile store closed\")\n</code></pre>"},{"location":"api/memory/#embeddings","title":"Embeddings","text":""},{"location":"api/memory/#reachy_agent.memory.embeddings.EmbeddingService","title":"<code>EmbeddingService(model_name=DEFAULT_MODEL)</code>","text":"<p>Service for generating text embeddings.</p> <p>Lazily loads the sentence-transformers model on first use to avoid slow startup times when embeddings aren't immediately needed.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the sentence-transformers model to use. Defaults to all-MiniLM-L6-v2 (384 dimensions, fast).</p> <code>DEFAULT_MODEL</code> Example <p>service = EmbeddingService() embedding = service.embed(\"Hello world\") len(embedding) 384</p> Source code in <code>src/reachy_agent/memory/embeddings.py</code> <pre><code>def __init__(self, model_name: str = DEFAULT_MODEL) -&gt; None:\n    self.model_name = model_name\n    self._model: SentenceTransformer | None = None\n    self._dimension: int | None = None\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.embeddings.EmbeddingService.embed","title":"<code>embed(text)</code>","text":"<p>Generate embedding for a single text string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to embed.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>List of floats representing the embedding vector.</p> Source code in <code>src/reachy_agent/memory/embeddings.py</code> <pre><code>def embed(self, text: str) -&gt; list[float]:\n    \"\"\"Generate embedding for a single text string.\n\n    Args:\n        text: The text to embed.\n\n    Returns:\n        List of floats representing the embedding vector.\n    \"\"\"\n    embedding = self.model.encode(text, convert_to_numpy=True)\n    return embedding.tolist()\n</code></pre>"},{"location":"api/memory/#reachy_agent.memory.embeddings.EmbeddingService.embed_batch","title":"<code>embed_batch(texts)</code>","text":"<p>Generate embeddings for multiple texts efficiently.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str]</code> <p>List of texts to embed.</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>List of embedding vectors, one per input text.</p> Source code in <code>src/reachy_agent/memory/embeddings.py</code> <pre><code>def embed_batch(self, texts: list[str]) -&gt; list[list[float]]:\n    \"\"\"Generate embeddings for multiple texts efficiently.\n\n    Args:\n        texts: List of texts to embed.\n\n    Returns:\n        List of embedding vectors, one per input text.\n    \"\"\"\n    if not texts:\n        return []\n\n    embeddings = self.model.encode(\n        texts,\n        convert_to_numpy=True,\n        show_progress_bar=len(texts) &gt; 10,\n    )\n    return [emb.tolist() for emb in embeddings]\n</code></pre>"},{"location":"api/memory/#context-builder","title":"Context Builder","text":""},{"location":"api/memory/#reachy_agent.memory.context_builder","title":"<code>context_builder</code>","text":"<p>Context builder for memory injection.</p> <p>Builds context strings for injection into Claude's system prompt, providing personalized context without explicit tool calls.</p>"},{"location":"api/memory/#reachy_agent.memory.context_builder.build_memory_context","title":"<code>build_memory_context(profile=None, last_session=None)</code>","text":"<p>Convenience function to build memory context.</p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>UserProfile | None</code> <p>User profile to include.</p> <code>None</code> <code>last_session</code> <code>SessionSummary | None</code> <p>Last session summary to include.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted markdown context string.</p> Source code in <code>src/reachy_agent/memory/context_builder.py</code> <pre><code>def build_memory_context(\n    profile: UserProfile | None = None,\n    last_session: SessionSummary | None = None,\n) -&gt; str:\n    \"\"\"Convenience function to build memory context.\n\n    Args:\n        profile: User profile to include.\n        last_session: Last session summary to include.\n\n    Returns:\n        Formatted markdown context string.\n    \"\"\"\n    builder = MemoryContextBuilder()\n    return builder.build(profile, last_session)\n</code></pre>"},{"location":"api/memory/#usage-example","title":"Usage Example","text":"<pre><code>from reachy_agent.memory.manager import MemoryManager\nfrom reachy_agent.memory.types import MemoryType\n\n# Initialize\nmanager = MemoryManager(\n    chroma_path=\"~/.reachy/memory/chroma\",\n    sqlite_path=\"~/.reachy/memory/reachy.db\",\n)\nawait manager.initialize()\n\n# Session management\nawait manager.start_session(user_id=\"alice\")\n\n# Store memories\nawait manager.store_memory(\n    \"User prefers morning meetings\",\n    MemoryType.PREFERENCE\n)\n\n# Search memories\nresults = await manager.search_memories(\"meeting preferences\")\nfor result in results:\n    print(f\"{result.memory.content} (score: {result.score})\")\n\n# Profile operations\nprofile = await manager.get_profile()\nawait manager.update_preference(\"greeting\", \"informal\")\n\n# Cleanup and close\nawait manager.end_session(\n    summary_text=\"Discussed scheduling preferences\",\n    key_topics=[\"meetings\", \"calendar\"]\n)\nawait manager.close()\n</code></pre>"},{"location":"api/memory/#configuration","title":"Configuration","text":"<p>Memory is configured in <code>config/default.yaml</code>:</p> <pre><code>memory:\n  chroma_path: ~/.reachy/memory/chroma\n  sqlite_path: ~/.reachy/memory/reachy.db\n  embedding_model: all-MiniLM-L6-v2\n  max_memories: 10000\n  retention_days: 90\n</code></pre>"},{"location":"api/memory/#related-documentation","title":"Related Documentation","text":"<ul> <li>Memory System Architecture</li> </ul>"},{"location":"api/motion-blending/","title":"Motion Blending API","text":"<p>The motion blending system orchestrates multiple motion sources to create natural, lifelike robot movements. It uses a priority-based composition model where primary motions are exclusive and secondary motions are additive overlays.</p>"},{"location":"api/motion-blending/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TB\n    subgraph Controller[\"MotionBlendController\"]\n        direction TB\n        Loop[\"100Hz control loop\"]\n        Commands[\"20Hz daemon commands\"]\n    end\n\n    subgraph Primary[\"Primary Sources (exclusive)\"]\n        direction LR\n        Breathing[\"BreathingMotion&lt;br/&gt;Z-axis + antennas\"]\n        Idle[\"IdleBehavior&lt;br/&gt;look-around\"]\n    end\n\n    subgraph Secondary[\"Secondary Sources (additive)\"]\n        Wobble[\"HeadWobble&lt;br/&gt;speech animation\"]\n    end\n\n    Controller --&gt; Primary\n    Controller --&gt; Secondary\n    Primary --&gt; Compose[\"Compose Pose\"]\n    Secondary --&gt; Compose\n    Compose --&gt; Daemon[\"ReachyDaemonClient&lt;br/&gt;look_at, set_antenna_state\"]\n\n    style Controller fill:#7c3aed,color:#fff\n    style Primary fill:#a78bfa,color:#000\n    style Secondary fill:#c4b5fd,color:#000\n    style Daemon fill:#4c1d95,color:#fff</code></pre>"},{"location":"api/motion-blending/#core-types","title":"Core Types","text":""},{"location":"api/motion-blending/#headpose","title":"HeadPose","text":"<p>Complete pose snapshot representing all controllable head axes.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.HeadPose","title":"<code>HeadPose(pitch=0.0, yaw=0.0, roll=0.0, z=0.0, left_antenna=90.0, right_antenna=90.0, timestamp=datetime.now())</code>  <code>dataclass</code>","text":"<p>Complete head pose snapshot.</p> <p>All angles in degrees. Convention: - pitch: positive = look up - yaw: positive = look left - roll: positive = tilt right (from robot's perspective) - z: vertical offset in millimeters</p> <p>Antenna angles: 0 = flat/back, 90 = vertical (straight up)</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.HeadPose.neutral","title":"<code>neutral()</code>  <code>classmethod</code>","text":"<p>Return a neutral (center) pose with antennas vertical.</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>@classmethod\ndef neutral(cls) -&gt; HeadPose:\n    \"\"\"Return a neutral (center) pose with antennas vertical.\"\"\"\n    return cls(pitch=0.0, yaw=0.0, roll=0.0, z=0.0, left_antenna=90.0, right_antenna=90.0)\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.HeadPose.lerp","title":"<code>lerp(target, t)</code>","text":"<p>Linear interpolation toward target pose.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>HeadPose</code> <p>Target pose to interpolate toward.</p> required <code>t</code> <code>float</code> <p>Interpolation factor (0.0 = self, 1.0 = target).</p> required <p>Returns:</p> Type Description <code>HeadPose</code> <p>New HeadPose interpolated between self and target.</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>def lerp(self, target: HeadPose, t: float) -&gt; HeadPose:\n    \"\"\"Linear interpolation toward target pose.\n\n    Args:\n        target: Target pose to interpolate toward.\n        t: Interpolation factor (0.0 = self, 1.0 = target).\n\n    Returns:\n        New HeadPose interpolated between self and target.\n    \"\"\"\n    t = max(0.0, min(1.0, t))\n    return HeadPose(\n        pitch=self.pitch + (target.pitch - self.pitch) * t,\n        yaw=self.yaw + (target.yaw - self.yaw) * t,\n        roll=self.roll + (target.roll - self.roll) * t,\n        z=self.z + (target.z - self.z) * t,\n        left_antenna=self.left_antenna + (target.left_antenna - self.left_antenna) * t,\n        right_antenna=self.right_antenna + (target.right_antenna - self.right_antenna) * t,\n    )\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.HeadPose.clamp","title":"<code>clamp(limits)</code>","text":"<p>Clamp pose to safety limits.</p> <p>Parameters:</p> Name Type Description Default <code>limits</code> <code>PoseLimits</code> <p>PoseLimits defining safe ranges.</p> required <p>Returns:</p> Type Description <code>HeadPose</code> <p>New HeadPose with values clamped to limits.</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>def clamp(self, limits: PoseLimits) -&gt; HeadPose:\n    \"\"\"Clamp pose to safety limits.\n\n    Args:\n        limits: PoseLimits defining safe ranges.\n\n    Returns:\n        New HeadPose with values clamped to limits.\n    \"\"\"\n\n    def _clamp(value: float, range_tuple: tuple[float, float]) -&gt; float:\n        return max(range_tuple[0], min(range_tuple[1], value))\n\n    return HeadPose(\n        pitch=_clamp(self.pitch, limits.pitch_range),\n        yaw=_clamp(self.yaw, limits.yaw_range),\n        roll=_clamp(self.roll, limits.roll_range),\n        z=_clamp(self.z, limits.z_range),\n        left_antenna=_clamp(self.left_antenna, limits.antenna_range),\n        right_antenna=_clamp(self.right_antenna, limits.antenna_range),\n    )\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.HeadPose.__add__","title":"<code>__add__(offset)</code>","text":"<p>Add offset to create new pose.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>PoseOffset</code> <p>PoseOffset to add.</p> required <p>Returns:</p> Type Description <code>HeadPose</code> <p>New HeadPose with offset applied.</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>def __add__(self, offset: PoseOffset) -&gt; HeadPose:\n    \"\"\"Add offset to create new pose.\n\n    Args:\n        offset: PoseOffset to add.\n\n    Returns:\n        New HeadPose with offset applied.\n    \"\"\"\n    return HeadPose(\n        pitch=self.pitch + offset.pitch,\n        yaw=self.yaw + offset.yaw,\n        roll=self.roll + offset.roll,\n        z=self.z + offset.z,\n        left_antenna=self.left_antenna + offset.left_antenna,\n        right_antenna=self.right_antenna + offset.right_antenna,\n    )\n</code></pre>"},{"location":"api/motion-blending/#poseoffset","title":"PoseOffset","text":"<p>Delta values for secondary (additive) motions.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.PoseOffset","title":"<code>PoseOffset(pitch=0.0, yaw=0.0, roll=0.0, z=0.0, left_antenna=0.0, right_antenna=0.0, generation=0)</code>  <code>dataclass</code>","text":"<p>Delta values to add to a base pose.</p> <p>Used by secondary motion sources (wobble, face tracking) to overlay motion on top of primary poses.</p> <p>All angles in degrees, z in millimeters.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.PoseOffset.__add__","title":"<code>__add__(other)</code>","text":"<p>Add two offsets together.</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>def __add__(self, other: PoseOffset) -&gt; PoseOffset:\n    \"\"\"Add two offsets together.\"\"\"\n    return PoseOffset(\n        pitch=self.pitch + other.pitch,\n        yaw=self.yaw + other.yaw,\n        roll=self.roll + other.roll,\n        z=self.z + other.z,\n        left_antenna=self.left_antenna + other.left_antenna,\n        right_antenna=self.right_antenna + other.right_antenna,\n        generation=max(self.generation, other.generation),\n    )\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.PoseOffset.scale","title":"<code>scale(factor)</code>","text":"<p>Scale offset by factor.</p> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>float</code> <p>Scaling factor (0.0 to 1.0 typical).</p> required <p>Returns:</p> Type Description <code>PoseOffset</code> <p>New PoseOffset with scaled values.</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>def scale(self, factor: float) -&gt; PoseOffset:\n    \"\"\"Scale offset by factor.\n\n    Args:\n        factor: Scaling factor (0.0 to 1.0 typical).\n\n    Returns:\n        New PoseOffset with scaled values.\n    \"\"\"\n    return PoseOffset(\n        pitch=self.pitch * factor,\n        yaw=self.yaw * factor,\n        roll=self.roll * factor,\n        z=self.z * factor,\n        left_antenna=self.left_antenna * factor,\n        right_antenna=self.right_antenna * factor,\n        generation=self.generation,\n    )\n</code></pre>"},{"location":"api/motion-blending/#poselimits","title":"PoseLimits","text":"<p>Safety bounds for all pose axes.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.PoseLimits","title":"<code>PoseLimits(pitch_range=(-45.0, 45.0), yaw_range=(-45.0, 45.0), roll_range=(-30.0, 30.0), z_range=(-50.0, 50.0), antenna_range=(0.0, 90.0))</code>  <code>dataclass</code>","text":"<p>Safety limits for pose values.</p> <p>All angles in degrees, z in millimeters.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.PoseLimits.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create from dictionary.</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; PoseLimits:\n    \"\"\"Create from dictionary.\"\"\"\n    return cls(**{k: v for k, v in data.items() if hasattr(cls, k)})\n</code></pre>"},{"location":"api/motion-blending/#motionpriority","title":"MotionPriority","text":"<p>Enum defining motion source priority levels.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.MotionPriority","title":"<code>MotionPriority</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Motion priority levels.</p> Exclusive motions - only one can be active at a time <p>(breathing, idle look-around, emotions, dances)</p> <p>SECONDARY: Additive motions - overlay on top of primary            (speech wobble, face tracking)</p>"},{"location":"api/motion-blending/#motionsource-protocol","title":"MotionSource Protocol","text":"<p>Protocol that all motion sources must implement.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.MotionSource","title":"<code>MotionSource</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for motion sources (breathing, wobble, idle, etc.).</p> <p>Motion sources provide contributions to the final robot pose. Primary sources return HeadPose (exclusive), secondary return PoseOffset (additive).</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.MotionSource.is_active","title":"<code>is_active</code>  <code>property</code>","text":"<p>Whether motion source is currently active.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.MotionSource.priority","title":"<code>priority</code>  <code>property</code>","text":"<p>Motion priority level (PRIMARY or SECONDARY).</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.MotionSource.get_contribution","title":"<code>get_contribution(base_pose)</code>  <code>async</code>","text":"<p>Get this source's contribution to the final pose.</p> <p>Parameters:</p> Name Type Description Default <code>base_pose</code> <code>HeadPose</code> <p>Current base pose for reference.</p> required <p>Returns:</p> Type Description <code>MotionContribution</code> <p>HeadPose for PRIMARY priority (replaces base)</p> <code>MotionContribution</code> <p>PoseOffset for SECONDARY priority (adds to base)</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>async def get_contribution(self, base_pose: HeadPose) -&gt; MotionContribution:\n    \"\"\"Get this source's contribution to the final pose.\n\n    Args:\n        base_pose: Current base pose for reference.\n\n    Returns:\n        HeadPose for PRIMARY priority (replaces base)\n        PoseOffset for SECONDARY priority (adds to base)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.MotionSource.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the motion source.</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start the motion source.\"\"\"\n    ...\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.motion_types.MotionSource.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the motion source.</p> Source code in <code>src/reachy_agent/behaviors/motion_types.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop the motion source.\"\"\"\n    ...\n</code></pre>"},{"location":"api/motion-blending/#motionblendcontroller","title":"MotionBlendController","text":"<p>The central orchestrator that composes motion sources and sends commands to the daemon.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController","title":"<code>MotionBlendController(config=None, send_pose_callback=None, sdk_client=None)</code>","text":"<p>Orchestrates motion sources and sends composed poses to the daemon.</p> <p>The controller maintains: - A registry of motion sources (primary and secondary) - The currently active primary source - A configurable control loop (default 30Hz, config may set 100Hz) - A configurable rate limiter for daemon commands (default 15Hz, config: 20Hz)</p> Example <p>config = BlendControllerConfig() controller = MotionBlendController(     config,     send_pose_callback=daemon_client.send_pose )</p> <p>Initialize the blend controller.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BlendControllerConfig | None</code> <p>Controller configuration. Uses defaults if not provided.</p> <code>None</code> <code>send_pose_callback</code> <code>Callable[[HeadPose], Any] | None</code> <p>Async callback to send poses via HTTP daemon.                 Used as fallback if SDK is unavailable.</p> <code>None</code> <code>sdk_client</code> <code>ReachySDKClient | None</code> <p>Optional SDK client for direct motion control.         Preferred over HTTP callback when available.</p> <code>None</code> Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>def __init__(\n    self,\n    config: BlendControllerConfig | None = None,\n    send_pose_callback: Callable[[HeadPose], Any] | None = None,\n    sdk_client: ReachySDKClient | None = None,\n) -&gt; None:\n    \"\"\"Initialize the blend controller.\n\n    Args:\n        config: Controller configuration. Uses defaults if not provided.\n        send_pose_callback: Async callback to send poses via HTTP daemon.\n                            Used as fallback if SDK is unavailable.\n        sdk_client: Optional SDK client for direct motion control.\n                    Preferred over HTTP callback when available.\n    \"\"\"\n    self.config = config or BlendControllerConfig()\n    self._send_pose = send_pose_callback\n    self._sdk_client = sdk_client\n\n    # Motion source registry\n    self._sources: dict[str, MotionSource] = {}\n    self._active_primary: str | None = None\n    self._active_secondaries: set[str] = set()\n\n    # Control loop state\n    self._running = False\n    self._task: asyncio.Task[None] | None = None\n    self._current_pose = HeadPose.neutral()\n    self._target_pose = HeadPose.neutral()\n    self._last_command_time: datetime | None = None\n\n    # SDK fallback tracking\n    self._sdk_failures: int = 0\n    self._sdk_fallback_active: bool = False\n\n    # Motion health tracking (detects when both SDK and HTTP are failing)\n    self._consecutive_total_failures: int = 0\n    self._motion_healthy: bool = True\n    self._UNHEALTHY_THRESHOLD: int = 10  # After 10 consecutive total failures\n\n    # HTTP fallback failure tracking\n    self._http_failures: int = 0\n\n    # Listening state (freezes antennas during user speech)\n    self._listening = False\n    self._frozen_antenna_left: float = 90.0  # Vertical (straight up)\n    self._frozen_antenna_right: float = 90.0\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController--register-motion-sources","title":"Register motion sources","text":"<p>controller.register_source(\"breathing\", breathing_motion) controller.register_source(\"wobble\", wobble_motion)</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController--activate-primary-source","title":"Activate primary source","text":"<p>controller.set_primary(\"breathing\")</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController--start-the-control-loop","title":"Start the control loop","text":"<p>await controller.start()</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController--later-during-speech","title":"Later, during speech","text":"<p>controller.enable_secondary(\"wobble\")</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the motion blend control loop.</p> Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start the motion blend control loop.\"\"\"\n    if self._running:\n        log.warning(\"Blend controller already running\")\n        return\n\n    if not self.config.enabled:\n        log.info(\"Motion blending is disabled in config\")\n        return\n\n    self._running = True\n    self._task = asyncio.create_task(self._control_loop())\n    log.info(\n        \"Motion blend controller started\",\n        tick_rate_hz=self.config.tick_rate_hz,\n        command_rate_hz=self.config.command_rate_hz,\n    )\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the motion blend control loop.</p> Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop the motion blend control loop.\"\"\"\n    self._running = False\n\n    if self._task:\n        self._task.cancel()\n        try:\n            await self._task\n        except asyncio.CancelledError:\n            pass\n        self._task = None\n\n    # Stop all active sources\n    if self._active_primary:\n        source = self._sources.get(self._active_primary)\n        if source:\n            await source.stop()\n\n    for name in list(self._active_secondaries):\n        source = self._sources.get(name)\n        if source:\n            await source.stop()\n\n    self._active_secondaries.clear()\n    log.info(\"Motion blend controller stopped\")\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController.register_source","title":"<code>register_source(name, source)</code>","text":"<p>Register a motion source.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier for the source.</p> required <code>source</code> <code>MotionSource</code> <p>Motion source implementing the MotionSource protocol.</p> required Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>def register_source(self, name: str, source: MotionSource) -&gt; None:\n    \"\"\"Register a motion source.\n\n    Args:\n        name: Unique identifier for the source.\n        source: Motion source implementing the MotionSource protocol.\n    \"\"\"\n    self._sources[name] = source\n    log.debug(\"Registered motion source\", name=name, priority=source.priority)\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController.unregister_source","title":"<code>unregister_source(name)</code>","text":"<p>Unregister a motion source.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the source to remove.</p> required Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>def unregister_source(self, name: str) -&gt; None:\n    \"\"\"Unregister a motion source.\n\n    Args:\n        name: Name of the source to remove.\n    \"\"\"\n    if name in self._sources:\n        # Deactivate if active\n        if name == self._active_primary:\n            self._active_primary = None\n        self._active_secondaries.discard(name)\n        del self._sources[name]\n        log.debug(\"Unregistered motion source\", name=name)\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController.set_primary","title":"<code>set_primary(name)</code>  <code>async</code>","text":"<p>Set the active primary motion source.</p> <p>Only one primary source can be active at a time. Setting to None deactivates all primary sources.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Name of the primary source to activate, or None.</p> required Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>async def set_primary(self, name: str | None) -&gt; None:\n    \"\"\"Set the active primary motion source.\n\n    Only one primary source can be active at a time.\n    Setting to None deactivates all primary sources.\n\n    Args:\n        name: Name of the primary source to activate, or None.\n    \"\"\"\n    # Stop current primary if different\n    if self._active_primary and self._active_primary != name:\n        source = self._sources.get(self._active_primary)\n        if source:\n            await source.stop()\n            log.info(\"Stopped primary motion\", name=self._active_primary)\n\n    # Start new primary\n    if name:\n        source = self._sources.get(name)\n        if source and source.priority == MotionPriority.PRIMARY:\n            await source.start()\n            self._active_primary = name\n            log.info(\"Started primary motion\", name=name)\n        elif source:\n            log.warning(\"Source is not PRIMARY priority\", name=name)\n        else:\n            log.warning(\"Unknown motion source\", name=name)\n    else:\n        self._active_primary = None\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController.enable_secondary","title":"<code>enable_secondary(name)</code>  <code>async</code>","text":"<p>Enable a secondary (additive) motion source.</p> <p>Multiple secondary sources can be active simultaneously.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the secondary source to enable.</p> required Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>async def enable_secondary(self, name: str) -&gt; None:\n    \"\"\"Enable a secondary (additive) motion source.\n\n    Multiple secondary sources can be active simultaneously.\n\n    Args:\n        name: Name of the secondary source to enable.\n    \"\"\"\n    source = self._sources.get(name)\n    if source and source.priority == MotionPriority.SECONDARY:\n        await source.start()\n        self._active_secondaries.add(name)\n        log.info(\"Enabled secondary motion\", name=name)\n    elif source:\n        log.warning(\"Source is not SECONDARY priority\", name=name)\n    else:\n        log.warning(\"Unknown motion source\", name=name)\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController.disable_secondary","title":"<code>disable_secondary(name)</code>  <code>async</code>","text":"<p>Disable a secondary motion source.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the secondary source to disable.</p> required Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>async def disable_secondary(self, name: str) -&gt; None:\n    \"\"\"Disable a secondary motion source.\n\n    Args:\n        name: Name of the secondary source to disable.\n    \"\"\"\n    source = self._sources.get(name)\n    if source:\n        await source.stop()\n        self._active_secondaries.discard(name)\n        log.info(\"Disabled secondary motion\", name=name)\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController.set_listening","title":"<code>set_listening(listening)</code>","text":"<p>Set listening state (freezes antennas during user speech).</p> <p>When listening is True, antenna positions are frozen to avoid distracting movements while the user speaks.</p> <p>Parameters:</p> Name Type Description Default <code>listening</code> <code>bool</code> <p>Whether the robot is listening to the user.</p> required Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>def set_listening(self, listening: bool) -&gt; None:\n    \"\"\"Set listening state (freezes antennas during user speech).\n\n    When listening is True, antenna positions are frozen to avoid\n    distracting movements while the user speaks.\n\n    Args:\n        listening: Whether the robot is listening to the user.\n    \"\"\"\n    if listening and not self._listening:\n        # Entering listening state - capture current antenna positions\n        self._frozen_antenna_left = self._current_pose.left_antenna\n        self._frozen_antenna_right = self._current_pose.right_antenna\n        log.debug(\"Entering listening state - antennas frozen\")\n    elif not listening and self._listening:\n        log.debug(\"Exiting listening state - antennas unfrozen\")\n\n    self._listening = listening\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.MotionBlendController.get_status","title":"<code>get_status()</code>","text":"<p>Get current controller status for debugging.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with controller state information.</p> Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>def get_status(self) -&gt; dict[str, Any]:\n    \"\"\"Get current controller status for debugging.\n\n    Returns:\n        Dictionary with controller state information.\n    \"\"\"\n    return {\n        \"running\": self._running,\n        \"enabled\": self.config.enabled,\n        \"active_primary\": self._active_primary,\n        \"active_secondaries\": list(self._active_secondaries),\n        \"registered_sources\": list(self._sources.keys()),\n        \"listening\": self._listening,\n        \"sdk_connected\": self._sdk_client.is_connected if self._sdk_client else False,\n        \"sdk_fallback_active\": self._sdk_fallback_active,\n        \"sdk_failures\": self._sdk_failures,\n        \"http_failures\": self._http_failures,\n        \"motion_healthy\": self._motion_healthy,\n        \"consecutive_total_failures\": self._consecutive_total_failures,\n        \"current_pose\": {\n            \"pitch\": self._current_pose.pitch,\n            \"yaw\": self._current_pose.yaw,\n            \"roll\": self._current_pose.roll,\n            \"z\": self._current_pose.z,\n            \"left_antenna\": self._current_pose.left_antenna,\n            \"right_antenna\": self._current_pose.right_antenna,\n        },\n    }\n</code></pre>"},{"location":"api/motion-blending/#blendcontrollerconfig","title":"BlendControllerConfig","text":""},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.BlendControllerConfig","title":"<code>BlendControllerConfig(tick_rate_hz=30.0, command_rate_hz=15.0, smoothing_factor=0.35, enabled=True, pose_limits=PoseLimits())</code>  <code>dataclass</code>","text":"<p>Configuration for the motion blend controller.</p> <p>Attributes:</p> Name Type Description <code>tick_rate_hz</code> <code>float</code> <p>Internal control loop rate (default 30Hz, config: 100Hz).</p> <code>command_rate_hz</code> <code>float</code> <p>Rate to send commands to daemon (default 15Hz, config: 20Hz).</p> <code>smoothing_factor</code> <code>float</code> <p>Pose interpolation factor (0.0-1.0).</p> <code>enabled</code> <code>bool</code> <p>Whether blending is active.</p> <code>pose_limits</code> <code>PoseLimits</code> <p>Safety limits for poses.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.blend_controller.BlendControllerConfig.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create from dictionary.</p> Source code in <code>src/reachy_agent/behaviors/blend_controller.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; BlendControllerConfig:\n    \"\"\"Create from dictionary.\"\"\"\n    pose_limits_data = data.get(\"pose_limits\")\n    # Filter out pose_limits when creating config to avoid passing it twice\n    filtered_data = {k: v for k, v in data.items() if hasattr(cls, k) and k != \"pose_limits\"}\n    config = cls(**filtered_data)\n    if pose_limits_data:\n        config.pose_limits = PoseLimits.from_dict(pose_limits_data)\n    return config\n</code></pre>"},{"location":"api/motion-blending/#breathingmotion","title":"BreathingMotion","text":"<p>Subtle idle animation with Z-axis oscillation and antenna sway.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.breathing.BreathingMotion","title":"<code>BreathingMotion(config=None)</code>","text":"<p>Breathing motion source - primary motion for idle state.</p> <p>Creates a subtle \"alive\" feeling through synchronized oscillations. This is a PRIMARY motion source, meaning it provides complete poses rather than offsets.</p> <p>The breathing pattern consists of: - Slow Z-axis oscillation (like breathing) - Antenna sway in opposite directions (attentive appearance) - Micro head pitch movements (subtle life)</p> Example <p>config = BreathingConfig(z_amplitude_mm=5.0) breathing = BreathingMotion(config) await breathing.start()</p> <p>Initialize breathing motion.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BreathingConfig | None</code> <p>Optional configuration. Uses defaults if not provided.</p> <code>None</code> Source code in <code>src/reachy_agent/behaviors/breathing.py</code> <pre><code>def __init__(self, config: BreathingConfig | None = None) -&gt; None:\n    \"\"\"Initialize breathing motion.\n\n    Args:\n        config: Optional configuration. Uses defaults if not provided.\n    \"\"\"\n    self.config = config or BreathingConfig()\n    self._active = False\n    self._start_time: datetime | None = None\n    self._base_pose = HeadPose.neutral()\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.breathing.BreathingMotion--in-control-loop","title":"In control loop:","text":"<p>pose = await breathing.get_contribution(base_pose)</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.breathing.BreathingMotion.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start breathing animation.</p> Source code in <code>src/reachy_agent/behaviors/breathing.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start breathing animation.\"\"\"\n    self._active = True\n    self._start_time = datetime.now()\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.breathing.BreathingMotion.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop breathing animation.</p> Source code in <code>src/reachy_agent/behaviors/breathing.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop breathing animation.\"\"\"\n    self._active = False\n    self._start_time = None\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.breathing.BreathingMotion.get_contribution","title":"<code>get_contribution(base_pose)</code>  <code>async</code>","text":"<p>Calculate breathing pose at current time.</p> <p>Parameters:</p> Name Type Description Default <code>base_pose</code> <code>HeadPose</code> <p>Current base pose (may be used for reference).</p> required <p>Returns:</p> Type Description <code>HeadPose</code> <p>HeadPose with breathing oscillations applied.</p> Source code in <code>src/reachy_agent/behaviors/breathing.py</code> <pre><code>async def get_contribution(self, base_pose: HeadPose) -&gt; HeadPose:\n    \"\"\"Calculate breathing pose at current time.\n\n    Args:\n        base_pose: Current base pose (may be used for reference).\n\n    Returns:\n        HeadPose with breathing oscillations applied.\n    \"\"\"\n    if not self._active or self._start_time is None:\n        return self._base_pose\n\n    elapsed = (datetime.now() - self._start_time).total_seconds()\n\n    # Z-axis oscillation (breathing)\n    z_offset = self.config.z_amplitude_mm * math.sin(\n        2 * math.pi * self.config.z_frequency_hz * elapsed\n    )\n\n    # Antenna oscillation (opposite directions for natural look)\n    antenna_wave = self.config.antenna_amplitude_deg * math.sin(\n        2 * math.pi * self.config.antenna_frequency_hz * elapsed\n    )\n    left_antenna = self.config.antenna_base_angle + antenna_wave\n    right_antenna = self.config.antenna_base_angle - antenna_wave  # Opposite\n\n    # Micro pitch movement (subtle life)\n    pitch_offset = self.config.pitch_amplitude_deg * math.sin(\n        2 * math.pi * self.config.pitch_frequency_hz * elapsed\n    )\n\n    # Micro yaw drift (very subtle wandering)\n    yaw_offset = self.config.yaw_amplitude_deg * math.sin(\n        2 * math.pi * self.config.yaw_frequency_hz * elapsed\n    )\n\n    return HeadPose(\n        pitch=self._base_pose.pitch + pitch_offset,\n        yaw=self._base_pose.yaw + yaw_offset,\n        roll=self._base_pose.roll,  # No roll oscillation\n        z=self._base_pose.z + z_offset,\n        left_antenna=left_antenna,\n        right_antenna=right_antenna,\n    )\n</code></pre>"},{"location":"api/motion-blending/#breathingconfig","title":"BreathingConfig","text":""},{"location":"api/motion-blending/#reachy_agent.behaviors.breathing.BreathingConfig","title":"<code>BreathingConfig(enabled=True, z_amplitude_mm=5.0, z_frequency_hz=0.1, antenna_amplitude_deg=15.0, antenna_frequency_hz=0.5, antenna_base_angle=90.0, pitch_amplitude_deg=1.5, pitch_frequency_hz=0.12, yaw_amplitude_deg=0.8, yaw_frequency_hz=0.07)</code>  <code>dataclass</code>","text":"<p>Configuration for breathing behavior.</p> <p>All frequencies in Hz, amplitudes in their respective units (degrees for angles, mm for z-axis).</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.breathing.BreathingConfig.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create from dictionary.</p> Source code in <code>src/reachy_agent/behaviors/breathing.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; BreathingConfig:\n    \"\"\"Create from dictionary.\"\"\"\n    return cls(**{k: v for k, v in data.items() if hasattr(cls, k)})\n</code></pre>"},{"location":"api/motion-blending/#headwobble","title":"HeadWobble","text":"<p>Audio-reactive motion for natural speech animation.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.wobble.HeadWobble","title":"<code>HeadWobble(config=None)</code>","text":"<p>Head wobble motion source for speech animation.</p> <p>This is a SECONDARY motion source that provides additive offsets based on audio level. During TTS playback, it creates subtle head movements that make speech feel more natural.</p> <p>The wobble algorithm: 1. Receives audio level (0.0 to 1.0) from TTS system 2. Maps level to displacement using configurable curves 3. Adds Perlin-like noise for organic variation 4. Returns PoseOffset to blend with primary motion</p> Example <p>config = WobbleConfig(max_pitch_deg=8.0) wobble = HeadWobble(config) await wobble.start()</p> <p>Initialize head wobble.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WobbleConfig | None</code> <p>Optional configuration. Uses defaults if not provided.</p> <code>None</code> Source code in <code>src/reachy_agent/behaviors/wobble.py</code> <pre><code>def __init__(self, config: WobbleConfig | None = None) -&gt; None:\n    \"\"\"Initialize head wobble.\n\n    Args:\n        config: Optional configuration. Uses defaults if not provided.\n    \"\"\"\n    self.config = config or WobbleConfig()\n    self._active = False\n    self._start_time: datetime | None = None\n\n    # Audio tracking\n    self._audio_level: float = 0.0\n    self._smoothed_level: float = 0.0\n    self._generation: int = 0\n\n    # Noise state for organic variation\n    self._noise_offset_x = random.uniform(0, 1000)\n    self._noise_offset_y = random.uniform(0, 1000)\n    self._noise_offset_z = random.uniform(0, 1000)\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.wobble.HeadWobble--during-tts-playback","title":"During TTS playback:","text":"<p>wobble.update_audio_level(0.7)  # Speaking loudly offset = await wobble.get_contribution(base_pose)</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.wobble.HeadWobble--when-tts-ends","title":"When TTS ends:","text":"<p>wobble.update_audio_level(0.0) await wobble.stop()</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.wobble.HeadWobble.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start wobble animation.</p> Source code in <code>src/reachy_agent/behaviors/wobble.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start wobble animation.\"\"\"\n    self._active = True\n    self._start_time = datetime.now()\n    self._generation += 1\n    self._smoothed_level = 0.0\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.wobble.HeadWobble.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop wobble animation.</p> Source code in <code>src/reachy_agent/behaviors/wobble.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop wobble animation.\"\"\"\n    self._active = False\n    self._start_time = None\n    self._audio_level = 0.0\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.wobble.HeadWobble.get_contribution","title":"<code>get_contribution(base_pose)</code>  <code>async</code>","text":"<p>Calculate wobble offset at current time.</p> <p>Parameters:</p> Name Type Description Default <code>base_pose</code> <code>Any</code> <p>Current base pose (for reference, not used).</p> required <p>Returns:</p> Type Description <code>PoseOffset</code> <p>PoseOffset with wobble displacements.</p> Source code in <code>src/reachy_agent/behaviors/wobble.py</code> <pre><code>async def get_contribution(self, base_pose: Any) -&gt; PoseOffset:\n    \"\"\"Calculate wobble offset at current time.\n\n    Args:\n        base_pose: Current base pose (for reference, not used).\n\n    Returns:\n        PoseOffset with wobble displacements.\n    \"\"\"\n    if not self._active or self._start_time is None:\n        return PoseOffset(generation=self._generation)\n\n    # Smooth the audio level\n    self._smoothed_level += (\n        self._audio_level - self._smoothed_level\n    ) * self.config.smoothing_factor\n\n    elapsed = (datetime.now() - self._start_time).total_seconds()\n\n    # Calculate base displacement from audio level\n    level = self._smoothed_level\n\n    # Map audio level to displacement (non-linear curve for natural feel)\n    # Uses sqrt for more responsive feel at low levels\n    mapped_level = math.sqrt(level)\n\n    # Generate noise-like variation using sum of sines\n    # This approximates Perlin noise without the complexity\n    noise_x = self._pseudo_noise(elapsed, self._noise_offset_x)\n    noise_y = self._pseudo_noise(elapsed, self._noise_offset_y)\n    noise_z = self._pseudo_noise(elapsed, self._noise_offset_z)\n\n    # Calculate final offsets\n    pitch_offset = (\n        mapped_level * self.config.max_pitch_deg\n        + noise_x * self.config.noise_scale * self.config.max_pitch_deg\n    )\n\n    yaw_offset = (\n        noise_y * self.config.noise_scale * self.config.max_yaw_deg * (1 + level)\n    )\n\n    roll_offset = (\n        noise_z * self.config.noise_scale * self.config.max_roll_deg * (1 + level)\n    )\n\n    return PoseOffset(\n        pitch=pitch_offset,\n        yaw=yaw_offset,\n        roll=roll_offset,\n        z=0.0,  # No Z-axis wobble\n        left_antenna=0.0,  # No antenna wobble\n        right_antenna=0.0,\n        generation=self._generation,\n    )\n</code></pre>"},{"location":"api/motion-blending/#wobbleconfig","title":"WobbleConfig","text":""},{"location":"api/motion-blending/#reachy_agent.behaviors.wobble.WobbleConfig","title":"<code>WobbleConfig(max_pitch_deg=8.0, max_yaw_deg=6.0, max_roll_deg=4.0, latency_compensation_ms=80.0, smoothing_factor=0.3, noise_scale=0.2, enabled=True)</code>  <code>dataclass</code>","text":"<p>Configuration for head wobble during speech.</p> <p>All amplitudes in degrees. Based on Conversation App parameters.</p> <p>Attributes:</p> Name Type Description <code>max_pitch_deg</code> <code>float</code> <p>Maximum pitch displacement.</p> <code>max_yaw_deg</code> <code>float</code> <p>Maximum yaw displacement.</p> <code>max_roll_deg</code> <code>float</code> <p>Maximum roll displacement.</p> <code>latency_compensation_ms</code> <code>float</code> <p>Audio processing latency to compensate.</p> <code>smoothing_factor</code> <code>float</code> <p>Smoothing for transitions (0.0-1.0).</p> <code>noise_scale</code> <code>float</code> <p>Scale factor for Perlin-like noise overlay.</p> <code>enabled</code> <code>bool</code> <p>Whether wobble is active.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.wobble.WobbleConfig.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create from dictionary.</p> Source code in <code>src/reachy_agent/behaviors/wobble.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; WobbleConfig:\n    \"\"\"Create from dictionary.\"\"\"\n    return cls(**{k: v for k, v in data.items() if hasattr(cls, k)})\n</code></pre>"},{"location":"api/motion-blending/#idlebehaviorcontroller","title":"IdleBehaviorController","text":"<p>Look-around behavior when the robot is idle.</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController","title":"<code>IdleBehaviorController(daemon_client=None, config=None)</code>","text":"<p>Controls autonomous idle behaviors for the robot.</p> <p>This controller runs in the background and makes the robot look around naturally when not engaged in conversation. It can be paused during user interactions and resumed after.</p> <p>Implements MotionSource protocol for integration with MotionBlendController. As a PRIMARY motion source, it provides complete HeadPose values.</p> Example usage <p>client = ReachyDaemonClient(base_url=\"http://localhost:8000\") controller = IdleBehaviorController(client) await controller.start()</p> With MotionBlendController <p>blend_controller.register_source(\"idle\", idle_controller) await blend_controller.set_primary(\"idle\")</p> <p>Initialize the idle behavior controller.</p> <p>Parameters:</p> Name Type Description Default <code>daemon_client</code> <code>ReachyDaemonClient | None</code> <p>Client for sending commands to the robot.           Optional when used with MotionBlendController.</p> <code>None</code> <code>config</code> <code>IdleBehaviorConfig | None</code> <p>Configuration for idle behavior. Uses defaults if not provided.</p> <code>None</code> Source code in <code>src/reachy_agent/behaviors/idle.py</code> <pre><code>def __init__(\n    self,\n    daemon_client: ReachyDaemonClient | None = None,\n    config: IdleBehaviorConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize the idle behavior controller.\n\n    Args:\n        daemon_client: Client for sending commands to the robot.\n                      Optional when used with MotionBlendController.\n        config: Configuration for idle behavior. Uses defaults if not provided.\n    \"\"\"\n    self.client = daemon_client\n    self.config = config or IdleBehaviorConfig()\n    self._state = IdleState.STOPPED\n    self._state_lock = asyncio.Lock()  # Thread-safe state management\n    self._task: asyncio.Task[None] | None = None\n    self._last_interaction: datetime | None = None\n    self._last_target: LookTarget | None = None\n    self._movement_count: int = 0\n\n    # MotionSource protocol support\n    self._current_pose = HeadPose.neutral()\n    self._target_pose = HeadPose.neutral()\n    self._start_pose = HeadPose.neutral()  # Pose at start of movement\n    self._pose_active = False\n\n    # Easing support - track movement timing\n    self._movement_start_time: datetime | None = None\n    self._movement_duration: float = 1.5  # Current movement duration\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController--later-when-user-starts-talking","title":"... later when user starts talking ...","text":"<p>await controller.pause()</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController--after-conversation-ends","title":"... after conversation ends ...","text":"<p>await controller.resume()</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController--when-shutting-down","title":"... when shutting down ...","text":"<p>await controller.stop()</p>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the idle behavior loop.</p> <p>This begins the background task that controls idle movements. Safe to call multiple times.</p> Source code in <code>src/reachy_agent/behaviors/idle.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start the idle behavior loop.\n\n    This begins the background task that controls idle movements.\n    Safe to call multiple times.\n    \"\"\"\n    if self._task is not None and not self._task.done():\n        log.debug(\"Idle behavior already running\")\n        return\n\n    log.info(\"Starting idle behavior controller\")\n    self._state = IdleState.IDLE\n    self._pose_active = True\n    self._current_pose = HeadPose.neutral()\n    self._task = asyncio.create_task(self._idle_loop())\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the idle behavior loop.</p> <p>This cancels the background task and waits for cleanup.</p> Source code in <code>src/reachy_agent/behaviors/idle.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop the idle behavior loop.\n\n    This cancels the background task and waits for cleanup.\n    \"\"\"\n    if self._task is None:\n        self._pose_active = False\n        return\n\n    log.info(\"Stopping idle behavior controller\")\n    self._state = IdleState.STOPPED\n    self._pose_active = False\n    self._task.cancel()\n\n    try:\n        await self._task\n    except asyncio.CancelledError:\n        pass\n\n    self._task = None\n    log.info(\n        \"Idle behavior stopped\",\n        total_movements=self._movement_count,\n    )\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController.pause","title":"<code>pause()</code>  <code>async</code>","text":"<p>Pause idle behaviors (e.g., when user starts talking).</p> <p>The loop continues running but doesn't execute movements. Uses async lock for thread-safe state modification.</p> Source code in <code>src/reachy_agent/behaviors/idle.py</code> <pre><code>async def pause(self) -&gt; None:\n    \"\"\"Pause idle behaviors (e.g., when user starts talking).\n\n    The loop continues running but doesn't execute movements.\n    Uses async lock for thread-safe state modification.\n    \"\"\"\n    async with self._state_lock:\n        if self._state == IdleState.IDLE:\n            log.debug(\"Pausing idle behavior\")\n            self._state = IdleState.PAUSED\n            self._last_interaction = datetime.now()\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController.resume","title":"<code>resume()</code>  <code>async</code>","text":"<p>Resume idle behaviors after a pause.</p> <p>Respects the interaction cooldown before starting movements again. Uses async lock for thread-safe state modification.</p> Source code in <code>src/reachy_agent/behaviors/idle.py</code> <pre><code>async def resume(self) -&gt; None:\n    \"\"\"Resume idle behaviors after a pause.\n\n    Respects the interaction cooldown before starting movements again.\n    Uses async lock for thread-safe state modification.\n    \"\"\"\n    async with self._state_lock:\n        if self._state == IdleState.PAUSED:\n            log.debug(\"Resuming idle behavior\")\n            self._state = IdleState.IDLE\n            self._last_interaction = datetime.now()\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController.notify_interaction","title":"<code>notify_interaction()</code>  <code>async</code>","text":"<p>Notify the controller that a user interaction occurred.</p> <p>Call this when user sends input or agent responds. Automatically pauses if configured to do so. Uses async lock for thread-safe state modification.</p> Source code in <code>src/reachy_agent/behaviors/idle.py</code> <pre><code>async def notify_interaction(self) -&gt; None:\n    \"\"\"Notify the controller that a user interaction occurred.\n\n    Call this when user sends input or agent responds.\n    Automatically pauses if configured to do so.\n    Uses async lock for thread-safe state modification.\n    \"\"\"\n    async with self._state_lock:\n        self._last_interaction = datetime.now()\n        if self.config.pause_on_interaction and self._state == IdleState.IDLE:\n            self._state = IdleState.PAUSED\n            log.debug(\"Pausing idle behavior due to interaction\")\n</code></pre>"},{"location":"api/motion-blending/#reachy_agent.behaviors.idle.IdleBehaviorController.get_contribution","title":"<code>get_contribution(base_pose)</code>  <code>async</code>","text":"<p>Get idle motion's contribution to the final pose.</p> <p>As a PRIMARY source, returns complete HeadPose values with ease-in-out interpolation for smooth movements.</p> <p>Parameters:</p> Name Type Description Default <code>base_pose</code> <code>HeadPose</code> <p>Current base pose for reference.</p> required <p>Returns:</p> Type Description <code>HeadPose</code> <p>HeadPose with eased interpolation toward target.</p> Source code in <code>src/reachy_agent/behaviors/idle.py</code> <pre><code>async def get_contribution(self, base_pose: HeadPose) -&gt; HeadPose:\n    \"\"\"Get idle motion's contribution to the final pose.\n\n    As a PRIMARY source, returns complete HeadPose values with\n    ease-in-out interpolation for smooth movements.\n\n    Args:\n        base_pose: Current base pose for reference.\n\n    Returns:\n        HeadPose with eased interpolation toward target.\n    \"\"\"\n    if not self.is_active:\n        return self._current_pose\n\n    # Calculate eased position if we're in a movement\n    if self._movement_start_time is not None:\n        elapsed = (datetime.now() - self._movement_start_time).total_seconds()\n        progress = min(1.0, elapsed / self._movement_duration)\n\n        # Use ease-in-out interpolation\n        self._current_pose = self._start_pose.ease_in_out(self._target_pose, progress)\n\n        # Clear movement timing when complete\n        if progress &gt;= 1.0:\n            self._movement_start_time = None\n\n    return self._current_pose\n</code></pre>"},{"location":"api/motion-blending/#usage-examples","title":"Usage Examples","text":""},{"location":"api/motion-blending/#basic-motion-blending","title":"Basic Motion Blending","text":"<pre><code>from reachy_agent.behaviors.blend_controller import (\n    MotionBlendController,\n    BlendControllerConfig,\n)\nfrom reachy_agent.behaviors.breathing import BreathingMotion, BreathingConfig\nfrom reachy_agent.behaviors.wobble import HeadWobble, WobbleConfig\nfrom reachy_agent.behaviors.motion_types import HeadPose\n\n# Create controller with pose callback\nasync def send_pose(pose: HeadPose) -&gt; None:\n    await daemon_client.look_at(\n        pitch=pose.pitch,\n        yaw=pose.yaw,\n        roll=pose.roll,\n    )\n    await daemon_client.set_antenna_state(\n        left_angle=pose.left_antenna,\n        right_angle=pose.right_antenna,\n    )\n\nconfig = BlendControllerConfig(\n    tick_rate_hz=100.0,\n    command_rate_hz=20.0,\n    smoothing_factor=0.3,\n)\ncontroller = MotionBlendController(config, send_pose_callback=send_pose)\n\n# Register motion sources\nbreathing = BreathingMotion(BreathingConfig())\nwobble = HeadWobble(WobbleConfig())\n\ncontroller.register_source(\"breathing\", breathing)\ncontroller.register_source(\"wobble\", wobble)\n\n# Start with breathing as primary\nawait controller.start()\nawait controller.set_primary(\"breathing\")\n\n# Enable wobble overlay during speech\nawait controller.enable_secondary(\"wobble\")\nwobble.simulate_speech(duration=2.0)  # For testing\n\n# Freeze antennas while user speaks\ncontroller.set_listening(True)\n# ... user speaks ...\ncontroller.set_listening(False)\n\n# Cleanup\nawait controller.stop()\n</code></pre>"},{"location":"api/motion-blending/#standalone-breathing-demo","title":"Standalone Breathing Demo","text":"<pre><code>from reachy_agent.behaviors.breathing import run_breathing_demo\n\n# Run breathing animation for 30 seconds\nawait run_breathing_demo(\n    daemon_url=\"http://localhost:8765\",\n    duration_seconds=30.0,\n)\n</code></pre>"},{"location":"api/motion-blending/#standalone-idle-demo","title":"Standalone Idle Demo","text":"<pre><code>from reachy_agent.behaviors.idle import run_idle_demo\n\n# Run idle look-around for 30 seconds\nawait run_idle_demo(\n    daemon_url=\"http://localhost:8765\",\n    duration_seconds=30.0,\n)\n</code></pre>"},{"location":"api/motion-blending/#configuration","title":"Configuration","text":"<p>Motion blending can be configured via <code>config/default.yaml</code>:</p> <pre><code>motion_blend:\n  enabled: true\n  tick_rate_hz: 100.0\n  command_rate_hz: 20.0\n  smoothing_factor: 0.3\n  pose_limits:\n    pitch_range: [-45.0, 45.0]\n    yaw_range: [-45.0, 45.0]\n    roll_range: [-30.0, 30.0]\n    z_range: [-50.0, 50.0]\n    antenna_range: [0.0, 90.0]\n\nbreathing:\n  enabled: true\n  z_amplitude_mm: 5.0\n  z_frequency_hz: 0.15\n  antenna_amplitude_deg: 10.0\n  antenna_frequency_hz: 0.2\n  antenna_base_angle: 45.0\n\nwobble:\n  enabled: true\n  max_pitch_deg: 8.0\n  max_yaw_deg: 5.0\n  pitch_scale: 1.0\n  yaw_scale: 0.6\n  smoothing_factor: 0.3\n  noise_frequency: 3.0\n\nidle_behavior:\n  enabled: true\n  min_look_interval: 3.0\n  max_look_interval: 8.0\n  movement_duration: 1.5\n  yaw_range: [-35.0, 35.0]\n  pitch_range: [-15.0, 20.0]\n  roll_range: [-8.0, 8.0]\n  curiosity_chance: 0.15\n  pause_on_interaction: true\n</code></pre>"},{"location":"api/motion-blending/#design-patterns","title":"Design Patterns","text":""},{"location":"api/motion-blending/#primary-vs-secondary-motions","title":"Primary vs Secondary Motions","text":"<ul> <li>Primary (Exclusive): Only one active at a time. Provides complete pose values.</li> <li>Examples: Breathing, Idle look-around, Playing emotions</li> <li> <p>Use <code>set_primary()</code> to switch between them</p> </li> <li> <p>Secondary (Additive): Multiple can be active simultaneously. Provides delta offsets.</p> </li> <li>Examples: Speech wobble, Face tracking adjustments</li> <li>Use <code>enable_secondary()</code> / <code>disable_secondary()</code></li> </ul>"},{"location":"api/motion-blending/#composition-model","title":"Composition Model","text":"<pre><code>final_pose = primary_pose + secondary_offset_1 + secondary_offset_2 + ...\n</code></pre> <p>The blend controller: 1. Gets the active primary source's complete <code>HeadPose</code> 2. Accumulates all active secondary sources' <code>PoseOffset</code> deltas 3. Adds offsets to the primary pose 4. Applies safety limits via <code>clamp()</code> 5. Smoothly interpolates toward target using <code>lerp()</code> 6. Sends final pose to daemon at command rate</p>"},{"location":"api/motion-blending/#listening-state","title":"Listening State","text":"<p>When <code>set_listening(True)</code> is called, antenna positions are frozen to avoid distracting movements while the user speaks. Head movement continues normally.</p>"},{"location":"api/motion-blending/#sdk-motion-control-integration","title":"SDK Motion Control Integration","text":"<p>The blend controller supports two motion backends with automatic failover:</p>"},{"location":"api/motion-blending/#latency-comparison","title":"Latency Comparison","text":"Backend Transport Latency Use Case SDK (Zenoh) Pub/Sub 1-5ms Preferred for smooth animation HTTP (REST) TCP/HTTP 10-50ms Fallback when SDK unavailable"},{"location":"api/motion-blending/#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph Controller[\"MotionBlendController\"]\n        Loop[\"100Hz Control Loop\"]\n        Send[\"_send_pose_to_daemon()\"]\n    end\n\n    subgraph SDK[\"SDK Path (Preferred)\"]\n        ZClient[\"ReachySDKClient\"]\n        Zenoh[\"Zenoh Transport&lt;br/&gt;(1-5ms latency)\"]\n    end\n\n    subgraph HTTP[\"HTTP Path (Fallback)\"]\n        Callback[\"send_pose_callback\"]\n        REST[\"HTTP REST&lt;br/&gt;(10-50ms latency)\"]\n    end\n\n    subgraph Daemon[\"Reachy Daemon\"]\n        Motors[\"Motor Controllers\"]\n    end\n\n    Loop --&gt; Send\n    Send --&gt;|\"Try first\"| ZClient\n    ZClient --&gt; Zenoh\n    Zenoh --&gt; Motors\n    Send --&gt;|\"Fallback\"| Callback\n    Callback --&gt; REST\n    REST --&gt; Motors\n\n    style SDK fill:#c8e6c9\n    style HTTP fill:#fff9c4</code></pre>"},{"location":"api/motion-blending/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<p>The SDK connection uses a circuit breaker to automatically fallback to HTTP:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; SDK_Active\n    SDK_Active --&gt; SDK_Failing: Exception/False\n    SDK_Failing --&gt; SDK_Failing: failures &lt; 5\n    SDK_Failing --&gt; HTTP_Fallback: failures &gt;= 5\n    HTTP_Fallback --&gt; SDK_Active: reset_sdk_fallback()\n    SDK_Active --&gt; SDK_Active: Success (reset counter)</code></pre> <ul> <li>Failure threshold: 5 consecutive failures</li> <li>Recovery: Call <code>reset_sdk_fallback()</code> or restart controller</li> <li>Logging: Warnings emitted at threshold</li> </ul>"},{"location":"api/motion-blending/#sdk-client-api","title":"SDK Client API","text":"<pre><code>from reachy_agent.mcp_servers.reachy.sdk_client import ReachySDKClient\n\n# Initialize with SDK client for low-latency motion\nsdk_client = ReachySDKClient(config)\nawait sdk_client.connect()\n\ncontroller = MotionBlendController(\n    config=BlendControllerConfig(),\n    send_pose_callback=http_fallback,  # Optional HTTP fallback\n    sdk_client=sdk_client,              # Preferred SDK client\n)\n</code></pre>"},{"location":"api/motion-blending/#coordinate-transformations","title":"Coordinate Transformations","text":"<p>The SDK client handles coordinate system differences:</p> Component Agent Format SDK Format Head angles Degrees Radians + 4x4 matrix Antennas 0\u00b0 = flat 0 rad = vertical <pre><code># Agent pose (degrees, 0\u00b0 = flat antennas)\npose = HeadPose(pitch=15.0, yaw=-10.0, left_antenna=45.0, right_antenna=45.0)\n\n# SDK client converts internally:\n# - pitch/yaw \u2192 rotation matrix via scipy.spatial.transform.Rotation\n# - antennas: 45\u00b0 \u2192 \u03c0/4 rad, inverted (SDK 0 = vertical)\nawait sdk_client.set_pose(pose)\n</code></pre>"},{"location":"api/motion-blending/#configuration_1","title":"Configuration","text":"<pre><code>sdk:\n  enabled: true\n  robot_name: \"reachy_mini\"\n  localhost_only: true\n  spawn_daemon: false\n  media_backend: \"no_media\"\n  connect_timeout: 5.0\n  fallback_to_http: true\n</code></pre>"},{"location":"api/permissions/","title":"Permissions Module API","text":"<p>The permissions module implements the 4-tier permission system for tool execution control.</p>"},{"location":"api/permissions/#permission-tiers","title":"Permission Tiers","text":""},{"location":"api/permissions/#reachy_agent.permissions.tiers.PermissionTier","title":"<code>PermissionTier</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Permission tier levels.</p> <p>Lower numbers = more permissive.</p>"},{"location":"api/permissions/#permission-evaluator","title":"Permission Evaluator","text":""},{"location":"api/permissions/#reachy_agent.permissions.tiers.PermissionEvaluator","title":"<code>PermissionEvaluator(config=None, default_tier=PermissionTier.CONFIRM)</code>","text":"<p>Evaluates tool permissions against configured rules.</p> <p>Initialize the permission evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PermissionConfig | None</code> <p>Permission configuration. Uses defaults if None.</p> <code>None</code> <code>default_tier</code> <code>PermissionTier</code> <p>Default tier for unmatched tools.</p> <code>CONFIRM</code> Source code in <code>src/reachy_agent/permissions/tiers.py</code> <pre><code>def __init__(\n    self,\n    config: PermissionConfig | None = None,\n    default_tier: PermissionTier = PermissionTier.CONFIRM,\n) -&gt; None:\n    \"\"\"Initialize the permission evaluator.\n\n    Args:\n        config: Permission configuration. Uses defaults if None.\n        default_tier: Default tier for unmatched tools.\n    \"\"\"\n    self.config = config or PermissionConfig.default()\n    self.default_tier = default_tier\n    self._rules = self.config.rules\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.tiers.PermissionEvaluator.evaluate","title":"<code>evaluate(tool_name)</code>","text":"<p>Evaluate permissions for a tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool to check.</p> required <p>Returns:</p> Type Description <code>PermissionDecision</code> <p>PermissionDecision with tier, behavior, and reason.</p> Source code in <code>src/reachy_agent/permissions/tiers.py</code> <pre><code>def evaluate(self, tool_name: str) -&gt; PermissionDecision:\n    \"\"\"Evaluate permissions for a tool.\n\n    Args:\n        tool_name: Name of the tool to check.\n\n    Returns:\n        PermissionDecision with tier, behavior, and reason.\n    \"\"\"\n    # Find first matching rule\n    for rule in self._rules:\n        if rule.matches(tool_name):\n            tier = rule.permission_tier\n            return PermissionDecision(\n                tool_name=tool_name,\n                tier=tier,\n                behavior=TIER_BEHAVIORS[tier],\n                reason=rule.reason,\n                matched_rule=rule,\n            )\n\n    # No matching rule - use default tier\n    return PermissionDecision(\n        tool_name=tool_name,\n        tier=self.default_tier,\n        behavior=TIER_BEHAVIORS[self.default_tier],\n        reason=\"No matching rule - using default tier\",\n        matched_rule=None,\n    )\n</code></pre>"},{"location":"api/permissions/#permission-decision","title":"Permission Decision","text":""},{"location":"api/permissions/#reachy_agent.permissions.tiers.PermissionDecision","title":"<code>PermissionDecision(tool_name, tier, behavior, reason, matched_rule=None)</code>  <code>dataclass</code>","text":"<p>Result of a permission check.</p>"},{"location":"api/permissions/#reachy_agent.permissions.tiers.PermissionDecision.allowed","title":"<code>allowed</code>  <code>property</code>","text":"<p>Whether the tool execution is allowed.</p>"},{"location":"api/permissions/#reachy_agent.permissions.tiers.PermissionDecision.needs_confirmation","title":"<code>needs_confirmation</code>  <code>property</code>","text":"<p>Whether user confirmation is required.</p>"},{"location":"api/permissions/#reachy_agent.permissions.tiers.PermissionDecision.should_notify","title":"<code>should_notify</code>  <code>property</code>","text":"<p>Whether user should be notified.</p>"},{"location":"api/permissions/#permission-hooks","title":"Permission Hooks","text":""},{"location":"api/permissions/#reachy_agent.permissions.hooks.PermissionHooks","title":"<code>PermissionHooks(evaluator=None, confirmation_callback=None, notification_callback=None, audit_callback=None)</code>","text":"<p>Permission enforcement hooks for Claude Agent SDK.</p> <p>Provides PreToolUse and PostToolUse hook implementations that enforce the permission tier system.</p> <p>Initialize permission hooks.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator</code> <code>PermissionEvaluator | None</code> <p>Permission evaluator. Uses default if None.</p> <code>None</code> <code>confirmation_callback</code> <code>ConfirmationCallback | None</code> <p>Async function to request user confirmation. Receives (tool_name, reason, tool_input) and returns bool.</p> <code>None</code> <code>notification_callback</code> <code>Callable[[str, str], Awaitable[None]] | None</code> <p>Async function to notify user. Receives (tool_name, message).</p> <code>None</code> <code>audit_callback</code> <code>Callable[[ToolExecution], Awaitable[None]] | None</code> <p>Async function to log tool executions. Receives ToolExecution record.</p> <code>None</code> Source code in <code>src/reachy_agent/permissions/hooks.py</code> <pre><code>def __init__(\n    self,\n    evaluator: PermissionEvaluator | None = None,\n    confirmation_callback: ConfirmationCallback | None = None,\n    notification_callback: Callable[[str, str], Awaitable[None]] | None = None,\n    audit_callback: Callable[[ToolExecution], Awaitable[None]] | None = None,\n) -&gt; None:\n    \"\"\"Initialize permission hooks.\n\n    Args:\n        evaluator: Permission evaluator. Uses default if None.\n        confirmation_callback: Async function to request user confirmation.\n            Receives (tool_name, reason, tool_input) and returns bool.\n        notification_callback: Async function to notify user.\n            Receives (tool_name, message).\n        audit_callback: Async function to log tool executions.\n            Receives ToolExecution record.\n    \"\"\"\n    self.evaluator = evaluator or PermissionEvaluator()\n    self._confirmation_callback = confirmation_callback\n    self._notification_callback = notification_callback\n    self._audit_callback = audit_callback\n    self._pending_executions: dict[str, ToolExecution] = {}\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.hooks.PermissionHooks.pre_tool_use","title":"<code>pre_tool_use(tool_name, tool_input)</code>  <code>async</code>","text":"<p>Pre-tool-use hook for permission enforcement.</p> <p>Called before each tool execution. Returns None to allow execution, or a dict with an error message to block it.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool being called.</p> required <code>tool_input</code> <code>dict[str, Any]</code> <p>Input parameters for the tool.</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>None to allow execution, or error dict to block.</p> <p>Raises:</p> Type Description <code>PermissionDeniedError</code> <p>If the tool is forbidden.</p> <code>ConfirmationTimeoutError</code> <p>If confirmation times out.</p> Source code in <code>src/reachy_agent/permissions/hooks.py</code> <pre><code>async def pre_tool_use(\n    self,\n    tool_name: str,\n    tool_input: dict[str, Any],\n) -&gt; dict[str, Any] | None:\n    \"\"\"Pre-tool-use hook for permission enforcement.\n\n    Called before each tool execution. Returns None to allow\n    execution, or a dict with an error message to block it.\n\n    Args:\n        tool_name: Name of the tool being called.\n        tool_input: Input parameters for the tool.\n\n    Returns:\n        None to allow execution, or error dict to block.\n\n    Raises:\n        PermissionDeniedError: If the tool is forbidden.\n        ConfirmationTimeoutError: If confirmation times out.\n    \"\"\"\n    # Evaluate permissions\n    decision = self.evaluator.evaluate(tool_name)\n\n    log.info(\n        \"Permission check\",\n        tool_name=tool_name,\n        tier=decision.tier.name,\n        allowed=decision.allowed,\n        reason=decision.reason,\n    )\n\n    # Create audit record\n    execution = ToolExecution(\n        tool_name=tool_name,\n        tool_input=tool_input,\n        permission_tier=decision.tier.value,\n    )\n    self._pending_executions[execution.id] = execution\n\n    # Handle based on tier\n    if decision.tier == PermissionTier.FORBIDDEN:\n        execution.decision = \"denied\"\n        execution.result = \"error\"\n        await self._log_execution(execution)\n\n        log.warning(\n            \"Tool execution denied\",\n            tool_name=tool_name,\n            reason=decision.reason,\n        )\n\n        return {\n            \"error\": f\"This action is not allowed: {decision.reason}\",\n            \"tier\": \"forbidden\",\n        }\n\n    if decision.needs_confirmation:\n        # Tier 3: Requires confirmation\n        confirmed = await self._request_confirmation(\n            tool_name, decision.reason, tool_input\n        )\n\n        if not confirmed:\n            execution.decision = \"denied\"\n            execution.result = \"error\"\n            await self._log_execution(execution)\n\n            log.info(\"User denied confirmation\", tool_name=tool_name)\n            return {\n                \"error\": \"User declined to confirm this action\",\n                \"tier\": \"confirm\",\n            }\n\n        execution.decision = \"confirmed\"\n\n    elif decision.should_notify:\n        # Tier 2: Notify user\n        await self._notify_user(\n            tool_name,\n            f\"Executing {tool_name}: {decision.reason}\",\n        )\n        execution.decision = \"notified\"\n\n    else:\n        # Tier 1: Autonomous\n        execution.decision = \"allowed\"\n\n    # Store execution ID for post-hook correlation\n    return {\"_execution_id\": execution.id}\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.hooks.PermissionHooks.post_tool_use","title":"<code>post_tool_use(tool_name, tool_input, tool_result, execution_id=None, error=None)</code>  <code>async</code>","text":"<p>Post-tool-use hook for audit logging.</p> <p>Called after each tool execution completes.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool that was called.</p> required <code>tool_input</code> <code>dict[str, Any]</code> <p>Input parameters that were used.</p> required <code>tool_result</code> <code>Any</code> <p>Result from the tool execution.</p> required <code>execution_id</code> <code>str | None</code> <p>ID from pre-hook for correlation.</p> <code>None</code> <code>error</code> <code>Exception | None</code> <p>Exception if the tool failed.</p> <code>None</code> Source code in <code>src/reachy_agent/permissions/hooks.py</code> <pre><code>async def post_tool_use(\n    self,\n    tool_name: str,\n    tool_input: dict[str, Any],\n    tool_result: Any,  # noqa: ARG002\n    execution_id: str | None = None,\n    error: Exception | None = None,\n) -&gt; None:\n    \"\"\"Post-tool-use hook for audit logging.\n\n    Called after each tool execution completes.\n\n    Args:\n        tool_name: Name of the tool that was called.\n        tool_input: Input parameters that were used.\n        tool_result: Result from the tool execution.\n        execution_id: ID from pre-hook for correlation.\n        error: Exception if the tool failed.\n    \"\"\"\n    # Find the execution record\n    execution = None\n    if execution_id and execution_id in self._pending_executions:\n        execution = self._pending_executions.pop(execution_id)\n    else:\n        # Create a new record if we don't have one\n        execution = ToolExecution(\n            tool_name=tool_name,\n            tool_input=tool_input,\n            permission_tier=0,\n            decision=\"unknown\",\n        )\n\n    # Update with result\n    if error:\n        execution.result = \"error\"\n    else:\n        execution.result = \"success\"\n\n    # Calculate duration (approximate)\n    execution.duration_ms = int(\n        (datetime.now() - execution.timestamp).total_seconds() * 1000\n    )\n\n    # Log the execution\n    await self._log_execution(execution)\n\n    log.info(\n        \"Tool execution completed\",\n        tool_name=tool_name,\n        result=execution.result,\n        duration_ms=execution.duration_ms,\n    )\n</code></pre>"},{"location":"api/permissions/#sdk-hook-factory","title":"SDK Hook Factory","text":""},{"location":"api/permissions/#reachy_agent.permissions.hooks.create_sdk_permission_hook","title":"<code>create_sdk_permission_hook(evaluator=None, config_path=None)</code>","text":"<p>Create an SDK-compatible PreToolUse hook for permission enforcement.</p> <p>This factory creates a hook function that can be used directly with the Claude Agent SDK's HookMatcher. It implements the 4-tier permission system (AUTONOMOUS, NOTIFY, CONFIRM, FORBIDDEN).</p> <p>Parameters:</p> Name Type Description Default <code>evaluator</code> <code>PermissionEvaluator | None</code> <p>Pre-configured PermissionEvaluator. Uses default if None.</p> <code>None</code> <code>config_path</code> <code>str | None</code> <p>Path to permissions.yaml. Used if evaluator is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[SDKHookInput, str | None, Any], Awaitable[SDKHookJSONOutput]]</code> <p>Async hook function compatible with SDK's PreToolUse hook format.</p> Example <p>hook = create_sdk_permission_hook() options = ClaudeAgentOptions( ...     hooks={ ...         \"PreToolUse\": [HookMatcher(matcher=None, hooks=[hook])] ...     } ... )</p> Hook Return Values <ul> <li>Empty dict <code>{}</code>: Allow execution (AUTONOMOUS, NOTIFY tiers)</li> <li><code>{\"hookSpecificOutput\": {..., \"permissionDecision\": \"deny\"}}</code>: Block (FORBIDDEN)</li> <li><code>{\"hookSpecificOutput\": {..., \"permissionDecision\": \"ask\"}}</code>: Confirm (CONFIRM tier)</li> </ul> Source code in <code>src/reachy_agent/permissions/hooks.py</code> <pre><code>def create_sdk_permission_hook(\n    evaluator: PermissionEvaluator | None = None,\n    config_path: str | None = None,\n) -&gt; Callable[[SDKHookInput, str | None, Any], Awaitable[SDKHookJSONOutput]]:\n    \"\"\"Create an SDK-compatible PreToolUse hook for permission enforcement.\n\n    This factory creates a hook function that can be used directly with\n    the Claude Agent SDK's HookMatcher. It implements the 4-tier permission\n    system (AUTONOMOUS, NOTIFY, CONFIRM, FORBIDDEN).\n\n    Args:\n        evaluator: Pre-configured PermissionEvaluator. Uses default if None.\n        config_path: Path to permissions.yaml. Used if evaluator is None.\n\n    Returns:\n        Async hook function compatible with SDK's PreToolUse hook format.\n\n    Example:\n        &gt;&gt;&gt; hook = create_sdk_permission_hook()\n        &gt;&gt;&gt; options = ClaudeAgentOptions(\n        ...     hooks={\n        ...         \"PreToolUse\": [HookMatcher(matcher=None, hooks=[hook])]\n        ...     }\n        ... )\n\n    Hook Return Values:\n        - Empty dict `{}`: Allow execution (AUTONOMOUS, NOTIFY tiers)\n        - `{\"hookSpecificOutput\": {..., \"permissionDecision\": \"deny\"}}`: Block (FORBIDDEN)\n        - `{\"hookSpecificOutput\": {..., \"permissionDecision\": \"ask\"}}`: Confirm (CONFIRM tier)\n    \"\"\"\n    from pathlib import Path\n\n    from reachy_agent.permissions.tiers import PermissionConfig\n\n    # Create or use provided evaluator\n    if evaluator is None:\n        if config_path:\n            config = PermissionConfig.from_yaml(Path(config_path))\n            evaluator = PermissionEvaluator(config=config)\n        else:\n            evaluator = PermissionEvaluator()\n\n    # Capture evaluator in closure\n    perm_evaluator = evaluator\n\n    async def sdk_permission_hook(\n        input_data: SDKHookInput,\n        tool_use_id: str | None,\n        context: Any,\n    ) -&gt; SDKHookJSONOutput:\n        \"\"\"SDK-compatible PreToolUse hook for 4-tier permissions.\n\n        Args:\n            input_data: Contains tool_name and tool_input.\n            tool_use_id: Optional tool use ID for tracking.\n            context: SDK hook context.\n\n        Returns:\n            Hook output with permission decision.\n        \"\"\"\n        tool_name = input_data.get(\"tool_name\", \"\")\n        tool_input = input_data.get(\"tool_input\", {})\n\n        # Strip SDK prefix to get original tool name\n        # SDK format: mcp__server__tool \u2192 extract tool\n        original_tool = tool_name\n        if tool_name.startswith(\"mcp__\"):\n            parts = tool_name.split(\"__\")\n            if len(parts) &gt;= 3:\n                original_tool = parts[2]\n\n        # Evaluate permission tier\n        decision = perm_evaluator.evaluate(original_tool)\n\n        log.info(\n            \"SDK permission hook evaluation\",\n            tool_name=tool_name,\n            original_tool=original_tool,\n            tier=decision.tier.name,\n            allowed=decision.allowed,\n            reason=decision.reason,\n        )\n\n        # Handle based on tier\n        if decision.tier == PermissionTier.FORBIDDEN:\n            return {\n                \"hookSpecificOutput\": {\n                    \"hookEventName\": \"PreToolUse\",\n                    \"permissionDecision\": \"deny\",\n                    \"permissionDecisionReason\": f\"Tool {original_tool} is forbidden: {decision.reason}\",\n                }\n            }\n\n        if decision.needs_confirmation:\n            return {\n                \"hookSpecificOutput\": {\n                    \"hookEventName\": \"PreToolUse\",\n                    \"permissionDecision\": \"ask\",\n                    \"permissionDecisionReason\": f\"Confirm {original_tool}? {decision.reason}\",\n                }\n            }\n\n        if decision.should_notify:\n            log.info(\n                \"NOTIFY tier: Executing tool\",\n                tool=original_tool,\n                input=tool_input,\n            )\n\n        # AUTONOMOUS or NOTIFY: allow without blocking\n        return {}\n\n    return sdk_permission_hook\n</code></pre>"},{"location":"api/permissions/#tool-execution","title":"Tool Execution","text":""},{"location":"api/permissions/#reachy_agent.permissions.hooks.ToolExecution","title":"<code>ToolExecution(id=(lambda: str(uuid4()))(), timestamp=datetime.now(), tool_name='', tool_input=dict(), permission_tier=0, decision='', result='', duration_ms=0)</code>  <code>dataclass</code>","text":"<p>Audit log entry for a tool execution.</p> <p>Matches the ToolExecution schema in TECH_REQ.md.</p>"},{"location":"api/permissions/#configuration","title":"Configuration","text":"<p>Permissions are configured in <code>config/permissions.yaml</code>:</p> <pre><code># Default tier for unknown tools\ndefault_tier: 1\n\n# Tool-specific rules\nrules:\n  # Tier 1: Autonomous\n  - pattern: \"mcp__reachy__*\"\n    tier: 1\n  - pattern: \"mcp__memory__*\"\n    tier: 1\n\n  # Tier 3: Confirm\n  - pattern: \"mcp__calendar__create_*\"\n    tier: 3\n\n  # Tier 4: Forbidden\n  - pattern: \"mcp__banking__*\"\n    tier: 4\n\n# Timeouts\nconfirmation_timeout_seconds: 60\n</code></pre>"},{"location":"api/permissions/#usage-example","title":"Usage Example","text":"<pre><code>from reachy_agent.permissions.tiers import PermissionEvaluator, PermissionTier\n\n# Create evaluator\nevaluator = PermissionEvaluator()\n\n# Evaluate a tool\ndecision = evaluator.evaluate(\"move_head\")\nprint(f\"Tier: {decision.tier}\")\nprint(f\"Allowed: {decision.allowed}\")\n\n# Check specific properties\nif decision.needs_confirmation:\n    # Request user confirmation\n    pass\nelif decision.should_notify:\n    # Log notification\n    pass\n</code></pre>"},{"location":"api/permissions/#sdk-integration","title":"SDK Integration","text":"<pre><code>from reachy_agent.permissions.hooks import create_sdk_permission_hook\nfrom claude_agent_sdk import ClaudeAgentOptions, HookMatcher\n\n# Create SDK-compatible hook\npermission_hook = create_sdk_permission_hook()\n\n# Configure in SDK options\noptions = ClaudeAgentOptions(\n    hooks={\n        \"PreToolUse\": [\n            HookMatcher(matcher=None, hooks=[permission_hook])\n        ]\n    },\n)\n</code></pre>"},{"location":"api/permissions/#permission-handlers","title":"Permission Handlers","text":""},{"location":"api/permissions/#cli-handler","title":"CLI Handler","text":""},{"location":"api/permissions/#reachy_agent.permissions.handlers.cli_handler.CLIPermissionHandler","title":"<code>CLIPermissionHandler(console=None)</code>","text":"<p>               Bases: <code>PermissionHandler</code></p> <p>CLI-based permission handler using Rich for formatting.</p> <p>Displays confirmation prompts and notifications in the terminal with color-coded, formatted output.</p> <p>Features: - Rich panels for confirmation requests - Color-coded notifications by permission tier - Formatted tables for tool parameters - Timeout handling for confirmations</p> Example <pre><code>handler = CLIPermissionHandler()\n\n# Request confirmation (blocks until user responds or timeout)\napproved = await handler.request_confirmation(\n    tool_name=\"mcp__calendar__create_event\",\n    reason=\"This will create a new calendar event\",\n    tool_input={\"title\": \"Meeting\", \"date\": \"2024-01-15\"},\n    timeout_seconds=60.0,\n)\n\n# Display notification\nawait handler.notify(\n    tool_name=\"mcp__slack__send_message\",\n    message=\"Sent message to #general\",\n    tier=2,\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>console</code> <p>Rich Console instance for output.</p> <p>Initialize CLI permission handler.</p> <p>Parameters:</p> Name Type Description Default <code>console</code> <code>Console | None</code> <p>Optional Rich Console instance.     If not provided, creates a new one.</p> <code>None</code> Source code in <code>src/reachy_agent/permissions/handlers/cli_handler.py</code> <pre><code>def __init__(self, console: Console | None = None) -&gt; None:\n    \"\"\"Initialize CLI permission handler.\n\n    Args:\n        console: Optional Rich Console instance.\n                If not provided, creates a new one.\n    \"\"\"\n    self.console = console or Console()\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.cli_handler.CLIPermissionHandler.display_error","title":"<code>display_error(tool_name, error, code=None)</code>  <code>async</code>","text":"<p>Display error message in CLI.</p> <p>Shows a red error message with optional error code.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool that caused the error.</p> required <code>error</code> <code>str</code> <p>Error message.</p> required <code>code</code> <code>str | None</code> <p>Optional error code.</p> <code>None</code> Source code in <code>src/reachy_agent/permissions/handlers/cli_handler.py</code> <pre><code>async def display_error(\n    self,\n    tool_name: str,\n    error: str,\n    code: str | None = None,\n) -&gt; None:\n    \"\"\"Display error message in CLI.\n\n    Shows a red error message with optional error code.\n\n    Args:\n        tool_name: Name of the tool that caused the error.\n        error: Error message.\n        code: Optional error code.\n    \"\"\"\n    code_part = f\"[{code}] \" if code else \"\"\n    self.console.print(\n        f\"[red bold]\u2717 Error in {tool_name}:[/red bold] \"\n        f\"[red]{code_part}{error}[/red]\"\n    )\n\n    log.debug(\n        \"Displayed error\",\n        tool_name=tool_name,\n        error=error,\n        code=code,\n    )\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.cli_handler.CLIPermissionHandler.notify","title":"<code>notify(tool_name, message, tier=2)</code>  <code>async</code>","text":"<p>Display notification in CLI.</p> <p>Shows a color-coded notification message based on permission tier.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool that was executed.</p> required <code>message</code> <code>str</code> <p>Notification message.</p> required <code>tier</code> <code>int</code> <p>Permission tier (affects color).</p> <code>2</code> Source code in <code>src/reachy_agent/permissions/handlers/cli_handler.py</code> <pre><code>async def notify(\n    self,\n    tool_name: str,\n    message: str,\n    tier: int = 2,\n) -&gt; None:\n    \"\"\"Display notification in CLI.\n\n    Shows a color-coded notification message based on permission tier.\n\n    Args:\n        tool_name: Name of the tool that was executed.\n        message: Notification message.\n        tier: Permission tier (affects color).\n    \"\"\"\n    color = self.TIER_COLORS.get(tier, \"white\")\n    tier_name = self.TIER_NAMES.get(tier, f\"Tier {tier}\")\n\n    # Format: [TIER] tool_name: message\n    prefix = f\"[{color}][{tier_name}][/{color}]\"\n    tool_part = f\"[bold]{tool_name}[/bold]\"\n\n    self.console.print(f\"{prefix} {tool_part}: {message}\")\n\n    log.debug(\n        \"Displayed notification\",\n        tool_name=tool_name,\n        tier=tier,\n    )\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.cli_handler.CLIPermissionHandler.on_tool_complete","title":"<code>on_tool_complete(tool_name, result, duration_ms)</code>  <code>async</code>","text":"<p>Show tool execution completion.</p> <p>Displays a success indicator with duration.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the completed tool.</p> required <code>result</code> <code>Any</code> <p>The tool's return value.</p> required <code>duration_ms</code> <code>int</code> <p>Execution time in milliseconds.</p> required Source code in <code>src/reachy_agent/permissions/handlers/cli_handler.py</code> <pre><code>async def on_tool_complete(\n    self,\n    tool_name: str,\n    result: Any,\n    duration_ms: int,\n) -&gt; None:\n    \"\"\"Show tool execution completion.\n\n    Displays a success indicator with duration.\n\n    Args:\n        tool_name: Name of the completed tool.\n        result: The tool's return value.\n        duration_ms: Execution time in milliseconds.\n    \"\"\"\n    self.console.print(\n        f\"[dim]\u2713 {tool_name} completed in {duration_ms}ms[/dim]\"\n    )\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.cli_handler.CLIPermissionHandler.on_tool_start","title":"<code>on_tool_start(tool_name, tool_input)</code>  <code>async</code>","text":"<p>Show tool execution start.</p> <p>Displays a subtle indicator that a tool is running.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool being executed.</p> required <code>tool_input</code> <code>dict[str, Any]</code> <p>The tool's input parameters.</p> required Source code in <code>src/reachy_agent/permissions/handlers/cli_handler.py</code> <pre><code>async def on_tool_start(\n    self,\n    tool_name: str,\n    tool_input: dict[str, Any],\n) -&gt; None:\n    \"\"\"Show tool execution start.\n\n    Displays a subtle indicator that a tool is running.\n\n    Args:\n        tool_name: Name of the tool being executed.\n        tool_input: The tool's input parameters.\n    \"\"\"\n    self.console.print(f\"[dim]\u25b6 Executing {tool_name}...[/dim]\")\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.cli_handler.CLIPermissionHandler.print_permission_rules","title":"<code>print_permission_rules(rules)</code>","text":"<p>Display permission rules in a formatted table.</p> <p>Utility method for showing all configured permission rules.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[tuple[str, int, str]]</code> <p>List of (pattern, tier, reason) tuples.</p> required Source code in <code>src/reachy_agent/permissions/handlers/cli_handler.py</code> <pre><code>def print_permission_rules(\n    self,\n    rules: list[tuple[str, int, str]],\n) -&gt; None:\n    \"\"\"Display permission rules in a formatted table.\n\n    Utility method for showing all configured permission rules.\n\n    Args:\n        rules: List of (pattern, tier, reason) tuples.\n    \"\"\"\n    table = Table(title=\"Permission Rules\", show_header=True)\n    table.add_column(\"Pattern\", style=\"cyan\")\n    table.add_column(\"Tier\", style=\"yellow\")\n    table.add_column(\"Reason\", style=\"dim\")\n\n    for pattern, tier, reason in rules:\n        tier_name = self.TIER_NAMES.get(tier, str(tier))\n        color = self.TIER_COLORS.get(tier, \"white\")\n        table.add_row(\n            pattern,\n            f\"[{color}]{tier_name}[/{color}]\",\n            reason,\n        )\n\n    self.console.print(table)\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.cli_handler.CLIPermissionHandler.request_confirmation","title":"<code>request_confirmation(tool_name, reason, tool_input, timeout_seconds=60.0)</code>  <code>async</code>","text":"<p>Display confirmation prompt in CLI.</p> <p>Shows a formatted panel with tool details and waits for user input (y/n). Returns False on timeout.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool requiring confirmation.</p> required <code>reason</code> <code>str</code> <p>Human-readable explanation.</p> required <code>tool_input</code> <code>dict[str, Any]</code> <p>Tool parameters to display.</p> required <code>timeout_seconds</code> <code>float</code> <p>Maximum wait time.</p> <code>60.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if user confirmed, False if denied or timeout.</p> Source code in <code>src/reachy_agent/permissions/handlers/cli_handler.py</code> <pre><code>async def request_confirmation(\n    self,\n    tool_name: str,\n    reason: str,\n    tool_input: dict[str, Any],\n    timeout_seconds: float = 60.0,\n) -&gt; bool:\n    \"\"\"Display confirmation prompt in CLI.\n\n    Shows a formatted panel with tool details and waits for\n    user input (y/n). Returns False on timeout.\n\n    Args:\n        tool_name: Name of the tool requiring confirmation.\n        reason: Human-readable explanation.\n        tool_input: Tool parameters to display.\n        timeout_seconds: Maximum wait time.\n\n    Returns:\n        True if user confirmed, False if denied or timeout.\n    \"\"\"\n    # Build parameter table\n    table = Table(show_header=True, header_style=\"bold cyan\", box=None)\n    table.add_column(\"Parameter\", style=\"cyan\")\n    table.add_column(\"Value\", style=\"white\")\n\n    for key, value in tool_input.items():\n        # Truncate long values\n        str_value = str(value)\n        if len(str_value) &gt; 80:\n            str_value = str_value[:77] + \"...\"\n        table.add_row(key, str_value)\n\n    # Create confirmation panel\n    content = Text()\n    content.append(f\"{reason}\\n\\n\", style=\"white\")\n\n    panel = Panel(\n        table,\n        title=f\"[yellow bold]\ud83d\udd12 Confirmation Required: {tool_name}[/yellow bold]\",\n        subtitle=f\"[dim]Timeout in {int(timeout_seconds)}s[/dim]\",\n        border_style=\"yellow\",\n        padding=(1, 2),\n    )\n\n    self.console.print()\n    self.console.print(panel)\n    self.console.print(f\"[dim]{reason}[/dim]\")\n    self.console.print()\n\n    # Get user confirmation with timeout\n    try:\n        loop = asyncio.get_event_loop()\n        result = await asyncio.wait_for(\n            loop.run_in_executor(\n                None,\n                lambda: Confirm.ask(\n                    \"[yellow bold]Allow this action?[/yellow bold]\",\n                    console=self.console,\n                    default=False,\n                ),\n            ),\n            timeout=timeout_seconds,\n        )\n\n        if result:\n            self.console.print(\"[green]\u2713 Action approved[/green]\")\n            log.info(\"User confirmed action\", tool_name=tool_name)\n        else:\n            self.console.print(\"[red]\u2717 Action denied[/red]\")\n            log.info(\"User denied action\", tool_name=tool_name)\n\n        return result\n\n    except asyncio.TimeoutError:\n        self.console.print(\"[red]\u23f1 Confirmation timed out - action denied[/red]\")\n        log.warning(\n            \"Confirmation timed out\",\n            tool_name=tool_name,\n            timeout_seconds=timeout_seconds,\n        )\n        return False\n</code></pre>"},{"location":"api/permissions/#websocket-handler","title":"WebSocket Handler","text":""},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler","title":"<code>WebSocketPermissionHandler(on_broadcast=None)</code>","text":"<p>               Bases: <code>PermissionHandler</code></p> <p>WebSocket-based permission handler for web dashboard.</p> <p>Sends confirmation requests and notifications to connected WebSocket clients and handles their responses.</p> <p>Features: - Broadcasts to all connected clients - Tracks pending confirmations with asyncio.Future - Timeout handling with automatic denial - JSON message protocol for easy client integration</p> <p>Message Protocol: - confirmation_request: Ask user for approval - confirmation_timeout: Notify that confirmation timed out - notification: Inform user about action - error: Display error message - tool_start: Tool execution started - tool_complete: Tool execution finished</p> Example <pre><code>handler = WebSocketPermissionHandler()\n\n# In WebSocket endpoint:\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    handler.register_client(websocket)\n\n    try:\n        while True:\n            data = await websocket.receive_json()\n            if data[\"type\"] == \"confirmation_response\":\n                await handler.handle_confirmation_response(\n                    data[\"request_id\"],\n                    data[\"approved\"],\n                )\n    finally:\n        handler.unregister_client(websocket)\n</code></pre> <p>Attributes:</p> Name Type Description <code>on_broadcast</code> <p>Optional callback for outgoing messages.</p> <p>Initialize WebSocket permission handler.</p> <p>Parameters:</p> Name Type Description Default <code>on_broadcast</code> <code>Callable[[dict[str, Any]], Any] | None</code> <p>Optional callback for outgoing messages.          Called with message dict for each broadcast.</p> <code>None</code> Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>def __init__(\n    self,\n    on_broadcast: Callable[[dict[str, Any]], Any] | None = None,\n) -&gt; None:\n    \"\"\"Initialize WebSocket permission handler.\n\n    Args:\n        on_broadcast: Optional callback for outgoing messages.\n                     Called with message dict for each broadcast.\n    \"\"\"\n    self._connected_clients: list[Any] = []\n    self._pending_confirmations: dict[str, asyncio.Future[bool]] = {}\n    self._on_broadcast = on_broadcast\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.connected_client_count","title":"<code>connected_client_count</code>  <code>property</code>","text":"<p>Return number of connected clients.</p>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.broadcast_agent_response","title":"<code>broadcast_agent_response(text, turn_number)</code>  <code>async</code>","text":"<p>Broadcast an agent response to all clients.</p> <p>Utility method for sending agent responses through WebSocket.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The agent's response text.</p> required <code>turn_number</code> <code>int</code> <p>The conversation turn number.</p> required Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>async def broadcast_agent_response(\n    self,\n    text: str,\n    turn_number: int,\n) -&gt; None:\n    \"\"\"Broadcast an agent response to all clients.\n\n    Utility method for sending agent responses through WebSocket.\n\n    Args:\n        text: The agent's response text.\n        turn_number: The conversation turn number.\n    \"\"\"\n    await self._broadcast({\n        \"type\": \"agent_response\",\n        \"text\": text,\n        \"turn_number\": turn_number,\n    })\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.broadcast_status_update","title":"<code>broadcast_status_update(status)</code>  <code>async</code>","text":"<p>Broadcast a status update to all clients.</p> <p>Utility method for sending robot/agent status updates.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>dict[str, Any]</code> <p>Status dictionary with agent state info.</p> required Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>async def broadcast_status_update(\n    self,\n    status: dict[str, Any],\n) -&gt; None:\n    \"\"\"Broadcast a status update to all clients.\n\n    Utility method for sending robot/agent status updates.\n\n    Args:\n        status: Status dictionary with agent state info.\n    \"\"\"\n    await self._broadcast({\n        \"type\": \"status_update\",\n        \"status\": status,\n    })\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.display_error","title":"<code>display_error(tool_name, error, code=None)</code>  <code>async</code>","text":"<p>Send error message via WebSocket.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool that caused the error.</p> required <code>error</code> <code>str</code> <p>Error message.</p> required <code>code</code> <code>str | None</code> <p>Optional error code.</p> <code>None</code> Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>async def display_error(\n    self,\n    tool_name: str,\n    error: str,\n    code: str | None = None,\n) -&gt; None:\n    \"\"\"Send error message via WebSocket.\n\n    Args:\n        tool_name: Name of the tool that caused the error.\n        error: Error message.\n        code: Optional error code.\n    \"\"\"\n    await self._broadcast({\n        \"type\": \"error\",\n        \"tool_name\": tool_name,\n        \"error\": error,\n        \"code\": code,\n    })\n\n    log.debug(\n        \"Sent error\",\n        tool_name=tool_name,\n        error=error,\n        code=code,\n    )\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.handle_confirmation_response","title":"<code>handle_confirmation_response(request_id, approved)</code>  <code>async</code>","text":"<p>Handle confirmation response from client.</p> <p>Called when a client sends a confirmation_response message.</p> <p>Parameters:</p> Name Type Description Default <code>request_id</code> <code>str</code> <p>ID of the confirmation request.</p> required <code>approved</code> <code>bool</code> <p>Whether the user approved the action.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the request was found and handled.</p> Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>async def handle_confirmation_response(\n    self,\n    request_id: str,\n    approved: bool,\n) -&gt; bool:\n    \"\"\"Handle confirmation response from client.\n\n    Called when a client sends a confirmation_response message.\n\n    Args:\n        request_id: ID of the confirmation request.\n        approved: Whether the user approved the action.\n\n    Returns:\n        True if the request was found and handled.\n    \"\"\"\n    future = self._pending_confirmations.get(request_id)\n\n    if future and not future.done():\n        future.set_result(approved)\n        log.debug(\n            \"Handled confirmation response\",\n            request_id=request_id,\n            approved=approved,\n        )\n        return True\n\n    log.warning(\n        \"Confirmation response for unknown request\",\n        request_id=request_id,\n    )\n    return False\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.notify","title":"<code>notify(tool_name, message, tier=2)</code>  <code>async</code>","text":"<p>Send notification via WebSocket.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool that was executed.</p> required <code>message</code> <code>str</code> <p>Notification message.</p> required <code>tier</code> <code>int</code> <p>Permission tier for styling.</p> <code>2</code> Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>async def notify(\n    self,\n    tool_name: str,\n    message: str,\n    tier: int = 2,\n) -&gt; None:\n    \"\"\"Send notification via WebSocket.\n\n    Args:\n        tool_name: Name of the tool that was executed.\n        message: Notification message.\n        tier: Permission tier for styling.\n    \"\"\"\n    await self._broadcast({\n        \"type\": \"notification\",\n        \"tool_name\": tool_name,\n        \"message\": message,\n        \"tier\": tier,\n    })\n\n    log.debug(\n        \"Sent notification\",\n        tool_name=tool_name,\n        tier=tier,\n    )\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.on_tool_complete","title":"<code>on_tool_complete(tool_name, result, duration_ms)</code>  <code>async</code>","text":"<p>Broadcast tool execution completion.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the completed tool.</p> required <code>result</code> <code>Any</code> <p>The tool's return value (may be truncated).</p> required <code>duration_ms</code> <code>int</code> <p>Execution time in milliseconds.</p> required Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>async def on_tool_complete(\n    self,\n    tool_name: str,\n    result: Any,\n    duration_ms: int,\n) -&gt; None:\n    \"\"\"Broadcast tool execution completion.\n\n    Args:\n        tool_name: Name of the completed tool.\n        result: The tool's return value (may be truncated).\n        duration_ms: Execution time in milliseconds.\n    \"\"\"\n    # Serialize result, truncating if too large\n    try:\n        result_str = json.dumps(result)\n        if len(result_str) &gt; 1000:\n            result_str = result_str[:997] + \"...\"\n        serialized_result = json.loads(result_str)\n    except (TypeError, ValueError):\n        serialized_result = str(result)[:1000]\n\n    await self._broadcast({\n        \"type\": \"tool_complete\",\n        \"tool_name\": tool_name,\n        \"result\": serialized_result,\n        \"duration_ms\": duration_ms,\n    })\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.on_tool_start","title":"<code>on_tool_start(tool_name, tool_input)</code>  <code>async</code>","text":"<p>Broadcast tool execution start.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool being executed.</p> required <code>tool_input</code> <code>dict[str, Any]</code> <p>The tool's input parameters.</p> required Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>async def on_tool_start(\n    self,\n    tool_name: str,\n    tool_input: dict[str, Any],\n) -&gt; None:\n    \"\"\"Broadcast tool execution start.\n\n    Args:\n        tool_name: Name of the tool being executed.\n        tool_input: The tool's input parameters.\n    \"\"\"\n    await self._broadcast({\n        \"type\": \"tool_start\",\n        \"tool_name\": tool_name,\n        \"tool_input\": tool_input,\n    })\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.register_client","title":"<code>register_client(websocket)</code>","text":"<p>Register a WebSocket client for broadcasts.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <code>Any</code> <p>The WebSocket connection to register.</p> required Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>def register_client(self, websocket: Any) -&gt; None:\n    \"\"\"Register a WebSocket client for broadcasts.\n\n    Args:\n        websocket: The WebSocket connection to register.\n    \"\"\"\n    if websocket not in self._connected_clients:\n        self._connected_clients.append(websocket)\n        log.debug(\n            \"WebSocket client registered\",\n            total_clients=len(self._connected_clients),\n        )\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.request_confirmation","title":"<code>request_confirmation(tool_name, reason, tool_input, timeout_seconds=60.0)</code>  <code>async</code>","text":"<p>Request confirmation via WebSocket.</p> <p>Broadcasts a confirmation request to all connected clients and waits for a response.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool requiring confirmation.</p> required <code>reason</code> <code>str</code> <p>Human-readable explanation.</p> required <code>tool_input</code> <code>dict[str, Any]</code> <p>Tool parameters to display.</p> required <code>timeout_seconds</code> <code>float</code> <p>Maximum wait time.</p> <code>60.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if user confirmed, False if denied or timeout.</p> Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>async def request_confirmation(\n    self,\n    tool_name: str,\n    reason: str,\n    tool_input: dict[str, Any],\n    timeout_seconds: float = 60.0,\n) -&gt; bool:\n    \"\"\"Request confirmation via WebSocket.\n\n    Broadcasts a confirmation request to all connected clients\n    and waits for a response.\n\n    Args:\n        tool_name: Name of the tool requiring confirmation.\n        reason: Human-readable explanation.\n        tool_input: Tool parameters to display.\n        timeout_seconds: Maximum wait time.\n\n    Returns:\n        True if user confirmed, False if denied or timeout.\n    \"\"\"\n    request_id = str(uuid4())\n\n    # Create future for response\n    future: asyncio.Future[bool] = asyncio.Future()\n    self._pending_confirmations[request_id] = future\n\n    # Broadcast confirmation request\n    await self._broadcast({\n        \"type\": \"confirmation_request\",\n        \"request_id\": request_id,\n        \"tool_name\": tool_name,\n        \"reason\": reason,\n        \"tool_input\": tool_input,\n        \"timeout_seconds\": timeout_seconds,\n    })\n\n    log.debug(\n        \"Sent confirmation request\",\n        request_id=request_id,\n        tool_name=tool_name,\n    )\n\n    try:\n        result = await asyncio.wait_for(future, timeout=timeout_seconds)\n        log.info(\n            \"Confirmation response received\",\n            request_id=request_id,\n            approved=result,\n        )\n        return result\n\n    except asyncio.TimeoutError:\n        # Notify clients of timeout\n        await self._broadcast({\n            \"type\": \"confirmation_timeout\",\n            \"request_id\": request_id,\n            \"tool_name\": tool_name,\n        })\n\n        log.warning(\n            \"Confirmation timed out\",\n            request_id=request_id,\n            tool_name=tool_name,\n            timeout_seconds=timeout_seconds,\n        )\n        return False\n\n    finally:\n        # Clean up pending confirmation\n        self._pending_confirmations.pop(request_id, None)\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.handlers.web_handler.WebSocketPermissionHandler.unregister_client","title":"<code>unregister_client(websocket)</code>","text":"<p>Unregister a WebSocket client.</p> <p>Parameters:</p> Name Type Description Default <code>websocket</code> <code>Any</code> <p>The WebSocket connection to unregister.</p> required Source code in <code>src/reachy_agent/permissions/handlers/web_handler.py</code> <pre><code>def unregister_client(self, websocket: Any) -&gt; None:\n    \"\"\"Unregister a WebSocket client.\n\n    Args:\n        websocket: The WebSocket connection to unregister.\n    \"\"\"\n    if websocket in self._connected_clients:\n        self._connected_clients.remove(websocket)\n        log.debug(\n            \"WebSocket client unregistered\",\n            total_clients=len(self._connected_clients),\n        )\n</code></pre>"},{"location":"api/permissions/#audit-storage","title":"Audit Storage","text":""},{"location":"api/permissions/#reachy_agent.permissions.storage.sqlite_audit.SQLiteAuditStorage","title":"<code>SQLiteAuditStorage(db_path=DEFAULT_DB_PATH, retention_days=DEFAULT_RETENTION_DAYS)</code>","text":"<p>SQLite-based audit log storage.</p> <p>Stores tool execution records in a local SQLite database for compliance, debugging, and analytics purposes.</p> <p>The storage automatically manages retention by deleting records older than the configured retention period.</p> Example <pre><code>storage = SQLiteAuditStorage()\n\n# Store a record\nawait storage.store(AuditRecord(\n    id=str(uuid4()),\n    timestamp=datetime.now(),\n    tool_name=\"mcp__reachy__move_head\",\n    tool_input={\"direction\": \"left\"},\n    permission_tier=1,\n    decision=\"allowed\",\n))\n\n# Query recent records\nrecords = await storage.get_recent(limit=10)\n\n# Clean up old records\ndeleted = await storage.cleanup_old(days=7)\n</code></pre> <p>Attributes:</p> Name Type Description <code>db_path</code> <p>Path to the SQLite database file.</p> <code>retention_days</code> <p>Number of days to retain records.</p> <p>Initialize SQLite audit storage.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>Path | str</code> <p>Path to the SQLite database file.</p> <code>DEFAULT_DB_PATH</code> <code>retention_days</code> <code>int</code> <p>Number of days to retain records.</p> <code>DEFAULT_RETENTION_DAYS</code> Source code in <code>src/reachy_agent/permissions/storage/sqlite_audit.py</code> <pre><code>def __init__(\n    self,\n    db_path: Path | str = DEFAULT_DB_PATH,\n    retention_days: int = DEFAULT_RETENTION_DAYS,\n) -&gt; None:\n    \"\"\"Initialize SQLite audit storage.\n\n    Args:\n        db_path: Path to the SQLite database file.\n        retention_days: Number of days to retain records.\n    \"\"\"\n    self.db_path = Path(db_path).expanduser()\n    self.retention_days = retention_days\n    self._initialized = False\n    self._lock = asyncio.Lock()\n</code></pre>"},{"location":"api/permissions/#reachy_agent.permissions.storage.sqlite_audit.SQLiteAuditStorage.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close any open connections (no-op for SQLite, but good interface).</p> Source code in <code>src/reachy_agent/permissions/storage/sqlite_audit.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close any open connections (no-op for SQLite, but good interface).\"\"\"\n    pass\n</code></pre>"},{"location":"api/sdk-motion-control/","title":"SDK Motion Control API Reference","text":""},{"location":"api/sdk-motion-control/#overview","title":"Overview","text":"<p>The SDK Motion Control system provides low-latency robot control for Reachy Mini by using the official Python SDK with Zenoh pub/sub transport instead of HTTP REST API calls.</p>"},{"location":"api/sdk-motion-control/#purpose","title":"Purpose","text":"<p>Direct SDK control bypasses HTTP overhead, reducing motion control latency from 10-50ms (HTTP) to 1-5ms (Zenoh). This improvement is critical for smooth, responsive motion at the blend controller's 15-20Hz update rate.</p>"},{"location":"api/sdk-motion-control/#latency-comparison","title":"Latency Comparison","text":"Transport Latency Use Case SDK (Zenoh) 1-5ms Real-time motion control (blend controller) HTTP REST 10-50ms One-off commands, fallback, status queries"},{"location":"api/sdk-motion-control/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<p>The SDK client implements a circuit breaker for reliability:</p> <ol> <li>Normal Operation: SDK handles all motion commands</li> <li>Failure Detection: After 5 consecutive SDK failures, circuit opens</li> <li>Fallback Mode: HTTP REST API takes over motion control</li> <li>Recovery: Successful SDK reconnection closes circuit, resumes SDK mode</li> </ol> <p>This ensures graceful degradation when the SDK connection becomes unstable while maintaining low latency during normal operation.</p>"},{"location":"api/sdk-motion-control/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    BC[MotionBlendController&lt;br/&gt;15-20Hz Loop] --&gt; Decision{SDK Available?}\n\n    Decision --&gt;|Yes + Connected| SDK[ReachySDKClient]\n    Decision --&gt;|No / Failed| HTTP[DaemonClient HTTP]\n\n    SDK --&gt;|set_pose&lt;br/&gt;1-5ms| Zenoh[Zenoh Transport]\n    HTTP --&gt;|send_pose&lt;br/&gt;10-50ms| REST[REST API :8000]\n\n    Zenoh --&gt; Daemon[Reachy Daemon]\n    REST --&gt; Daemon\n\n    SDK -.-&gt;|5 failures| CB[Circuit Breaker]\n    CB -.-&gt;|Opens| HTTP\n\n    SDK --&gt;|Success| Reset[Reset Failures]\n    Reset -.-&gt;|Closes| CB\n\n    style SDK fill:#90EE90\n    style HTTP fill:#FFB6C1\n    style CB fill:#FFD700\n    style Daemon fill:#87CEEB</code></pre>"},{"location":"api/sdk-motion-control/#component-interaction","title":"Component Interaction","text":"<pre><code>sequenceDiagram\n    participant BC as BlendController\n    participant SDK as ReachySDKClient\n    participant HTTP as DaemonClient\n    participant Zenoh as Zenoh Transport\n    participant REST as REST API\n    participant Daemon as Reachy Daemon\n\n    BC-&gt;&gt;SDK: set_pose(HeadPose)\n    alt SDK Connected\n        SDK-&gt;&gt;Zenoh: set_target(matrix, antennas)\n        Zenoh-&gt;&gt;Daemon: Zenoh message (1-5ms)\n        Daemon--&gt;&gt;SDK: Success\n        SDK--&gt;&gt;BC: True\n    else SDK Failed (&lt; 5 failures)\n        SDK--&gt;&gt;BC: False\n        BC-&gt;&gt;SDK: Retry next tick\n    else SDK Failed (&gt;= 5 failures)\n        Note over SDK: Circuit Breaker Opens\n        BC-&gt;&gt;HTTP: send_pose(HeadPose)\n        HTTP-&gt;&gt;REST: POST /head/pose\n        REST-&gt;&gt;Daemon: HTTP (10-50ms)\n        Daemon--&gt;&gt;HTTP: 200 OK\n        HTTP--&gt;&gt;BC: Success\n    end</code></pre>"},{"location":"api/sdk-motion-control/#coordinate-systems","title":"Coordinate Systems","text":"<p>The SDK uses different coordinate conventions than the Reachy Agent's HeadPose format. The <code>ReachySDKClient</code> handles all conversions automatically.</p>"},{"location":"api/sdk-motion-control/#head-pose-conversion","title":"Head Pose Conversion","text":"<p>Our Convention (degrees): - Pitch: Positive = head tilts up, Negative = head tilts down - Yaw: Positive = head turns right, Negative = head turns left - Roll: Positive = head rolls right, Negative = head rolls left</p> <p>SDK Convention (radians + 4x4 matrix): - Pitch: INVERTED - Positive = head tilts down, Negative = head tilts up - Yaw: Same direction - Roll: Same direction - Format: 4x4 homogeneous transformation matrix (ZYX Euler convention)</p>"},{"location":"api/sdk-motion-control/#transformation-formulas","title":"Transformation Formulas","text":"<pre><code># Degrees to radians with pitch inversion\nroll_rad = math.radians(pose.roll)\npitch_rad = math.radians(-pose.pitch)  # INVERTED\nyaw_rad = math.radians(pose.yaw)\n\n# ZYX Euler to rotation matrix: R = Rz(yaw) * Ry(pitch) * Rx(roll)\nmatrix = np.eye(4)\nmatrix[0, 0] = cos(yaw) * cos(pitch)\nmatrix[0, 1] = cos(yaw) * sin(pitch) * sin(roll) - sin(yaw) * cos(roll)\nmatrix[0, 2] = cos(yaw) * sin(pitch) * cos(roll) + sin(yaw) * sin(roll)\nmatrix[1, 0] = sin(yaw) * cos(pitch)\nmatrix[1, 1] = sin(yaw) * sin(pitch) * sin(roll) + cos(yaw) * cos(roll)\nmatrix[1, 2] = sin(yaw) * sin(pitch) * cos(roll) - cos(yaw) * sin(roll)\nmatrix[2, 0] = -sin(pitch)\nmatrix[2, 1] = cos(pitch) * sin(roll)\nmatrix[2, 2] = cos(pitch) * cos(roll)\n</code></pre>"},{"location":"api/sdk-motion-control/#antenna-conversion","title":"Antenna Conversion","text":"<p>Our Convention (degrees): - 0\u00b0: Antennas flat/back (horizontal) - 90\u00b0: Antennas vertical (straight up)</p> <p>SDK Convention (radians): - 0 rad: Antennas vertical (straight up) - \u03c0/2 rad: Antennas flat/back (horizontal)</p>"},{"location":"api/sdk-motion-control/#transformation-formula","title":"Transformation Formula","text":"<pre><code># Our degrees to SDK radians (inverted mapping)\nleft_rad = math.radians(90.0 - left_deg)\nright_rad = math.radians(90.0 - right_deg)\n</code></pre> <p>Examples: | Our Degrees | SDK Radians | Position | |-------------|-------------|----------| | 0\u00b0 | \u03c0/2 (1.571) | Flat/back | | 45\u00b0 | \u03c0/4 (0.785) | 45\u00b0 angle | | 90\u00b0 | 0 | Vertical |</p>"},{"location":"api/sdk-motion-control/#configuration","title":"Configuration","text":""},{"location":"api/sdk-motion-control/#sdkclientconfig","title":"SDKClientConfig","text":"<p>Configuration dataclass for the SDK client.</p> <pre><code>@dataclass\nclass SDKClientConfig:\n    enabled: bool = True\n    robot_name: str = \"reachy_mini\"\n    max_workers: int = 1\n    connect_timeout_seconds: float = 10.0\n    fallback_to_http: bool = True\n    localhost_only: bool = True\n</code></pre>"},{"location":"api/sdk-motion-control/#attributes","title":"Attributes","text":"<p>enabled - Type: bool - Default: True - Description: Whether to use SDK for motion control. If False, HTTP fallback is always used.</p> <p>robot_name - Type: str - Default: \"reachy_mini\" - Description: Robot name for Zenoh connection discovery. Must match the daemon's robot name.</p> <p>max_workers - Type: int - Default: 1 - Description: Thread pool size for blocking SDK calls. SDK operations run in executor threads to maintain async compatibility.</p> <p>connect_timeout_seconds - Type: float - Default: 10.0 - Description: Timeout for initial SDK connection attempt. Connection failures fall back to HTTP.</p> <p>fallback_to_http - Type: bool - Default: True - Description: Enable HTTP fallback when SDK fails. Recommended for production reliability.</p> <p>localhost_only - Type: bool - Default: True - Description: Only connect to localhost daemons. Set to False for network robots (not recommended for Reachy Mini).</p>"},{"location":"api/sdk-motion-control/#factory-method","title":"Factory Method","text":"<p>from_dict(data: dict[str, Any]) -&gt; SDKClientConfig</p> <p>Create configuration from dictionary (typically from YAML config).</p> <p>Parameters: - <code>data</code> (dict): Configuration dictionary with optional keys matching attribute names</p> <p>Returns: SDKClientConfig instance with defaults for missing keys</p> <p>Example: <pre><code>config_dict = {\n    \"enabled\": True,\n    \"robot_name\": \"reachy_mini\",\n    \"connect_timeout_seconds\": 5.0\n}\nconfig = SDKClientConfig.from_dict(config_dict)\n</code></pre></p>"},{"location":"api/sdk-motion-control/#yaml-configuration","title":"YAML Configuration","text":"<p>Configuration is loaded from <code>config/default.yaml</code>:</p> <pre><code># Reachy SDK - Direct Python SDK for motion control\n# Uses Zenoh pub/sub (1-5ms latency) vs HTTP daemon (10-50ms)\nsdk:\n  enabled: true                    # Use SDK for motion control (blend controller)\n  robot_name: reachy_mini          # Robot name for Zenoh connection\n  connect_timeout_seconds: 10.0    # Timeout for SDK connection\n  fallback_to_http: true           # Fall back to HTTP if SDK fails\n  max_workers: 1                   # Thread pool size for blocking SDK calls\n  localhost_only: true             # Only connect to localhost daemons\n</code></pre>"},{"location":"api/sdk-motion-control/#reachysdkclient-api","title":"ReachySDKClient API","text":""},{"location":"api/sdk-motion-control/#class-reachysdkclient","title":"Class: ReachySDKClient","text":"<p>Direct Python SDK client for Reachy Mini motion control. Wraps blocking SDK calls in asyncio executors for compatibility with async agent architecture.</p> <pre><code>class ReachySDKClient:\n    def __init__(self, config: SDKClientConfig | None = None) -&gt; None\n    async def connect(self) -&gt; bool\n    async def disconnect(self) -&gt; None\n    async def set_pose(self, pose: HeadPose) -&gt; bool\n    def get_status(self) -&gt; dict[str, Any]\n\n    @property\n    def is_connected(self) -&gt; bool\n\n    @property\n    def last_error(self) -&gt; str | None\n</code></pre>"},{"location":"api/sdk-motion-control/#constructor","title":"Constructor","text":"<pre><code>ReachySDKClient(config: SDKClientConfig | None = None) -&gt; None\n</code></pre> <p>Initialize the SDK client.</p> <p>Parameters: - <code>config</code> (SDKClientConfig, optional): Client configuration. Uses defaults if not provided.</p> <p>Example: <pre><code>config = SDKClientConfig(enabled=True, robot_name=\"reachy_mini\")\nclient = ReachySDKClient(config)\n</code></pre></p>"},{"location":"api/sdk-motion-control/#methods","title":"Methods","text":""},{"location":"api/sdk-motion-control/#connect","title":"connect()","text":"<pre><code>async def connect() -&gt; bool\n</code></pre> <p>Connect to the robot via SDK.</p> <p>Purpose: Establish Zenoh connection to the Reachy daemon and initialize thread pool for blocking SDK operations.</p> <p>Returns: bool - <code>True</code>: Connection successful, SDK ready for motion commands - <code>False</code>: Connection failed or SDK disabled in config</p> <p>Raises: Does not raise exceptions - connection errors are logged and return False</p> <p>Behavior: 1. Checks if SDK is enabled in configuration 2. Imports <code>reachy_mini.ReachyMini</code> (lazy import to avoid errors if SDK not installed) 3. Creates ThreadPoolExecutor for blocking SDK calls 4. Connects to robot with configured timeout 5. Logs connection status</p> <p>Example: <pre><code>client = ReachySDKClient()\nif await client.connect():\n    log.info(\"SDK connected\")\n    # Proceed with motion commands\nelse:\n    log.warning(\"SDK connection failed, using HTTP fallback\")\n    # Fall back to HTTP daemon client\n</code></pre></p> <p>Notes: - Connection is non-blocking (runs in executor with timeout) - Daemon must be running before calling connect() - Uses <code>spawn_daemon=False</code> - expects existing daemon - Uses <code>media_backend=\"no_media\"</code> - only motion control needed - Sets SDK log level to WARNING to reduce noise</p>"},{"location":"api/sdk-motion-control/#disconnect","title":"disconnect()","text":"<pre><code>async def disconnect() -&gt; None\n</code></pre> <p>Disconnect from the robot and clean up resources.</p> <p>Purpose: Gracefully close Zenoh connection and shut down thread pool.</p> <p>Behavior: 1. Disconnects from robot SDK 2. Shuts down thread pool executor 3. Resets connection state</p> <p>Example: <pre><code>await client.connect()\n# ... use SDK ...\nawait client.disconnect()\n</code></pre></p>"},{"location":"api/sdk-motion-control/#set_pose","title":"set_pose()","text":"<pre><code>async def set_pose(pose: HeadPose) -&gt; bool\n</code></pre> <p>Send pose to robot via SDK set_target().</p> <p>Purpose: Primary method called by blend controller to update robot pose. Converts HeadPose format to SDK's matrix format and sends via Zenoh.</p> <p>Parameters: - <code>pose</code> (HeadPose): Target pose with roll, pitch, yaw (degrees), left_antenna, right_antenna (degrees)</p> <p>Returns: bool - <code>True</code>: Pose successfully sent to robot - <code>False</code>: SDK not connected, executor not initialized, or send failed</p> <p>Raises: Does not raise exceptions - errors are logged and return False</p> <p>Behavior: 1. Validates SDK connection and executor availability 2. Converts HeadPose degrees to 4x4 transformation matrix 3. Converts antenna degrees to SDK radians 4. Calls robot.set_target() in executor thread (blocking call) 5. Returns success/failure status</p> <p>Rate-Limited Warnings: - Connection warnings throttled to 1/second (avoids spam at 15Hz loop rate) - Executor warnings throttled to 1/second</p> <p>Example: <pre><code>pose = HeadPose(\n    yaw=10.0,      # 10\u00b0 right\n    pitch=5.0,     # 5\u00b0 up\n    roll=0.0,      # No roll\n    left_antenna=80.0,   # Near vertical\n    right_antenna=80.0   # Near vertical\n)\n\nsuccess = await client.set_pose(pose)\nif success:\n    log.debug(\"Pose sent via SDK\")\nelse:\n    # Fall back to HTTP\n    await http_client.send_pose(pose)\n</code></pre></p> <p>Performance: - Zenoh latency: 1-5ms typical - Thread executor overhead: &lt; 1ms - Total latency: 2-6ms (vs 10-50ms for HTTP)</p>"},{"location":"api/sdk-motion-control/#get_status","title":"get_status()","text":"<pre><code>def get_status() -&gt; dict[str, Any]\n</code></pre> <p>Get SDK client status for debugging.</p> <p>Purpose: Provide diagnostic information about SDK connection state and configuration.</p> <p>Returns: dict with keys: - <code>connected</code> (bool): Whether SDK is connected to robot - <code>config</code> (dict): Current configuration values - <code>last_error</code> (str | None): Last error message if any</p> <p>Example: <pre><code>status = client.get_status()\nprint(f\"SDK Connected: {status['connected']}\")\nprint(f\"Robot Name: {status['config']['robot_name']}\")\nif status['last_error']:\n    print(f\"Last Error: {status['last_error']}\")\n</code></pre></p>"},{"location":"api/sdk-motion-control/#properties","title":"Properties","text":""},{"location":"api/sdk-motion-control/#is_connected","title":"is_connected","text":"<pre><code>@property\ndef is_connected(self) -&gt; bool\n</code></pre> <p>Check if connected to robot via SDK.</p> <p>Returns: bool - <code>True</code>: SDK connected and ready for commands - <code>False</code>: SDK disconnected or not initialized</p> <p>Example: <pre><code>if client.is_connected:\n    await client.set_pose(pose)\nelse:\n    await client.connect()\n</code></pre></p>"},{"location":"api/sdk-motion-control/#last_error","title":"last_error","text":"<pre><code>@property\ndef last_error(self) -&gt; str | None\n</code></pre> <p>Get the last error message if any.</p> <p>Returns: str | None - Last error message string - None if no errors occurred</p> <p>Example: <pre><code>if not client.is_connected:\n    print(f\"Connection failed: {client.last_error}\")\n</code></pre></p>"},{"location":"api/sdk-motion-control/#circuit-breaker-pattern_1","title":"Circuit Breaker Pattern","text":"<p>The blend controller implements a circuit breaker to detect persistent SDK failures and automatically switch to HTTP fallback.</p>"},{"location":"api/sdk-motion-control/#failure-threshold","title":"Failure Threshold","text":"<p>SDK_MAX_FAILURES: 5 consecutive failures</p> <p>After 5 consecutive <code>set_pose()</code> failures, the circuit breaker opens and all subsequent motion commands use HTTP fallback.</p>"},{"location":"api/sdk-motion-control/#states","title":"States","text":"<p>Closed (Normal Operation): - SDK handles all motion commands - Failures increment counter - Counter resets on successful pose send</p> <p>Open (Fallback Mode): - HTTP REST API handles all motion commands - SDK is not attempted - State persists until manual reset or reconnection</p>"},{"location":"api/sdk-motion-control/#fallback-behavior","title":"Fallback Behavior","text":"<pre><code>async def _send_pose_to_daemon(self, pose: HeadPose) -&gt; None:\n    sdk_success = False\n    http_success = False\n\n    # Try SDK first (if circuit is closed)\n    if self._sdk_client and not self._sdk_fallback_active:\n        sdk_success = await self._sdk_client.set_pose(pose)\n\n        if sdk_success:\n            # Reset failure count on success\n            self._sdk_failures = 0\n            return\n\n        # Increment failure count\n        self._sdk_failures += 1\n\n        # Open circuit breaker after 5 failures\n        if self._sdk_failures &gt;= 5:\n            log.warning(\"SDK failing, switching to HTTP fallback\")\n            self._sdk_fallback_active = True\n\n    # Fall back to HTTP\n    if self._send_pose:\n        await self._send_pose(pose)\n        http_success = True\n</code></pre>"},{"location":"api/sdk-motion-control/#recovery","title":"Recovery","text":"<p>The circuit breaker automatically closes when SDK successfully reconnects:</p> <pre><code>if sdk_success and self._sdk_failures &gt; 0:\n    self._sdk_failures = 0\n    log.info(\"SDK connection recovered\")\n</code></pre> <p>Manual Reset: <pre><code>controller.reset_sdk_fallback()  # Force retry SDK after fixing issues\n</code></pre></p>"},{"location":"api/sdk-motion-control/#rate-limited-warnings","title":"Rate-Limited Warnings","text":"<p>To avoid log spam at 15Hz loop rate, connection warnings are throttled to 1/second:</p> <pre><code># Rate-limited warning (avoid spam at 15Hz loop rate)\nnow = time.monotonic()\nif not self.is_connected:\n    if now - self._last_disconnected_warning &gt; 1.0:\n        log.warning(\"sdk_set_pose_skipped\", reason=\"not_connected\")\n        self._last_disconnected_warning = now\n</code></pre>"},{"location":"api/sdk-motion-control/#blend-controller-integration","title":"Blend Controller Integration","text":""},{"location":"api/sdk-motion-control/#initialization","title":"Initialization","text":"<p>The <code>MotionBlendController</code> accepts an optional SDK client for direct motion control:</p> <pre><code>from reachy_agent.mcp_servers.reachy.sdk_client import ReachySDKClient, SDKClientConfig\nfrom reachy_agent.behaviors.blend_controller import MotionBlendController\n\n# Create SDK client\nsdk_config = SDKClientConfig(enabled=True, fallback_to_http=True)\nsdk_client = ReachySDKClient(sdk_config)\nawait sdk_client.connect()\n\n# Create blend controller with SDK support\ncontroller = MotionBlendController(\n    config=blend_config,\n    send_pose_callback=daemon_client.send_pose,  # HTTP fallback\n    sdk_client=sdk_client  # Preferred method\n)\n</code></pre>"},{"location":"api/sdk-motion-control/#fallback-chain","title":"Fallback Chain","text":"<p>The blend controller tries motion methods in priority order:</p> <ol> <li>SDK (Zenoh): 1-5ms latency - preferred method</li> <li>HTTP REST: 10-50ms latency - fallback when SDK unavailable</li> </ol> <pre><code># Priority: SDK &gt; HTTP\nif self._sdk_client and not self._sdk_fallback_active:\n    sdk_success = await self._sdk_client.set_pose(pose)\n    if sdk_success:\n        return  # Success via SDK\n    # SDK failed, try HTTP\n\nif self._send_pose:\n    await self._send_pose(pose)  # HTTP fallback\n</code></pre>"},{"location":"api/sdk-motion-control/#health-monitoring","title":"Health Monitoring","text":"<p>The blend controller tracks motion control health across both SDK and HTTP:</p> <p>Health States: - Healthy: At least one transport succeeding - Unhealthy: 10+ consecutive failures across all transports</p> <p>Monitoring Logic: <pre><code>if sdk_success or http_success:\n    self._consecutive_total_failures = 0\n    self._motion_healthy = True\nelif not sdk_success and not http_success:\n    self._consecutive_total_failures += 1\n    if self._consecutive_total_failures &gt;= 10:\n        log.error(\"motion_control_unhealthy\")\n        self._motion_healthy = False\n</code></pre></p>"},{"location":"api/sdk-motion-control/#status-information","title":"Status Information","text":"<p>The blend controller's <code>get_status()</code> method includes SDK diagnostics:</p> <pre><code>status = controller.get_status()\nprint(f\"SDK Connected: {status['sdk_connected']}\")\nprint(f\"SDK Fallback Active: {status['sdk_fallback_active']}\")\nprint(f\"SDK Failures: {status['sdk_failures']}\")\nprint(f\"HTTP Failures: {status['http_failures']}\")\nprint(f\"Motion Healthy: {status['motion_healthy']}\")\n</code></pre>"},{"location":"api/sdk-motion-control/#connection-lifecycle","title":"Connection Lifecycle","text":""},{"location":"api/sdk-motion-control/#typical-startup-sequence","title":"Typical Startup Sequence","text":"<pre><code># 1. Create configuration\nsdk_config = SDKClientConfig.from_dict(config_dict[\"sdk\"])\n\n# 2. Initialize client\nsdk_client = ReachySDKClient(sdk_config)\n\n# 3. Connect to robot\nif await sdk_client.connect():\n    log.info(\"SDK ready for motion control\")\nelse:\n    log.warning(\"SDK unavailable, using HTTP fallback\")\n\n# 4. Integrate with blend controller\ncontroller = MotionBlendController(\n    sdk_client=sdk_client,\n    send_pose_callback=http_fallback\n)\n\n# 5. Start motion control loop\nawait controller.start()\n</code></pre>"},{"location":"api/sdk-motion-control/#graceful-shutdown","title":"Graceful Shutdown","text":"<pre><code># 1. Stop blend controller\nawait controller.stop()\n\n# 2. Disconnect SDK\nif sdk_client.is_connected:\n    await sdk_client.disconnect()\n\n# 3. Cleanup complete\nlog.info(\"Motion control shutdown complete\")\n</code></pre>"},{"location":"api/sdk-motion-control/#reconnection-handling","title":"Reconnection Handling","text":"<p>The SDK client does not automatically reconnect. Application code must handle reconnection:</p> <pre><code># Monitor connection health\nif not sdk_client.is_connected:\n    log.warning(\"SDK disconnected, attempting reconnection\")\n\n    # Reset circuit breaker\n    controller.reset_sdk_fallback()\n\n    # Retry connection\n    if await sdk_client.connect():\n        log.info(\"SDK reconnected\")\n    else:\n        log.error(\"SDK reconnection failed\")\n</code></pre>"},{"location":"api/sdk-motion-control/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/sdk-motion-control/#sdk-not-installed","title":"SDK Not Installed","text":"<p>Symptom: Import error when connecting</p> <p>Error: <pre><code>ModuleNotFoundError: No module named 'reachy_mini'\n</code></pre></p> <p>Solution: <pre><code># Install Reachy SDK\npip install reachy_mini\n\n# Or add to requirements.txt\necho \"reachy_mini&gt;=1.0.0\" &gt;&gt; requirements.txt\npip install -r requirements.txt\n</code></pre></p> <p>Workaround: SDK client gracefully degrades to HTTP if import fails. Set <code>enabled: false</code> in config to skip SDK initialization.</p>"},{"location":"api/sdk-motion-control/#connection-timeout","title":"Connection Timeout","text":"<p>Symptom: SDK connection fails after timeout period</p> <p>Logs: <pre><code>SDK connection failed: timeout after 10.0 seconds\n</code></pre></p> <p>Causes: 1. Reachy daemon not running 2. Zenoh discovery issues 3. Incorrect robot_name configuration 4. Network connectivity problems</p> <p>Solutions:</p> <ol> <li> <p>Verify daemon is running: <pre><code># Check daemon status\nsystemctl status reachy\n\n# Or check process\nps aux | grep reachy\n</code></pre></p> </li> <li> <p>Verify Zenoh connectivity: <pre><code># Test Zenoh discovery\nzenoh-bridge-dds -l debug\n</code></pre></p> </li> <li> <p>Check robot name: <pre><code># Ensure config matches daemon\nsdk:\n  robot_name: reachy_mini  # Must match daemon's robot name\n</code></pre></p> </li> <li> <p>Increase timeout: <pre><code>sdk:\n  connect_timeout_seconds: 20.0  # Give more time for connection\n</code></pre></p> </li> </ol>"},{"location":"api/sdk-motion-control/#coordinate-mismatches","title":"Coordinate Mismatches","text":"<p>Symptom: Robot moves in unexpected directions</p> <p>Causes: 1. Pitch sign inversion not applied 2. Antenna convention mismatch 3. Incorrect Euler angle order</p> <p>Debugging:</p> <ol> <li> <p>Test known poses: <pre><code># Head straight up - should tilt head upward\npose = HeadPose(pitch=20.0, yaw=0.0, roll=0.0)\nawait client.set_pose(pose)\n\n# Antennas vertical - should point straight up\npose = HeadPose(left_antenna=90.0, right_antenna=90.0)\nawait client.set_pose(pose)\n</code></pre></p> </li> <li> <p>Verify transformations: <pre><code># Enable debug logging\nimport logging\nlogging.getLogger(\"reachy_agent.mcp_servers.reachy.sdk_client\").setLevel(logging.DEBUG)\n\n# Check matrix output\nmatrix = client._head_pose_to_matrix(pose)\nprint(f\"Transformation matrix:\\n{matrix}\")\n</code></pre></p> </li> <li> <p>Compare with HTTP: <pre><code># Send same pose via HTTP and SDK\npose = HeadPose(yaw=10.0, pitch=5.0)\n\n# Via SDK\nawait sdk_client.set_pose(pose)\ntime.sleep(2)\n\n# Via HTTP\nawait http_client.send_pose(pose)\n# Verify both produce same robot motion\n</code></pre></p> </li> </ol>"},{"location":"api/sdk-motion-control/#high-latency","title":"High Latency","text":"<p>Symptom: SDK latency higher than expected (&gt; 10ms)</p> <p>Causes: 1. Network overhead (not localhost) 2. Thread pool contention 3. Zenoh configuration issues</p> <p>Solutions:</p> <ol> <li> <p>Ensure localhost connection: <pre><code>sdk:\n  localhost_only: true  # Force localhost for lowest latency\n</code></pre></p> </li> <li> <p>Monitor thread pool: <pre><code>status = sdk_client.get_status()\nprint(f\"Executor workers: {status['config']['max_workers']}\")\n# Increase if bottleneck detected\n</code></pre></p> </li> <li> <p>Profile latency: <pre><code>import time\n\nstart = time.perf_counter()\nawait sdk_client.set_pose(pose)\nelapsed = (time.perf_counter() - start) * 1000\nprint(f\"SDK latency: {elapsed:.2f}ms\")\n</code></pre></p> </li> </ol>"},{"location":"api/sdk-motion-control/#circuit-breaker-stuck-open","title":"Circuit Breaker Stuck Open","text":"<p>Symptom: SDK fallback remains active despite SDK being available</p> <p>Logs: <pre><code>SDK failing consistently, switching to HTTP fallback\n</code></pre></p> <p>Solution: Manually reset circuit breaker after fixing underlying issue</p> <pre><code># Reset circuit breaker to retry SDK\ncontroller.reset_sdk_fallback()\n\n# Verify SDK connection\nif sdk_client.is_connected:\n    log.info(\"SDK ready, circuit breaker reset\")\nelse:\n    # Reconnect if needed\n    await sdk_client.connect()\n    controller.reset_sdk_fallback()\n</code></pre>"},{"location":"api/sdk-motion-control/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"api/sdk-motion-control/#latency-benchmarks","title":"Latency Benchmarks","text":"<p>Typical latencies measured on Raspberry Pi 4 (Reachy Mini hardware):</p> Operation SDK (Zenoh) HTTP REST Improvement set_pose() 2-5ms 15-30ms 5-10x faster Connection 500-2000ms 100-200ms 2-4x slower Disconnect 50-100ms &lt; 10ms 5-10x slower"},{"location":"api/sdk-motion-control/#throughput","title":"Throughput","text":"<p>SDK Maximum Rate: 100Hz (10ms period) - Tested stable at blend controller's 20Hz rate - Zenoh pub/sub designed for high-frequency updates</p> <p>HTTP Maximum Rate: 30Hz (33ms period) - Network stack overhead limits practical rate - Daemon REST API optimized for 15-20Hz</p>"},{"location":"api/sdk-motion-control/#resource-usage","title":"Resource Usage","text":"<p>Memory: - SDK client: ~5-10MB (Zenoh + ReachyMini instance) - Thread pool: ~1MB per worker</p> <p>CPU: - SDK overhead: &lt; 5% on Pi 4 at 20Hz - Zenoh transport: &lt; 3% on Pi 4</p> <p>Network: - Zenoh: Shared memory on localhost (no network overhead) - HTTP: TCP/IP stack overhead even on localhost</p>"},{"location":"api/sdk-motion-control/#related-apis","title":"Related APIs","text":"<ul> <li>DaemonClient API - HTTP fallback transport</li> <li>MotionBlendController - Motion orchestration</li> <li>HeadPose - Pose data structure</li> <li>Reachy Mini SDK - Official SDK documentation</li> </ul>"},{"location":"api/sdk-motion-control/#version-history","title":"Version History","text":"<ul> <li>v1.0.0 (2025-12-31): Initial SDK integration with circuit breaker pattern</li> <li>Zenoh transport for 1-5ms latency</li> <li>Automatic HTTP fallback on SDK failure</li> <li>Coordinate system conversions</li> <li>Thread pool executor for async compatibility</li> </ul>"},{"location":"api/simulation/","title":"Simulation Module API","text":"<p>The simulation module provides MuJoCo-based testing without physical hardware. It bridges the Reachy Agent MCP tools to the Reachy Mini daemon running in simulation mode.</p>"},{"location":"api/simulation/#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph Test[\"Test/Development Code\"]\n        SCRIPT[\"Validation Script&lt;br/&gt;or Test Case\"]\n    end\n\n    subgraph Simulation[\"Simulation Module\"]\n        ADAPTER[\"SimulationAdapter\"]\n        CLIENT[\"ReachyMiniClient\"]\n        LAUNCHER[\"SimulationDaemon\"]\n    end\n\n    subgraph Daemon[\"Reachy Mini Daemon\"]\n        API[\"FastAPI Server&lt;br/&gt;:8765\"]\n        BACKEND[\"MujocoBackend\"]\n    end\n\n    subgraph Physics[\"MuJoCo\"]\n        MODEL[\"MJCF Model\"]\n        ENGINE[\"Physics Engine\"]\n    end\n\n    SCRIPT --&gt; ADAPTER\n    ADAPTER --&gt; CLIENT\n    ADAPTER --&gt; LAUNCHER\n    LAUNCHER --&gt;|\"subprocess\"| API\n    CLIENT --&gt;|\"HTTP\"| API\n    API --&gt; BACKEND\n    BACKEND --&gt; ENGINE\n    ENGINE --&gt; MODEL\n\n    style ADAPTER fill:#e1bee7\n    style CLIENT fill:#bbdefb</code></pre>"},{"location":"api/simulation/#quick-start","title":"Quick Start","text":"<pre><code>from reachy_agent.simulation import create_simulation_adapter\n\nasync def main():\n    # Create adapter with factory function\n    adapter = create_simulation_adapter(\n        scene=\"empty\",      # or \"minimal\" for table with objects\n        headless=True,      # No GUI window (for CI)\n        port=8765,          # Daemon API port\n    )\n\n    # Use as async context manager\n    async with adapter:\n        client = adapter.client\n\n        # Move head left\n        result = await client.move_head(\"left\", speed=\"fast\")\n        print(f\"Move UUID: {result['uuid']}\")\n\n        # Nod twice\n        result = await client.nod(times=2)\n        print(f\"Nod completed: {result['moves']} moves\")\n\n        # Set antenna positions\n        await client.set_antenna_state(left_angle=90, right_angle=45)\n\n# Run\nimport asyncio\nasyncio.run(main())\n</code></pre>"},{"location":"api/simulation/#module-components","title":"Module Components","text":""},{"location":"api/simulation/#simulationconfig","title":"SimulationConfig","text":"<p>Configuration dataclass for the simulation daemon.</p> <pre><code>from reachy_agent.simulation import SimulationConfig, SimulationScene\n\nconfig = SimulationConfig(\n    scene=SimulationScene.MINIMAL,  # Scene with table + objects\n    headless=False,                  # Show MuJoCo viewer\n    host=\"127.0.0.1\",\n    port=8000,\n    startup_timeout=30.0,           # Seconds to wait for daemon\n    health_check_interval=0.5,      # Polling interval\n)\n</code></pre> <p>Fields:</p> Field Type Default Description <code>scene</code> <code>SimulationScene</code> <code>EMPTY</code> Simulation scene to load <code>headless</code> <code>bool</code> <code>False</code> Run without GUI window <code>host</code> <code>str</code> <code>\"127.0.0.1\"</code> Daemon bind address <code>port</code> <code>int</code> <code>8000</code> Daemon API port <code>startup_timeout</code> <code>float</code> <code>30.0</code> Max seconds to wait for startup <code>health_check_interval</code> <code>float</code> <code>0.5</code> Health check polling interval"},{"location":"api/simulation/#simulationscene","title":"SimulationScene","text":"<p>Available simulation scenes:</p> <pre><code>from reachy_agent.simulation import SimulationScene\n\nSimulationScene.EMPTY    # Robot only, no environment\nSimulationScene.MINIMAL  # Robot + table with apple, croissant, duck\n</code></pre>"},{"location":"api/simulation/#simulationdaemon","title":"SimulationDaemon","text":"<p>Manages the Reachy Mini daemon subprocess in simulation mode.</p> <pre><code>from reachy_agent.simulation import SimulationDaemon, SimulationConfig\n\ndaemon = SimulationDaemon(config=SimulationConfig(headless=True))\n\n# Manual lifecycle\nawait daemon.start()\ntry:\n    print(f\"Daemon URL: {daemon.base_url}\")\n    print(f\"Running: {daemon.is_running}\")\n    health = await daemon.health_check()\n    print(f\"Health: {health}\")\nfinally:\n    await daemon.stop()\n\n# Or use context manager\nasync with SimulationDaemon() as daemon:\n    print(f\"Ready at {daemon.base_url}\")\n</code></pre> <p>Properties:</p> Property Type Description <code>base_url</code> <code>str</code> HTTP endpoint (e.g., <code>http://127.0.0.1:8000</code>) <code>is_running</code> <code>bool</code> Whether daemon process is alive <p>Methods:</p> Method Returns Description <code>start()</code> <code>None</code> Launch daemon and wait for health <code>stop()</code> <code>None</code> Gracefully terminate daemon <code>restart()</code> <code>None</code> Stop then start <code>health_check()</code> <code>dict</code> Query daemon status"},{"location":"api/simulation/#simulationadapter","title":"SimulationAdapter","text":"<p>High-level adapter combining daemon lifecycle and client access.</p> <pre><code>from reachy_agent.simulation import SimulationAdapter, SimulationConfig\n\nadapter = SimulationAdapter(\n    config=SimulationConfig(scene=SimulationScene.EMPTY)\n)\n\nasync with adapter:\n    # Access the HTTP client\n    client = adapter.client\n\n    # Check status\n    health = await adapter.health_check()\n    print(f\"Simulation enabled: {health['simulation_enabled']}\")\n</code></pre> <p>Properties:</p> Property Type Description <code>client</code> <code>ReachyMiniClient</code> HTTP client for API calls <code>is_running</code> <code>bool</code> Whether daemon is running <code>base_url</code> <code>str</code> Daemon HTTP endpoint"},{"location":"api/simulation/#reachyminiclient","title":"ReachyMiniClient","text":"<p>HTTP client that maps high-level commands to Reachy Mini daemon API.</p> <pre><code>flowchart LR\n    subgraph Client[\"ReachyMiniClient Methods\"]\n        MH[\"move_head()\"]\n        LA[\"look_at()\"]\n        SA[\"set_antenna_state()\"]\n        N[\"nod()\"]\n        S[\"shake()\"]\n        R[\"rest()\"]\n        W[\"wake_up()\"]\n        SL[\"sleep()\"]\n    end\n\n    subgraph Daemon[\"Daemon Endpoints\"]\n        GOTO[\"/api/move/goto\"]\n        WAKE[\"/api/move/play/wake_up\"]\n        SLEEP[\"/api/move/play/goto_sleep\"]\n        STATUS[\"/api/daemon/status\"]\n    end\n\n    MH --&gt; GOTO\n    LA --&gt; GOTO\n    SA --&gt; GOTO\n    N --&gt; GOTO\n    S --&gt; GOTO\n    R --&gt; GOTO\n    W --&gt; WAKE\n    SL --&gt; SLEEP</code></pre>"},{"location":"api/simulation/#movement-methods","title":"Movement Methods","text":"<p><code>move_head(direction, speed, degrees)</code></p> <p>Move head in a cardinal direction.</p> <pre><code># Basic usage\nawait client.move_head(\"left\")\nawait client.move_head(\"right\", speed=\"fast\")\nawait client.move_head(\"up\", degrees=45)  # Custom angle\n\n# Parameters\ndirection: str  # \"left\", \"right\", \"up\", \"down\", \"front\"\nspeed: str      # \"slow\" (2s), \"normal\" (1s), \"fast\" (0.5s)\ndegrees: float  # Override default 30\u00b0 angle\n</code></pre> <p><code>look_at(roll, pitch, yaw, duration)</code></p> <p>Precise head positioning with Euler angles.</p> <pre><code># Look slightly up and to the left\nawait client.look_at(roll=0, pitch=-15, yaw=20)\n\n# Parameters (all in degrees)\nroll: float     # Tilt left/right\npitch: float    # Up (negative) / Down (positive)\nyaw: float      # Left (positive) / Right (negative)\nduration: float # Movement duration in seconds\n</code></pre> <p><code>set_antenna_state(left_angle, right_angle, duration_ms)</code></p> <p>Control antenna positions for expression.</p> <pre><code># Both antennas up\nawait client.set_antenna_state(left_angle=90, right_angle=90)\n\n# Asymmetric (curious expression)\nawait client.set_antenna_state(left_angle=30, right_angle=60)\n\n# Parameters\nleft_angle: float   # 0 (down) to 90 (up) degrees\nright_angle: float  # 0 (down) to 90 (up) degrees\nduration_ms: int    # Movement duration in milliseconds\n</code></pre>"},{"location":"api/simulation/#gesture-methods","title":"Gesture Methods","text":"<p><code>nod(times, speed)</code></p> <p>Perform nodding gesture (yes).</p> <pre><code>await client.nod(times=2, speed=\"normal\")\n\n# Returns: {\"status\": \"ok\", \"moves\": 4}\n</code></pre> <p><code>shake(times, speed)</code></p> <p>Perform head shake gesture (no).</p> <pre><code>await client.shake(times=3, speed=\"fast\")\n\n# Returns: {\"status\": \"ok\", \"moves\": 6}\n</code></pre> <p><code>rest()</code></p> <p>Return to neutral resting pose.</p> <pre><code>await client.rest()\n# Head centered, antennas at 45\u00b0, body yaw 0\n</code></pre>"},{"location":"api/simulation/#lifecycle-methods","title":"Lifecycle Methods","text":"<p><code>wake_up()</code></p> <p>Activate the robot (enable motors).</p> <pre><code>result = await client.wake_up()\n# Returns: {\"uuid\": \"...\"}\n</code></pre> <p><code>sleep()</code></p> <p>Deactivate the robot (disable motors, safe position).</p> <pre><code>result = await client.sleep()\n# Returns: {\"uuid\": \"...\"}\n</code></pre>"},{"location":"api/simulation/#status-methods","title":"Status Methods","text":"<p><code>get_status()</code></p> <p>Get daemon status information.</p> <pre><code>status = await client.get_status()\n# {\n#   \"robot_name\": \"reachy_mini\",\n#   \"state\": \"running\",\n#   \"simulation_enabled\": True,\n#   \"version\": \"1.2.3\"\n# }\n</code></pre> <p><code>get_full_state()</code></p> <p>Get complete robot state.</p> <pre><code>state = await client.get_full_state()\n# {\n#   \"head_pose\": {\"x\": 0, \"y\": 0, \"z\": 0, \"roll\": 0, \"pitch\": 0, \"yaw\": 0},\n#   \"antennas_position\": [0.785, 0.785],  # radians\n#   \"body_yaw\": 0\n# }\n</code></pre>"},{"location":"api/simulation/#factory-function","title":"Factory Function","text":"<p><code>create_simulation_adapter(scene, headless, port)</code></p> <p>Convenience function to create a configured adapter.</p> <pre><code>from reachy_agent.simulation import create_simulation_adapter\n\n# For development (with GUI)\nadapter = create_simulation_adapter(scene=\"minimal\", headless=False)\n\n# For CI/testing\nadapter = create_simulation_adapter(scene=\"empty\", headless=True, port=8765)\n</code></pre>"},{"location":"api/simulation/#daemon-api-mapping","title":"Daemon API Mapping","text":"<p>The client translates high-level commands to the Reachy Mini daemon's HTTP API.</p>"},{"location":"api/simulation/#simulation-mock-daemon","title":"Simulation (Mock Daemon)","text":"Client Method HTTP Endpoint Payload <code>move_head(\"left\")</code> <code>POST /api/move/goto</code> <code>{\"head_pose\": {\"yaw\": 0.52}, \"duration\": 1.0}</code> <code>look_at(pitch=-10)</code> <code>POST /api/move/goto</code> <code>{\"head_pose\": {\"pitch\": -0.17}, \"duration\": 1.0}</code> <code>set_antenna_state(45, 45)</code> <code>POST /api/move/goto</code> <code>{\"antennas\": [0.78, 0.78], \"duration\": 0.5}</code> <code>nod(times=2)</code> Multiple <code>POST /api/move/goto</code> Alternating pitch values <code>wake_up()</code> <code>POST /api/move/play/wake_up</code> None <code>sleep()</code> <code>POST /api/move/play/goto_sleep</code> None <code>get_status()</code> <code>GET /api/daemon/status</code> None"},{"location":"api/simulation/#real-hardware-pollen-daemon","title":"Real Hardware (Pollen Daemon)","text":"<p>Important: Real hardware uses <code>/api/move/set_target</code> instead of <code>/api/move/goto</code> for smooth movements. The <code>goto</code> API includes <code>x</code>, <code>y</code>, <code>z</code> position fields that default to 0, which causes the head to snap to origin. The <code>ReachyDaemonClient</code> auto-detects the backend and uses the appropriate API.</p> Client Method HTTP Endpoint Payload <code>move_head(\"left\")</code> <code>POST /api/move/set_target</code> <code>{\"target_head_pose\": {\"yaw\": 0.52}}</code> <code>look_at(pitch=-10)</code> <code>POST /api/move/set_target</code> <code>{\"target_head_pose\": {\"pitch\": -0.17}}</code> <code>set_antenna_state(45, 45)</code> <code>POST /api/move/set_target</code> <code>{\"target_antennas\": [0.78, 0.78]}</code>"},{"location":"api/simulation/#running-the-validation-script","title":"Running the Validation Script","text":"<p>The validation script tests all simulation capabilities:</p> <pre><code># Headless mode (for CI)\npython scripts/validate_simulation.py --headless\n\n# With GUI (requires mjpython on macOS)\nmjpython scripts/validate_simulation.py\n\n# Different scene\npython scripts/validate_simulation.py --scene minimal --headless\n</code></pre> <p>Output: <pre><code>============================================================\nReachy Agent MuJoCo Simulation Validation\n============================================================\n\nConfiguration:\n  Scene: empty\n  Headless: True\n\n[1/7] Starting simulation daemon...\n  Daemon started successfully!\n\n[2/7] Validating daemon health...\n  PASSED\n\n[3/7] Validating lifecycle controls...\n  PASSED\n\n[4/7] Validating head movement...\n  move_head(left): OK - uuid=8464395e...\n  move_head(right): OK - uuid=15b19951...\n  PASSED\n\n[5/7] Validating antenna control...\n  PASSED\n\n[6/7] Validating gestures...\n  nod: OK - 4 moves\n  shake: OK - 4 moves\n  PASSED\n\n[7/7] Validating precise positioning...\n  PASSED\n\n============================================================\nVALIDATION RESULT: ALL TESTS PASSED\n============================================================\n</code></pre></p>"},{"location":"api/simulation/#testing","title":"Testing","text":""},{"location":"api/simulation/#unit-tests-no-daemon-required","title":"Unit Tests (No Daemon Required)","text":"<pre><code># Run unit tests only\npytest tests/simulation/test_simulation_adapter.py -v -k \"Unit\"\n</code></pre>"},{"location":"api/simulation/#integration-tests-requires-mujoco","title":"Integration Tests (Requires MuJoCo)","text":"<pre><code># Run all simulation tests (starts daemon automatically)\npytest tests/simulation/ -v\n\n# Or with simulation marker\npytest -m simulation -v\n</code></pre>"},{"location":"api/simulation/#test-coverage","title":"Test Coverage","text":"<pre><code>pytest tests/simulation/ --cov=src/reachy_agent/simulation --cov-report=html\n</code></pre>"},{"location":"api/simulation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/simulation/#daemon-wont-start","title":"Daemon Won't Start","text":"<pre><code>RuntimeError: Failed to start simulation daemon. Command not found: mjpython\n</code></pre> <p>Solution: Install MuJoCo support: <pre><code>uv pip install \"reachy-mini[mujoco]\"\n</code></pre></p>"},{"location":"api/simulation/#daemon-times-out","title":"Daemon Times Out","text":"<pre><code>RuntimeError: Simulation daemon did not become healthy within 30.0s\n</code></pre> <p>Solutions: 1. Increase timeout: <code>SimulationConfig(startup_timeout=60.0)</code> 2. Check port availability: <code>lsof -i :8000</code> 3. Check daemon logs in stderr</p>"},{"location":"api/simulation/#gui-not-showing-macos","title":"GUI Not Showing (macOS)","text":"<p>On macOS, MuJoCo requires <code>mjpython</code> for GUI rendering:</p> <pre><code># Install mjpython\npip install mujoco\n\n# Run with mjpython\nmjpython scripts/validate_simulation.py\n</code></pre>"},{"location":"api/simulation/#port-already-in-use","title":"Port Already in Use","text":"<pre><code>OSError: [Errno 48] Address already in use\n</code></pre> <p>Solution: Use a different port: <pre><code>adapter = create_simulation_adapter(port=8766)\n</code></pre></p>"},{"location":"api/simulation/#platform-notes","title":"Platform Notes","text":"Platform GUI Support Command macOS <code>mjpython</code> required <code>mjpython -m reachy_mini.daemon.app.main --sim</code> Linux Native <code>python -m reachy_mini.daemon.app.main --sim</code> Windows Native <code>python -m reachy_mini.daemon.app.main --sim</code> <p>All platforms support <code>--headless</code> for CI/testing.</p>"},{"location":"api/voice-pipeline/","title":"Voice Pipeline API Reference","text":""},{"location":"api/voice-pipeline/#overview","title":"Overview","text":"<p>The Voice Pipeline module provides a complete voice interaction system for the Reachy robot, orchestrating wake word detection, speech recognition, agent processing, and text-to-speech playback. It implements a state machine that coordinates audio capture, voice activity detection, OpenAI Realtime API integration, and error recovery.</p> <p>Module: <code>reachy_agent.voice.pipeline</code></p> <p>Key Features: - State machine-based pipeline coordination - Multi-persona support with wake word switching - Automatic error recovery and degraded mode operation - Real-time audio streaming with OpenAI Realtime API - Voice activity detection (VAD) for speech segmentation - Integration with Reachy motion behaviors</p>"},{"location":"api/voice-pipeline/#voicepipeline","title":"VoicePipeline","text":"<p>Main orchestrator class that coordinates all voice interaction components.</p>"},{"location":"api/voice-pipeline/#constructor","title":"Constructor","text":"<pre><code>VoicePipeline(\n    agent: ReachyAgentLoop | None = None,\n    config: VoicePipelineConfig = VoicePipelineConfig(),\n    on_state_change: Callable[[VoicePipelineState, VoicePipelineState], None] | None = None,\n    on_transcription: Callable[[str], None] | None = None,\n    on_response: Callable[[str], None] | None = None,\n    on_degraded_mode: Callable[[str, bool], None] | None = None,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>agent</code> (ReachyAgentLoop | None): The agent instance for processing user input. Optional for testing.</li> <li><code>config</code> (VoicePipelineConfig): Configuration for pipeline components. Default: <code>VoicePipelineConfig()</code></li> <li><code>on_state_change</code> (Callable | None): Callback invoked on state transitions with signature <code>(from_state, to_state)</code>. Default: None</li> <li><code>on_transcription</code> (Callable | None): Callback invoked when user speech is transcribed with signature <code>(text)</code>. Default: None</li> <li><code>on_response</code> (Callable | None): Callback invoked when agent response text is available with signature <code>(text)</code>. Default: None</li> <li><code>on_degraded_mode</code> (Callable | None): Callback invoked when a component enters/exits degraded mode with signature <code>(component, is_degraded)</code>. Default: None</li> </ul> <p>Returns: VoicePipeline instance</p> <p>Example: <pre><code>from reachy_agent.agent import ReachyAgentLoop\nfrom reachy_agent.voice import VoicePipeline, VoicePipelineConfig\n\nagent = ReachyAgentLoop()\nconfig = VoicePipelineConfig()\n\npipeline = VoicePipeline(\n    agent=agent,\n    config=config,\n    on_transcription=lambda text: print(f\"User said: {text}\"),\n    on_response=lambda text: print(f\"Agent replied: {text}\"),\n)\n\nawait pipeline.initialize()\nawait pipeline.start()\n</code></pre></p>"},{"location":"api/voice-pipeline/#properties","title":"Properties","text":""},{"location":"api/voice-pipeline/#state","title":"state","text":"<ul> <li>Type: <code>VoicePipelineState</code></li> <li>Access: Read-only</li> <li>Description: Current state of the voice pipeline</li> </ul> <p>Example: <pre><code>&gt;&gt;&gt; pipeline.state\n&lt;VoicePipelineState.LISTENING_WAKE: 'listening_wake'&gt;\n</code></pre></p>"},{"location":"api/voice-pipeline/#is_running","title":"is_running","text":"<ul> <li>Type: <code>bool</code></li> <li>Access: Read-only</li> <li>Description: Whether the pipeline main loop is active</li> </ul> <p>Example: <pre><code>&gt;&gt;&gt; pipeline.is_running\nTrue\n</code></pre></p>"},{"location":"api/voice-pipeline/#is_degraded","title":"is_degraded","text":"<ul> <li>Type: <code>bool</code></li> <li>Access: Read-only</li> <li>Description: Whether any component is operating in degraded mode</li> </ul> <p>Example: <pre><code>&gt;&gt;&gt; pipeline.is_degraded\nFalse\n</code></pre></p>"},{"location":"api/voice-pipeline/#degraded_modes","title":"degraded_modes","text":"<ul> <li>Type: <code>set[str]</code></li> <li>Access: Read-only</li> <li>Description: Set of component names currently in degraded mode (e.g., <code>{\"audio\", \"wake_word\"}</code>)</li> </ul> <p>Example: <pre><code>&gt;&gt;&gt; pipeline.degraded_modes\n{\"audio\"}\n</code></pre></p>"},{"location":"api/voice-pipeline/#current_persona","title":"current_persona","text":"<ul> <li>Type: <code>PersonaConfig | None</code></li> <li>Access: Read-only</li> <li>Description: The currently active persona configuration, or None if no persona is active</li> </ul> <p>Example: <pre><code>&gt;&gt;&gt; pipeline.current_persona.display_name\n'Major Kusanagi'\n&gt;&gt;&gt; pipeline.current_persona.voice\n'nova'\n</code></pre></p>"},{"location":"api/voice-pipeline/#methods","title":"Methods","text":""},{"location":"api/voice-pipeline/#initialize","title":"initialize()","text":"<pre><code>async def initialize() -&gt; bool\n</code></pre> <p>Initialize all pipeline components (audio, wake word, VAD, OpenAI client).</p> <p>Returns: <code>bool</code> - True if initialization succeeded (possibly in degraded mode), False if initialization completely failed</p> <p>Raises: - <code>AudioDeviceError</code>: If audio devices cannot be initialized - <code>WakeWordError</code>: If wake word models cannot be loaded - <code>VADError</code>: If VAD model cannot be loaded</p> <p>Example: <pre><code>&gt;&gt;&gt; success = await pipeline.initialize()\n&gt;&gt;&gt; if success:\n...     print(f\"Initialized, degraded: {pipeline.is_degraded}\")\nInitialized, degraded: False\n</code></pre></p>"},{"location":"api/voice-pipeline/#start","title":"start()","text":"<pre><code>async def start() -&gt; None\n</code></pre> <p>Start the voice pipeline main loop. Must call <code>initialize()</code> first.</p> <p>Raises: - <code>RuntimeError</code>: If pipeline is already running or not initialized - <code>StateTransitionError</code>: If pipeline is in an invalid state for starting</p> <p>Example: <pre><code>&gt;&gt;&gt; await pipeline.start()\n# Pipeline is now listening for wake word\n</code></pre></p>"},{"location":"api/voice-pipeline/#stop","title":"stop()","text":"<pre><code>async def stop() -&gt; None\n</code></pre> <p>Stop the voice pipeline main loop and clean up resources.</p> <p>Example: <pre><code>&gt;&gt;&gt; await pipeline.stop()\n# Pipeline has stopped, resources cleaned up\n</code></pre></p>"},{"location":"api/voice-pipeline/#get_recovery_status","title":"get_recovery_status()","text":"<pre><code>def get_recovery_status() -&gt; dict\n</code></pre> <p>Get detailed status of recovery manager and degraded modes.</p> <p>Returns: <code>dict</code> - Status report with keys: - <code>degraded_modes</code>: List of component names in degraded mode - <code>strategies</code>: Dict mapping component names to recovery strategy details - <code>active_recoveries</code>: Number of active recovery attempts</p> <p>Example: <pre><code>&gt;&gt;&gt; status = pipeline.get_recovery_status()\n&gt;&gt;&gt; status[\"degraded_modes\"]\n['audio']\n&gt;&gt;&gt; status[\"strategies\"][\"audio\"][\"attempt\"]\n2\n</code></pre></p>"},{"location":"api/voice-pipeline/#context-manager-support","title":"Context Manager Support","text":"<p>VoicePipeline supports async context manager protocol for automatic resource cleanup:</p> <pre><code>async with VoicePipeline(agent=agent, config=config) as pipeline:\n    await pipeline.start()\n    # Pipeline runs...\n# Automatically stopped and cleaned up\n</code></pre>"},{"location":"api/voice-pipeline/#voicepipelineconfig","title":"VoicePipelineConfig","text":"<p>Configuration dataclass for all pipeline components.</p> <pre><code>@dataclass\nclass VoicePipelineConfig:\n    audio: AudioConfig = field(default_factory=AudioConfig)\n    wake_word: WakeWordConfig = field(default_factory=WakeWordConfig)\n    vad: VADConfig = field(default_factory=VADConfig)\n    realtime: RealtimeConfig = field(default_factory=RealtimeConfig)\n\n    # Timeout settings\n    listening_timeout: float = 30.0\n    processing_timeout: float = 45.0\n    playback_timeout: float = 60.0\n\n    # Persona settings\n    personas: dict[str, Any] = field(default_factory=dict)\n    default_persona: str = \"\"\n</code></pre> <p>Fields:</p> <ul> <li><code>audio</code> (AudioConfig): Audio device and recording configuration. See audio.py</li> <li><code>wake_word</code> (WakeWordConfig): Wake word detection configuration. See wake_word.py</li> <li><code>vad</code> (VADConfig): Voice activity detection configuration. See vad.py</li> <li><code>realtime</code> (RealtimeConfig): OpenAI Realtime API configuration. See openai_realtime.py</li> <li><code>listening_timeout</code> (float): Maximum seconds to wait for speech after wake word. Default: 30.0</li> <li><code>processing_timeout</code> (float): Maximum seconds to wait for agent response. Default: 45.0</li> <li><code>playback_timeout</code> (float): Maximum seconds to wait for TTS playback. Default: 60.0</li> <li><code>personas</code> (dict): Persona configurations keyed by wake word model name</li> <li><code>default_persona</code> (str): Wake word model name for the default persona</li> </ul> <p>Example: <pre><code>from reachy_agent.voice import (\n    VoicePipelineConfig,\n    AudioConfig,\n    WakeWordConfig,\n    VADConfig,\n    RealtimeConfig,\n)\n\nconfig = VoicePipelineConfig(\n    audio=AudioConfig(\n        input_device_index=None,  # Auto-detect\n        sample_rate=16000,\n    ),\n    wake_word=WakeWordConfig(\n        models=[\"hey_motoko\"],\n        sensitivity=0.5,\n    ),\n    listening_timeout=20.0,\n    personas={\n        \"hey_motoko\": {\n            \"name\": \"motoko\",\n            \"voice\": \"nova\",\n            \"display_name\": \"Major Kusanagi\",\n            \"prompt_path\": \"config/prompts/motoko.txt\",\n        }\n    },\n    default_persona=\"hey_motoko\",\n)\n</code></pre></p>"},{"location":"api/voice-pipeline/#voicepipelinestate","title":"VoicePipelineState","text":"<p>Enumeration of pipeline state machine states.</p> <pre><code>class VoicePipelineState(Enum):\n    IDLE = \"idle\"\n    LISTENING_WAKE = \"listening_wake\"\n    WAKE_DETECTED = \"wake_detected\"\n    LISTENING_SPEECH = \"listening_speech\"\n    PROCESSING_SPEECH = \"processing_speech\"\n    PLAYING_RESPONSE = \"playing_response\"\n    ERROR = \"error\"\n</code></pre> <p>Values:</p> <ul> <li><code>IDLE</code>: Pipeline not active, minimal resource usage</li> <li><code>LISTENING_WAKE</code>: Listening for wake word detection</li> <li><code>WAKE_DETECTED</code>: Wake word heard, preparing to listen for speech</li> <li><code>LISTENING_SPEECH</code>: Recording user speech with VAD</li> <li><code>PROCESSING_SPEECH</code>: Sending transcription to agent for processing</li> <li><code>PLAYING_RESPONSE</code>: Playing TTS audio response from agent</li> <li><code>ERROR</code>: Error occurred, attempting recovery</li> </ul> <p>Valid State Transitions:</p> From State To States <code>IDLE</code> <code>LISTENING_WAKE</code> <code>LISTENING_WAKE</code> <code>WAKE_DETECTED</code>, <code>ERROR</code>, <code>IDLE</code> <code>WAKE_DETECTED</code> <code>LISTENING_SPEECH</code>, <code>ERROR</code>, <code>LISTENING_WAKE</code> <code>LISTENING_SPEECH</code> <code>PROCESSING_SPEECH</code>, <code>ERROR</code>, <code>LISTENING_WAKE</code> <code>PROCESSING_SPEECH</code> <code>PLAYING_RESPONSE</code>, <code>ERROR</code>, <code>LISTENING_WAKE</code> <code>PLAYING_RESPONSE</code> <code>LISTENING_WAKE</code>, <code>ERROR</code> <code>ERROR</code> <code>LISTENING_WAKE</code>, <code>IDLE</code> <p>Example: <pre><code>def on_state_change(from_state: VoicePipelineState, to_state: VoicePipelineState):\n    if to_state == VoicePipelineState.WAKE_DETECTED:\n        print(\"Wake word detected!\")\n    elif to_state == VoicePipelineState.PROCESSING_SPEECH:\n        print(\"Processing user request...\")\n</code></pre></p>"},{"location":"api/voice-pipeline/#personaconfig","title":"PersonaConfig","text":"<p>Configuration for a persona tied to a wake word.</p> <pre><code>@dataclass\nclass PersonaConfig:\n    name: str\n    wake_word_model: str\n    voice: str\n    display_name: str\n    prompt_path: str\n    traits: dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Fields:</p> <ul> <li><code>name</code> (str): Internal identifier (e.g., \"motoko\", \"batou\")</li> <li><code>wake_word_model</code> (str): OpenWakeWord model name (e.g., \"hey_motoko\")</li> <li><code>voice</code> (str): OpenAI TTS voice name. Valid: <code>alloy</code>, <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, <code>shimmer</code></li> <li><code>display_name</code> (str): Human-readable name (e.g., \"Major Kusanagi\")</li> <li><code>prompt_path</code> (str): Path to the persona's system prompt file</li> <li><code>traits</code> (dict): Optional personality traits for runtime reference</li> </ul> <p>Validation Rules:</p> <ul> <li><code>voice</code> must be one of: <code>alloy</code>, <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, <code>shimmer</code></li> <li><code>prompt_path</code> file must exist</li> </ul> <p>Raises: - <code>ValueError</code>: If voice is invalid or prompt_path doesn't exist</p> <p>Factory Method:</p> <pre><code>@classmethod\ndef from_dict(cls, wake_word_model: str, data: dict[str, Any]) -&gt; PersonaConfig\n</code></pre> <p>Create a PersonaConfig from a dictionary.</p> <p>Parameters: - <code>wake_word_model</code> (str): Wake word model name to use - <code>data</code> (dict): Dictionary with keys: <code>name</code>, <code>voice</code>, <code>display_name</code>, <code>prompt_path</code>, <code>traits</code> (optional)</p> <p>Returns: <code>PersonaConfig</code> instance</p> <p>Example: <pre><code>from reachy_agent.voice import PersonaConfig\n\n# Direct construction\npersona = PersonaConfig(\n    name=\"motoko\",\n    wake_word_model=\"hey_motoko\",\n    voice=\"nova\",\n    display_name=\"Major Kusanagi\",\n    prompt_path=\"config/prompts/motoko.txt\",\n    traits={\"mood\": \"focused\", \"expertise\": \"security\"},\n)\n\n# From dictionary\ndata = {\n    \"name\": \"batou\",\n    \"voice\": \"onyx\",\n    \"display_name\": \"Batou\",\n    \"prompt_path\": \"config/prompts/batou.txt\",\n}\npersona = PersonaConfig.from_dict(\"hey_batou\", data)\n</code></pre></p>"},{"location":"api/voice-pipeline/#personamanager","title":"PersonaManager","text":"<p>Manages persona registration and lookup.</p> <pre><code>@dataclass\nclass PersonaManager:\n    personas: dict[str, PersonaConfig] = field(default_factory=dict)\n    current_persona: PersonaConfig | None = None\n    default_persona_key: str = \"\"\n</code></pre> <p>Fields:</p> <ul> <li><code>personas</code> (dict): Mapping of wake word model names to PersonaConfig</li> <li><code>current_persona</code> (PersonaConfig | None): Currently active persona</li> <li><code>default_persona_key</code> (str): Wake word model name of the default persona</li> </ul>"},{"location":"api/voice-pipeline/#methods_1","title":"Methods","text":""},{"location":"api/voice-pipeline/#register_persona","title":"register_persona()","text":"<pre><code>def register_persona(config: PersonaConfig) -&gt; None\n</code></pre> <p>Register a persona for a wake word model.</p> <p>Parameters: - <code>config</code> (PersonaConfig): Persona configuration to register</p> <p>Example: <pre><code>manager = PersonaManager()\nmanager.register_persona(persona)\n</code></pre></p>"},{"location":"api/voice-pipeline/#get_persona","title":"get_persona()","text":"<pre><code>def get_persona(wake_word_model: str) -&gt; PersonaConfig | None\n</code></pre> <p>Look up a persona by wake word model name.</p> <p>Parameters: - <code>wake_word_model</code> (str): Wake word model name</p> <p>Returns: <code>PersonaConfig | None</code> - The persona if found, None otherwise</p> <p>Example: <pre><code>&gt;&gt;&gt; persona = manager.get_persona(\"hey_motoko\")\n&gt;&gt;&gt; persona.display_name\n'Major Kusanagi'\n</code></pre></p>"},{"location":"api/voice-pipeline/#set_default","title":"set_default()","text":"<pre><code>def set_default(wake_word_model: str) -&gt; bool\n</code></pre> <p>Set the default persona by wake word model name.</p> <p>Parameters: - <code>wake_word_model</code> (str): Wake word model name to set as default</p> <p>Returns: <code>bool</code> - True if persona exists and was set, False otherwise</p> <p>Example: <pre><code>&gt;&gt;&gt; manager.set_default(\"hey_motoko\")\nTrue\n</code></pre></p>"},{"location":"api/voice-pipeline/#get_default","title":"get_default()","text":"<pre><code>def get_default() -&gt; PersonaConfig | None\n</code></pre> <p>Get the default persona.</p> <p>Returns: <code>PersonaConfig | None</code> - The default persona if set, None otherwise</p> <p>Example: <pre><code>&gt;&gt;&gt; default = manager.get_default()\n&gt;&gt;&gt; default.name\n'motoko'\n</code></pre></p>"},{"location":"api/voice-pipeline/#factory-method","title":"Factory Method","text":"<pre><code>@classmethod\ndef from_config(cls, config: dict[str, Any]) -&gt; PersonaManager\n</code></pre> <p>Create a PersonaManager from a configuration dictionary.</p> <p>Parameters: - <code>config</code> (dict): Configuration with keys:   - <code>personas</code> (dict): Mapping of wake word models to persona data   - <code>default_persona</code> (str, optional): Default wake word model</p> <p>Returns: <code>PersonaManager</code> - Configured persona manager</p> <p>Example: <pre><code>config = {\n    \"personas\": {\n        \"hey_motoko\": {\n            \"name\": \"motoko\",\n            \"voice\": \"nova\",\n            \"display_name\": \"Major Kusanagi\",\n            \"prompt_path\": \"config/prompts/motoko.txt\",\n        },\n        \"hey_batou\": {\n            \"name\": \"batou\",\n            \"voice\": \"onyx\",\n            \"display_name\": \"Batou\",\n            \"prompt_path\": \"config/prompts/batou.txt\",\n        },\n    },\n    \"default_persona\": \"hey_motoko\",\n}\n\nmanager = PersonaManager.from_config(config)\n</code></pre></p>"},{"location":"api/voice-pipeline/#usage-examples","title":"Usage Examples","text":""},{"location":"api/voice-pipeline/#basic-voice-pipeline","title":"Basic Voice Pipeline","text":"<pre><code>from reachy_agent.agent import ReachyAgentLoop\nfrom reachy_agent.voice import VoicePipeline, VoicePipelineConfig\n\n# Create agent and pipeline\nagent = ReachyAgentLoop()\npipeline = VoicePipeline(agent=agent)\n\n# Initialize and start\nawait pipeline.initialize()\nawait pipeline.start()\n\n# Pipeline now listens for wake word, processes speech, plays responses\n# Stop when done\nawait pipeline.stop()\n</code></pre>"},{"location":"api/voice-pipeline/#with-callbacks","title":"With Callbacks","text":"<pre><code>def on_state_change(from_state, to_state):\n    print(f\"State: {from_state.value} -&gt; {to_state.value}\")\n\ndef on_transcription(text):\n    print(f\"User said: {text}\")\n\ndef on_response(text):\n    print(f\"Agent replied: {text}\")\n\npipeline = VoicePipeline(\n    agent=agent,\n    on_state_change=on_state_change,\n    on_transcription=on_transcription,\n    on_response=on_response,\n)\n\nawait pipeline.initialize()\nawait pipeline.start()\n</code></pre>"},{"location":"api/voice-pipeline/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code>from reachy_agent.voice import AudioConfig, WakeWordConfig\n\nconfig = VoicePipelineConfig(\n    audio=AudioConfig(\n        sample_rate=16000,\n        chunk_duration_ms=50,\n    ),\n    wake_word=WakeWordConfig(\n        models=[\"hey_motoko\"],\n        sensitivity=0.6,\n    ),\n    listening_timeout=25.0,\n)\n\npipeline = VoicePipeline(agent=agent, config=config)\n</code></pre>"},{"location":"api/voice-pipeline/#using-context-manager","title":"Using Context Manager","text":"<pre><code>async with VoicePipeline(agent=agent, config=config) as pipeline:\n    await pipeline.start()\n\n    # Wait for keyboard interrupt or other signal\n    await asyncio.Event().wait()\n\n# Pipeline automatically stopped and cleaned up\n</code></pre>"},{"location":"api/voice-pipeline/#multi-persona-setup","title":"Multi-Persona Setup","text":"<pre><code>config = VoicePipelineConfig(\n    personas={\n        \"hey_motoko\": {\n            \"name\": \"motoko\",\n            \"voice\": \"nova\",\n            \"display_name\": \"Major Kusanagi\",\n            \"prompt_path\": \"config/prompts/motoko.txt\",\n        },\n        \"hey_batou\": {\n            \"name\": \"batou\",\n            \"voice\": \"onyx\",\n            \"display_name\": \"Batou\",\n            \"prompt_path\": \"config/prompts/batou.txt\",\n        },\n    },\n    default_persona=\"hey_motoko\",\n    wake_word=WakeWordConfig(\n        models=[\"hey_motoko\", \"hey_batou\"],  # Both wake words active\n    ),\n)\n\npipeline = VoicePipeline(agent=agent, config=config)\n# Pipeline will switch personas when different wake words are detected\n</code></pre>"},{"location":"api/voice-pipeline/#related-modules","title":"Related Modules","text":"<ul> <li>AudioManager - Audio device management and streaming</li> <li>WakeWordDetector - Wake word detection with OpenWakeWord</li> <li>VoiceActivityDetector - Speech segmentation with Silero VAD</li> <li>OpenAIRealtimeClient - OpenAI Realtime API integration</li> <li>PipelineRecoveryManager - Error recovery and degraded mode handling</li> </ul>"},{"location":"architecture/agent-loop/","title":"ReachyAgentLoop Deep Dive","text":"<p>The <code>ReachyAgentLoop</code> is the brain of your Reachy robot. It implements the Perceive \u2192 Think \u2192 Act cycle - a common pattern in robotics AI. What makes this implementation special is how it uses the Claude Agent SDK to handle the \"Think\" phase, while integrating permission enforcement and memory for personalization.</p>"},{"location":"architecture/agent-loop/#the-core-interaction-cycle","title":"The Core Interaction Cycle","text":"<pre><code>%%{init: {'theme': 'default'}}%%\nflowchart TB\n    START[process_input Called] --&gt; PERCEIVE\n\n    subgraph PERCEIVE[\"1. PERCEIVE\"]\n        P1[Receive user input]\n        P2[Pause idle behavior]\n        P3[Build context]\n        P1 --&gt; P2 --&gt; P3\n    end\n\n    PERCEIVE --&gt; THINK\n\n    subgraph THINK[\"2. THINK\"]\n        T1[Send to Claude via SDK]\n        T2[Claude decides on actions]\n        T3[Stream response back]\n        T1 --&gt; T2 --&gt; T3\n    end\n\n    THINK --&gt; ACT\n\n    subgraph ACT[\"3. ACT\"]\n        A1[SDK executes MCP tools]\n        A2[Permission hooks check]\n        A3[Robot performs actions]\n        A1 --&gt; A2 --&gt; A3\n    end\n\n    ACT --&gt; RETURN[Return Response]\n    RETURN --&gt; RESUME[Resume idle behavior]\n\n    style PERCEIVE fill:#e3f2fd,stroke:#1565c0\n    style THINK fill:#fff3e0,stroke:#f57c00\n    style ACT fill:#e8f5e9,stroke:#2e7d32</code></pre>"},{"location":"architecture/agent-loop/#key-components","title":"Key Components","text":""},{"location":"architecture/agent-loop/#1-state-machine-agentstate","title":"1. State Machine (AgentState)","text":"<p>The agent tracks its current state to prevent concurrent processing:</p> State Description <code>INITIALIZING</code> Starting up, loading config <code>READY</code> Accepting input <code>PROCESSING</code> Handling a query <code>ERROR</code> Something went wrong <code>SHUTDOWN</code> Shutting down gracefully <pre><code>class AgentState(str, Enum):\n    INITIALIZING = \"initializing\"\n    READY = \"ready\"\n    PROCESSING = \"processing\"\n    ERROR = \"error\"\n    SHUTDOWN = \"shutdown\"\n</code></pre>"},{"location":"architecture/agent-loop/#2-initialization","title":"2. Initialization","text":"<p>The <code>__init__</code> method configures all major components:</p> Component Purpose <code>config</code> Configuration from YAML files <code>daemon_url</code> Where to reach the robot hardware <code>_client</code> The Claude SDK client <code>_permission_evaluator</code> Checks if tools are allowed <code>_idle_controller</code> Makes robot look around when idle <code>_memory_manager</code> ChromaDB + SQLite for personalization"},{"location":"architecture/agent-loop/#3-mcp-server-configuration","title":"3. MCP Server Configuration","text":"<p>MCP (Model Context Protocol) servers expose tools to Claude via a standard protocol. The SDK handles these as subprocess stdio servers - it spawns a Python process for each server and communicates over stdin/stdout.</p> <pre><code>def _build_mcp_servers(self) -&gt; dict[str, dict[str, Any]]:\n    servers = {}\n\n    # Reachy server for robot control (23 tools)\n    servers[\"reachy\"] = {\n        \"type\": \"stdio\",\n        \"command\": python_executable,\n        \"args\": [\"-m\", \"reachy_agent.mcp_servers.reachy\", self.daemon_url],\n    }\n\n    # Memory server for personalization (4 tools)\n    if self._enable_memory:\n        servers[\"memory\"] = {\n            \"type\": \"stdio\",\n            \"command\": python_executable,\n            \"args\": [\"-m\", \"reachy_agent.mcp_servers.memory\"],\n        }\n\n    return servers\n</code></pre>"},{"location":"architecture/agent-loop/#4-permission-hooks","title":"4. Permission Hooks","text":"<p>The 4-tier permission system is enforced via SDK hooks. When Claude wants to use a tool, the SDK calls the permission hook before execution.</p> <pre><code>%%{init: {'theme': 'default'}}%%\nflowchart LR\n    REQ[Tool Request] --&gt; HOOK{Permission Hook}\n    HOOK --&gt;|Tier 1| AUTO[\"Allow&lt;br/&gt;{}\"]\n    HOOK --&gt;|Tier 2| NOTIFY[\"Allow + Log&lt;br/&gt;{}\"]\n    HOOK --&gt;|Tier 3| CONFIRM[\"Ask User&lt;br/&gt;{ask}\"]\n    HOOK --&gt;|Tier 4| BLOCK[\"Block&lt;br/&gt;{deny}\"]\n\n    style AUTO fill:#c8e6c9,stroke:#2e7d32\n    style NOTIFY fill:#fff9c4,stroke:#f9a825\n    style CONFIRM fill:#ffe0b2,stroke:#ef6c00\n    style BLOCK fill:#ffcdd2,stroke:#c62828</code></pre> <p>The hook returns one of:</p> Return Value Meaning Tier <code>{}</code> Allow execution 1 (Autonomous), 2 (Notify) <code>{\"permissionDecision\": \"ask\"}</code> Prompt user 3 (Confirm) <code>{\"permissionDecision\": \"deny\"}</code> Block execution 4 (Forbidden) <pre><code>async def _permission_hook(self, input_data, tool_use_id, context):\n    tool_name = input_data.get(\"tool_name\", \"\")\n\n    # Strip SDK prefix: mcp__reachy__move_head \u2192 move_head\n    if tool_name.startswith(\"mcp__\"):\n        parts = tool_name.split(\"__\")\n        original_tool = parts[2]\n\n    # Evaluate permission tier\n    decision = self._permission_evaluator.evaluate(original_tool)\n\n    if decision.tier == PermissionTier.FORBIDDEN:\n        return {\"hookSpecificOutput\": {\"permissionDecision\": \"deny\", ...}}\n\n    if decision.needs_confirmation:\n        return {\"hookSpecificOutput\": {\"permissionDecision\": \"ask\", ...}}\n\n    return {}  # Allow\n</code></pre>"},{"location":"architecture/agent-loop/#5-main-processing-flow","title":"5. Main Processing Flow","text":"<p>The <code>process_input()</code> method is the main entry point:</p> <pre><code>async def process_input(self, user_input: str, context=None):\n    # 1. Check state\n    if self.state != AgentState.READY:\n        return AgentResponse(error=\"Agent not ready\")\n\n    # 2. Pause idle behavior (robot stops looking around)\n    if self._idle_controller:\n        await self._idle_controller.pause()\n\n    # 3. Process with SDK\n    augmented_input = self._build_augmented_input(user_input, context)\n    response = await self._process_with_sdk(augmented_input, context)\n\n    # 4. Resume idle behavior\n    if self._idle_controller:\n        await self._idle_controller.resume()\n\n    return response\n</code></pre>"},{"location":"architecture/agent-loop/#6-sdk-interaction","title":"6. SDK Interaction","text":"<p>The SDK acts as a streaming message processor. When you call <code>client.query()</code>, Claude processes your input and may decide to call tools. The SDK automatically executes those tool calls via MCP, and streams results back.</p> Message Type Content Purpose <code>AssistantMessage</code> <code>TextBlock</code> Text response from Claude <code>AssistantMessage</code> <code>ToolUseBlock</code> Claude is calling a tool <code>AssistantMessage</code> <code>ToolResultBlock</code> Tool execution result <code>ResultMessage</code> Stats Final cost and duration <pre><code>async def _process_with_sdk(self, augmented_input, context):\n    async with ClaudeSDKClient(options=self._build_sdk_options()) as client:\n        await client.query(augmented_input)\n\n        async for message in client.receive_response():\n            if isinstance(message, AssistantMessage):\n                for block in message.content:\n                    if isinstance(block, TextBlock):\n                        response_text += block.text\n                    elif isinstance(block, ToolUseBlock):\n                        tool_calls_made.append({...})\n\n            elif isinstance(message, ResultMessage):\n                cost_usd = message.total_cost_usd\n</code></pre>"},{"location":"architecture/agent-loop/#7-memory-integration","title":"7. Memory Integration","text":"<p>Memory provides personalization by storing user preferences and session history:</p> <pre><code>async def _initialize_memory(self):\n    self._memory_manager = MemoryManager(\n        chroma_path=chroma_path,           # Vector store for semantic search\n        sqlite_path=sqlite_path,            # Structured data (profiles)\n        embedding_model=\"all-MiniLM-L6-v2\", # Local embeddings\n    )\n    await self._memory_manager.initialize()\n\n    # Load context for injection into system prompt\n    self._user_profile = await self._memory_manager.get_profile()\n    self._last_session = await self._memory_manager.get_last_session()\n\n    # Start tracking this session\n    await self._memory_manager.start_session()\n</code></pre> <p>The profile and last session get injected into the system prompt so Claude knows who it's talking to and what happened previously.</p>"},{"location":"architecture/agent-loop/#complete-architecture-flow","title":"Complete Architecture Flow","text":"<pre><code>%%{init: {'theme': 'default'}}%%\nflowchart TB\n    subgraph Input[\"User Input\"]\n        USER[User Message]\n    end\n\n    subgraph Agent[\"ReachyAgentLoop\"]\n        PROCESS[process_input]\n        AUGMENT[_build_augmented_input]\n        SDK_PROCESS[_process_with_sdk]\n    end\n\n    subgraph SDK[\"ClaudeSDKClient\"]\n        QUERY[query]\n        STREAM[receive_response]\n\n        subgraph Hooks[\"Permission Hooks\"]\n            PRE[PreToolUse]\n            EVAL[PermissionEvaluator]\n        end\n    end\n\n    subgraph Claude[\"Claude API\"]\n        API[HTTPS Request]\n        RESPONSE[Streaming Response]\n    end\n\n    subgraph MCP[\"MCP Servers (stdio)\"]\n        REACHY_MCP[Reachy Server&lt;br/&gt;23 tools]\n        MEMORY_MCP[Memory Server&lt;br/&gt;4 tools]\n    end\n\n    subgraph Hardware[\"Reachy Hardware\"]\n        DAEMON[Reachy Daemon&lt;br/&gt;HTTP :8000]\n        ROBOT[Head \u2022 Body \u2022 Antennas]\n    end\n\n    USER --&gt; PROCESS\n    PROCESS --&gt; AUGMENT\n    AUGMENT --&gt; SDK_PROCESS\n    SDK_PROCESS --&gt; QUERY\n    QUERY --&gt; API\n    API --&gt; RESPONSE\n    RESPONSE --&gt; STREAM\n\n    STREAM --&gt;|Tool Request| PRE\n    PRE --&gt; EVAL\n    EVAL --&gt;|Allow| REACHY_MCP\n    EVAL --&gt;|Allow| MEMORY_MCP\n\n    REACHY_MCP --&gt;|HTTP| DAEMON\n    DAEMON --&gt; ROBOT\n\n    style SDK fill:#7c4dff,stroke:#311b92,color:#fff\n    style Claude fill:#f9a825,stroke:#e65100\n    style Hardware fill:#4caf50,stroke:#1b5e20</code></pre>"},{"location":"architecture/agent-loop/#usage-examples","title":"Usage Examples","text":""},{"location":"architecture/agent-loop/#basic-usage","title":"Basic Usage","text":"<pre><code>from reachy_agent.agent import ReachyAgentLoop\n\nasync def main():\n    agent = ReachyAgentLoop(\n        daemon_url=\"http://localhost:8765\",  # Simulation port\n        enable_idle_behavior=True,\n        enable_memory=True,\n    )\n\n    # Use context manager for automatic cleanup\n    async with agent.session():\n        response = await agent.process_input(\"Wave hello!\")\n\n        print(f\"Response: {response.text}\")\n        print(f\"Tools called: {response.tool_calls}\")\n        print(f\"Cost: ${response.cost_usd:.4f}\")\n</code></pre>"},{"location":"architecture/agent-loop/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code>from reachy_agent.agent import ReachyAgentLoop\nfrom reachy_agent.behaviors.idle import IdleBehaviorConfig\nfrom reachy_agent.utils.config import load_config\n\nasync def main():\n    config = load_config(\"config/default.yaml\")\n\n    idle_config = IdleBehaviorConfig(\n        look_around_interval=5.0,\n        movement_range=0.3,\n    )\n\n    agent = ReachyAgentLoop(\n        config=config,\n        daemon_url=\"http://localhost:8000\",\n        enable_idle_behavior=True,\n        idle_config=idle_config,\n        enable_memory=True,\n    )\n\n    await agent.initialize()\n\n    try:\n        # Interactive loop\n        while True:\n            user_input = input(\"You: \")\n            if user_input.lower() in [\"quit\", \"exit\"]:\n                break\n\n            response = await agent.process_input(user_input)\n            print(f\"Reachy: {response.text}\")\n    finally:\n        await agent.shutdown()\n</code></pre>"},{"location":"architecture/agent-loop/#programmatic-control","title":"Programmatic Control","text":"<pre><code>async def demo_sequence():\n    async with agent.session() as loop:\n        # Wake up the robot\n        await loop.process_input(\"Wake up and say hello!\")\n\n        # Pause idle behavior for precise control\n        if loop.idle_controller:\n            await loop.idle_controller.pause()\n\n        # Execute a sequence\n        await loop.process_input(\"Look left, then right, then nod\")\n\n        # Resume idle behavior\n        if loop.idle_controller:\n            await loop.idle_controller.resume()\n</code></pre>"},{"location":"architecture/agent-loop/#key-takeaways","title":"Key Takeaways","text":"Concept Description Perceive \u2192 Think \u2192 Act Classic robotics AI pattern with Claude handling \"Think\" MCP for tool access Robot control exposed as MCP tools, SDK handles transport Permission hooks 4-tier system enforced at SDK hook level Memory for context User profile and session history injected into prompts Idle behavior Robot looks around autonomously when not processing State machine Prevents concurrent processing, tracks lifecycle"},{"location":"architecture/agent-loop/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Overview - System-level architecture</li> <li>MCP Tools Reference - All 27 MCP tools</li> <li>Permission System - 4-tier permission details</li> <li>Memory API - ChromaDB + SQLite integration</li> </ul>"},{"location":"architecture/agent-loop/#source-code","title":"Source Code","text":"<p>The implementation is in <code>src/reachy_agent/agent/agent.py</code>.</p>"},{"location":"architecture/daemon-compatibility/","title":"Daemon Compatibility &amp; Coexistence","text":"<p>This document explains how Claude in the Shell interacts with the Reachy Mini daemon and coexists with other Reachy applications.</p>"},{"location":"architecture/daemon-compatibility/#daemon-architecture","title":"Daemon Architecture","text":"<p>The Reachy Mini daemon (provided by Pollen Robotics) exposes two communication protocols:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Reachy Mini Daemon                            \u2502\n\u2502                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   FastAPI REST     \u2502     \u2502      Zenoh Pub/Sub             \u2502  \u2502\n\u2502  \u2502   (HTTP :8000)     \u2502     \u2502   (Real-time messaging)        \u2502  \u2502\n\u2502  \u2502                    \u2502     \u2502                                \u2502  \u2502\n\u2502  \u2502  \u2022 Request/Response\u2502     \u2502  \u2022 100Hz control loop          \u2502  \u2502\n\u2502  \u2502  \u2022 Tool commands   \u2502     \u2502  \u2022 Streaming sensor data       \u2502  \u2502\n\u2502  \u2502  \u2022 Status queries  \u2502     \u2502  \u2022 Low-latency motion          \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502            \u2502                           \u2502                         \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                           \u2502                                      \u2502\n\u2502                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \u2502\n\u2502                  \u2502  Hardware HAL  \u2502                              \u2502\n\u2502                  \u2502  (Motors, IMU, \u2502                              \u2502\n\u2502                  \u2502   Camera, Mic) \u2502                              \u2502\n\u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/daemon-compatibility/#protocol-comparison","title":"Protocol Comparison","text":"Aspect HTTP REST Zenoh Port <code>:8000</code> Default Zenoh ports Latency ~10-50ms ~1-5ms Pattern Request/Response Publish/Subscribe Use Case Command-driven control Real-time streaming Connection Per-request Persistent session"},{"location":"architecture/daemon-compatibility/#how-each-app-uses-the-daemon","title":"How Each App Uses the Daemon","text":""},{"location":"architecture/daemon-compatibility/#claude-in-the-shell-http-rest","title":"Claude in the Shell (HTTP REST)","text":"<pre><code># Our ReachyDaemonClient uses HTTP\nclass ReachyDaemonClient:\n    def __init__(self, base_url=\"http://localhost:8000\"):\n        self._client = httpx.AsyncClient(base_url=base_url)\n\n    async def move_head(self, direction: str):\n        # Uses set_target for smooth movements on real hardware\n        return await self._request(\"POST\", \"/api/move/set_target\", ...)\n</code></pre> <p>Why HTTP works well for us: - MCP tools are naturally request/response - Claude thinks, then acts (not continuous control) - Simpler async integration with Claude SDK - No dependency on Zenoh library</p>"},{"location":"architecture/daemon-compatibility/#conversation-app-zenoh","title":"Conversation App (Zenoh)","text":"<pre><code># Conversation App uses ReachyMini class with Zenoh\nfrom reachy_mini import ReachyMini\n\nrobot = ReachyMini()  # Connects via Zenoh\nrobot.set_target(head=head_pose)  # 100Hz updates\n</code></pre> <p>Why Zenoh works well for them: - Real-time audio streaming requires low latency - Continuous head tracking during conversation - Primary/secondary motion blending at 100Hz</p>"},{"location":"architecture/daemon-compatibility/#dashboard-zenoh","title":"Dashboard (Zenoh)","text":"<p>The Pollen dashboard also uses Zenoh for: - Real-time status monitoring - App lifecycle management - Live camera preview</p>"},{"location":"architecture/daemon-compatibility/#coexistence-scenarios","title":"Coexistence Scenarios","text":""},{"location":"architecture/daemon-compatibility/#scenario-1-claude-in-the-shell-dashboard","title":"Scenario 1: Claude in the Shell + Dashboard \u2705","text":"<p>Both can run simultaneously:</p> <pre><code>Dashboard (Zenoh) \u2500\u2500\u2500\u2500\u2500\u2510\n                       \u251c\u2500\u2500\u25ba Daemon \u2500\u2500\u25ba Robot\nClaude (HTTP REST) \u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The daemon handles concurrent access gracefully. Motion commands from either source are queued.</p>"},{"location":"architecture/daemon-compatibility/#scenario-2-claude-in-the-shell-conversation-app","title":"Scenario 2: Claude in the Shell + Conversation App \u26a0\ufe0f","text":"<p>May conflict if both try to control the robot:</p> <pre><code>Conversation App (Zenoh) \u2500\u2500\u2500\u2510\n                            \u251c\u2500\u2500\u25ba Daemon \u2500\u2500\u25ba Robot (conflicts!)\nClaude (HTTP REST) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Recommendation: Run one at a time, or designate one as \"primary controller\".</p>"},{"location":"architecture/daemon-compatibility/#scenario-3-multiple-claude-instances","title":"Scenario 3: Multiple Claude Instances \u274c","text":"<p>Not recommended - permission states may conflict:</p> <pre><code>Claude Instance 1 \u2500\u2500\u2500\u2510\n                     \u251c\u2500\u2500\u25ba Daemon (race conditions)\nClaude Instance 2 \u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/daemon-compatibility/#daemon-api-endpoints-we-use","title":"Daemon API Endpoints We Use","text":""},{"location":"architecture/daemon-compatibility/#movement","title":"Movement","text":"<ul> <li><code>POST /api/move/set_target</code> - Smooth incremental movement (real hardware)</li> <li><code>POST /api/move/goto</code> - Position movement (simulation only - see note below)</li> <li><code>POST /api/move/play/wake_up</code> - Wake up sequence</li> <li><code>POST /api/move/play/goto_sleep</code> - Sleep sequence</li> <li><code>POST /api/move/play/recorded-move-dataset/{dataset}/{move}</code> - HuggingFace emotions</li> </ul> <p>Important: Real hardware requires <code>/api/move/set_target</code> for smooth movements. The <code>goto</code> API includes <code>x</code>, <code>y</code>, <code>z</code> position fields that default to 0, causing the head to snap to origin. Our <code>ReachyDaemonClient</code> auto-detects the backend and uses the appropriate API.</p>"},{"location":"architecture/daemon-compatibility/#status","title":"Status","text":"<ul> <li><code>GET /api/daemon/status</code> - Daemon health and config</li> <li><code>GET /api/state/full</code> - Robot pose and sensor state</li> </ul>"},{"location":"architecture/daemon-compatibility/#kinematics","title":"Kinematics","text":"<ul> <li><code>POST /api/kinematics/look_at_world</code> - Look at 3D point</li> <li><code>POST /api/kinematics/look_at_pixel</code> - Look at camera pixel</li> </ul>"},{"location":"architecture/daemon-compatibility/#motors","title":"Motors","text":"<ul> <li><code>POST /api/motors/set_mode/{mode}</code> - Enable/disable/gravity compensation</li> </ul>"},{"location":"architecture/daemon-compatibility/#movementmanager-pattern-from-conversation-app","title":"MovementManager Pattern (From Conversation App)","text":"<p>The Conversation App uses a sophisticated <code>MovementManager</code> that we could adopt for smoother motion:</p> <pre><code>class MovementManager:\n    \"\"\"Orchestrates 100Hz motion with primary/secondary blending.\"\"\"\n\n    def __init__(self):\n        self.primary_queue = []      # Sequential moves (emotions, dances)\n        self.secondary_offset = {}   # Additive moves (speech sway, tracking)\n        self.breathing = BreathingMove()  # Idle behavior\n\n    def working_loop(self):\n        \"\"\"100Hz control loop.\"\"\"\n        while not self.stop_event.is_set():\n            primary = self._get_current_primary()\n            secondary = self._get_secondary_offsets()\n            final_pose = self._compose_poses(primary, secondary)\n            self.robot.set_target(head=final_pose)\n            time.sleep(0.01)  # 100Hz\n</code></pre> <p>Key concepts: - Primary moves: Mutually exclusive (emotions, dances) - Secondary moves: Additive overlays (speech sway, head tracking) - Breathing: Subtle idle animation when no other moves active</p>"},{"location":"architecture/daemon-compatibility/#potential-enhancement-for-claude-in-the-shell","title":"Potential Enhancement for Claude in the Shell","text":"<p>We could add a similar pattern:</p> <pre><code># Future: Add to idle behavior controller\nclass IdleMotionBlender:\n    \"\"\"Blend MCP tool commands with continuous idle behavior.\"\"\"\n\n    async def execute_with_blend(self, tool_command):\n        \"\"\"Execute tool while maintaining secondary idle motion.\"\"\"\n        self.pause_idle()\n        result = await self.daemon_client.execute(tool_command)\n        self.resume_idle()\n        return result\n</code></pre>"},{"location":"architecture/daemon-compatibility/#native-sdk-emotions","title":"Native SDK Emotions","text":"<p>Both apps can use the official emotion library from HuggingFace:</p> <pre><code># Dataset: pollen-robotics/reachy-mini-emotions-library\n# Available moves: curious1, cheerful1, amazed1, exhausted1, etc.\n\n# Via HTTP (our approach)\nawait client.play_recorded_move(\n    \"pollen-robotics/reachy-mini-emotions-library\",\n    \"curious1\"\n)\n\n# Via Zenoh (Conversation App approach)\nrobot.play_recorded_move(\n    \"pollen-robotics/reachy-mini-emotions-library\",\n    \"curious1\"\n)\n</code></pre> <p>Both methods trigger the same pre-recorded motion capture animations.</p>"},{"location":"architecture/daemon-compatibility/#future-considerations","title":"Future Considerations","text":""},{"location":"architecture/daemon-compatibility/#1-hybrid-approach","title":"1. Hybrid Approach","text":"<p>We could add Zenoh for specific features:</p> <pre><code># Hypothetical: Use Zenoh for real-time head tracking\n# while keeping HTTP for tool commands\nclass HybridController:\n    def __init__(self):\n        self.http_client = ReachyDaemonClient()  # Tools\n        self.zenoh_client = ZenohClient()        # Real-time\n</code></pre>"},{"location":"architecture/daemon-compatibility/#2-dashboard-integration","title":"2. Dashboard Integration","text":"<p>We could register as a ReachyMiniApp for dashboard visibility:</p> <pre><code>from reachy_mini import ReachyMiniApp\n\nclass ClaudeInTheShellApp(ReachyMiniApp):\n    custom_app_url = \"http://localhost:8080\"\n\n    def run(self, reachy_mini, stop_event):\n        # Launch our agent in a thread\n        self.agent = ReachyAgentLoop()\n        asyncio.run(self.agent.run())\n</code></pre>"},{"location":"architecture/daemon-compatibility/#3-app-store-distribution","title":"3. App Store Distribution","text":"<p>To publish to the Reachy App Store:</p> <ol> <li>Restructure as a <code>ReachyMiniApp</code> subclass</li> <li>Publish to HuggingFace Spaces</li> <li>Submit for official listing</li> </ol> <p>See Reachy Mini App Development Guide.</p>"},{"location":"architecture/daemon-compatibility/#summary","title":"Summary","text":"Aspect Current Approach Alternative Protocol HTTP REST Could add Zenoh Daemon port <code>:8000</code> Same Coexistence \u2705 With Dashboard \u26a0\ufe0f With Conversation App Motion quality Good (tool-based) Better with blending Distribution Standalone install Could add App Store <p>The HTTP approach is well-suited for Claude's tool-based interaction model. Adding Zenoh support is an option for Phase 2 if we need real-time features like head tracking during speech.</p>"},{"location":"architecture/overview/","title":"Reachy Agent Architecture","text":"<p>This document describes the architecture of Reachy Agent, an embodied AI system that transforms the Reachy Mini desktop robot into an autonomous Claude-powered assistant.</p>"},{"location":"architecture/overview/#system-overview","title":"System Overview","text":"<pre><code>flowchart TB\n    subgraph Cloud[\"Cloud Services\"]\n        CLAUDE[(\"Claude API&lt;br/&gt;api.anthropic.com\")]\n        HA[\"Home Assistant\"]\n        GH[\"GitHub API\"]\n        CAL[\"Google Calendar\"]\n    end\n\n    subgraph Agent[\"Reachy Agent (Raspberry Pi 4)\"]\n        SDK[\"Claude Agent SDK\"]\n\n        subgraph Core[\"Agent Core\"]\n            LOOP[\"Agent Loop&lt;br/&gt;(Perceive\u2192Think\u2192Act)\"]\n            PERM[\"Permission System&lt;br/&gt;(4 Tiers)\"]\n            MEM[\"Memory System&lt;br/&gt;(ChromaDB + SQLite)\"]\n        end\n\n        subgraph MCP[\"MCP Servers\"]\n            REACHY_MCP[\"Reachy MCP&lt;br/&gt;(23 tools)\"]\n            HA_MCP[\"Home Assistant MCP\"]\n            GH_MCP[\"GitHub MCP\"]\n        end\n\n        subgraph Perception[\"Perception Layer\"]\n            WAKE[\"Wake Word&lt;br/&gt;(OpenWakeWord)\"]\n            AUDIO[\"Spatial Audio&lt;br/&gt;(4-mic array)\"]\n            VISION[\"Vision&lt;br/&gt;(OpenCV)\"]\n        end\n\n        subgraph Voice[\"Voice Pipeline\"]\n            VPIPE[\"VoicePipeline&lt;br/&gt;(State Machine)\"]\n            REALTIME[\"OpenAI Realtime&lt;br/&gt;(STT/TTS)\"]\n            PERSONA[\"PersonaManager&lt;br/&gt;(Motoko/Batou)\"]\n        end\n\n        subgraph Motion[\"Motion System\"]\n            BLEND[\"MotionBlendController&lt;br/&gt;(100Hz orchestration)\"]\n            SDK_CLIENT[\"ReachySDKClient&lt;br/&gt;(1-5ms Zenoh)\"]\n        end\n    end\n\n    subgraph Hardware[\"Reachy Mini Hardware\"]\n        DAEMON[\"Reachy Daemon&lt;br/&gt;(FastAPI :8000)\"]\n\n        subgraph Physical[\"Physical Components\"]\n            HEAD[\"Head&lt;br/&gt;(6 DOF)\"]\n            BODY[\"Body&lt;br/&gt;(360\u00b0 rotation)\"]\n            ANT[\"Antennas&lt;br/&gt;(expression)\"]\n            CAM[\"Camera\"]\n            MIC[\"4-Mic Array\"]\n            SPK[\"Speaker\"]\n        end\n    end\n\n    CLAUDE &lt;--&gt; SDK\n    SDK --&gt; LOOP\n    LOOP &lt;--&gt; PERM\n    LOOP &lt;--&gt; MEM\n    LOOP --&gt; MCP\n\n    REACHY_MCP --&gt; DAEMON\n    HA_MCP --&gt; HA\n    GH_MCP --&gt; GH\n\n    DAEMON --&gt; Physical\n    Perception --&gt; LOOP\n\n    CAM --&gt; VISION\n    MIC --&gt; AUDIO\n    MIC --&gt; WAKE\n\n    style CLAUDE fill:#f9a825\n    style SDK fill:#7c4dff\n    style REACHY_MCP fill:#00bcd4\n    style DAEMON fill:#4caf50</code></pre>"},{"location":"architecture/overview/#component-layers","title":"Component Layers","text":""},{"location":"architecture/overview/#layer-1-cloud-intelligence","title":"Layer 1: Cloud Intelligence","text":"<p>The system uses Claude API for natural language understanding and reasoning:</p> <pre><code>sequenceDiagram\n    participant U as User\n    participant W as Wake Word\n    participant A as Agent Loop\n    participant C as Claude API\n    participant T as MCP Tools\n\n    U-&gt;&gt;W: \"Hey Reachy\"\n    W-&gt;&gt;A: Wake signal\n    A-&gt;&gt;A: Record audio\n    U-&gt;&gt;A: \"Turn left and nod\"\n    A-&gt;&gt;C: User request + context\n    C-&gt;&gt;A: Tool calls: [move_head, nod]\n    A-&gt;&gt;T: Execute move_head(\"left\")\n    T--&gt;&gt;A: Success\n    A-&gt;&gt;T: Execute nod()\n    T--&gt;&gt;A: Success\n    A-&gt;&gt;C: Tool results\n    C-&gt;&gt;A: \"I've turned left and nodded!\"\n    A-&gt;&gt;U: Speak response</code></pre>"},{"location":"architecture/overview/#layer-2-agent-core","title":"Layer 2: Agent Core","text":"<p>The agent core manages the perception-action loop:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Passive\n\n    Passive --&gt; Alert: Motion detected\n    Passive --&gt; Engaged: Wake word heard\n\n    Alert --&gt; Passive: 30s timeout\n    Alert --&gt; Engaged: Wake word heard\n    Alert --&gt; Engaged: Face detected\n\n    Engaged --&gt; Alert: 60s silence\n    Engaged --&gt; Engaged: User speaking\n\n    note right of Passive\n        - Wake word detection only\n        - Minimal CPU usage\n        - Antennas down\n    end note\n\n    note right of Alert\n        - Motion/face tracking\n        - Periodic Claude check-ins\n        - Antennas mid-position\n    end note\n\n    note right of Engaged\n        - Full agent loop active\n        - Continuous listening\n        - Antennas up (privacy indicator)\n    end note</code></pre>"},{"location":"architecture/overview/#layer-3-mcp-protocol-true-mcp-architecture","title":"Layer 3: MCP Protocol (True MCP Architecture)","text":"<p>MCP (Model Context Protocol) provides a standardized interface between Claude and tools. Key change: The agent now uses true MCP protocol with subprocess communication:</p> <pre><code>flowchart LR\n    subgraph Agent[\"Agent Core (agent.py)\"]\n        SDK[\"ReachyAgentLoop\"]\n        CLIENT[\"MCP ClientSession&lt;br/&gt;(stdio transport)\"]\n    end\n\n    subgraph MCP[\"MCP Layer (subprocess)\"]\n        direction TB\n        PROTO[\"MCP Protocol&lt;br/&gt;(JSON-RPC over stdio)\"]\n\n        subgraph Servers[\"MCP Servers\"]\n            R[\"reachy-mcp&lt;br/&gt;(23 tools)\"]\n            H[\"homeassistant-mcp\"]\n            G[\"github-mcp\"]\n        end\n    end\n\n    subgraph Backends[\"Backend Services\"]\n        RD[\"Reachy Daemon&lt;br/&gt;:8000\"]\n        HA[\"Home Assistant\"]\n        GH[\"GitHub API\"]\n    end\n\n    SDK --&gt; CLIENT\n    CLIENT &lt;--&gt;|\"ListTools&lt;br/&gt;CallTool\"| PROTO\n    PROTO &lt;--&gt; Servers\n    R --&gt;|HTTP| RD\n    H --&gt;|REST API| HA\n    G --&gt;|REST API| GH\n\n    style PROTO fill:#e1bee7\n    style CLIENT fill:#c8e6c9</code></pre> <p>MCP Protocol Flow: 1. Agent spawns MCP server as subprocess (<code>python -m reachy_agent.mcp_servers.reachy</code>) 2. Agent discovers tools dynamically via <code>ListTools</code> (23 tools) 3. Agent executes tools via <code>CallTool</code> over stdio transport 4. MCP server calls Daemon HTTP API</p>"},{"location":"architecture/overview/#layer-4-permission-system","title":"Layer 4: Permission System","text":"<p>Actions are classified into 4 permission tiers:</p> <pre><code>flowchart TD\n    REQ[\"Tool Request\"] --&gt; EVAL[\"Permission Evaluator\"]\n\n    EVAL --&gt; T1{\"Tier 1?&lt;br/&gt;Autonomous\"}\n    EVAL --&gt; T2{\"Tier 2?&lt;br/&gt;Notify\"}\n    EVAL --&gt; T3{\"Tier 3?&lt;br/&gt;Confirm\"}\n    EVAL --&gt; T4{\"Tier 4?&lt;br/&gt;Forbidden\"}\n\n    T1 --&gt;|Yes| EXEC[\"Execute Immediately\"]\n    T2 --&gt;|Yes| EXECN[\"Execute + Notify User\"]\n    T3 --&gt;|Yes| ASK[\"Ask for Confirmation\"]\n    T4 --&gt;|Yes| BLOCK[\"Block + Log\"]\n\n    ASK --&gt;|Approved| EXEC\n    ASK --&gt;|Denied| BLOCK\n\n    EXEC --&gt; LOG[\"Audit Log\"]\n    EXECN --&gt; LOG\n    BLOCK --&gt; LOG\n\n    style T1 fill:#c8e6c9\n    style T2 fill:#fff9c4\n    style T3 fill:#ffe0b2\n    style T4 fill:#ffcdd2</code></pre> <p>Permission Tier Examples:</p> Tier Category Tools Rationale 1 Autonomous <code>move_head</code>, <code>play_emotion</code>, <code>capture_image</code> Body control is safe, reversible 2 Notify <code>homeassistant.*</code>, <code>send_notification</code> User should know about actions 3 Confirm <code>github.create_pr</code>, <code>calendar.create_event</code> Irreversible external changes 4 Forbidden <code>execute_code</code>, <code>delete_*</code>, <code>admin_*</code> Security-critical operations"},{"location":"architecture/overview/#layer-5-hardware-interface","title":"Layer 5: Hardware Interface","text":"<p>The Reachy Daemon provides a REST API to control hardware:</p> <pre><code>flowchart TB\n    subgraph Agent[\"Reachy Agent\"]\n        CLIENT[\"ReachyMiniClient&lt;br/&gt;(HTTP Client)\"]\n    end\n\n    subgraph Daemon[\"Reachy Daemon (FastAPI)\"]\n        API[\"REST API&lt;br/&gt;:8000\"]\n\n        subgraph Backends[\"Backend Selector\"]\n            HW[\"RobotBackend&lt;br/&gt;(Real Hardware)\"]\n            SIM[\"MujocoBackend&lt;br/&gt;(Simulation)\"]\n        end\n\n        subgraph Controllers[\"Motor Controllers\"]\n            HEAD_C[\"Head Controller\"]\n            BODY_C[\"Body Controller\"]\n            ANT_C[\"Antenna Controller\"]\n        end\n    end\n\n    subgraph Hardware[\"Physical/Simulated\"]\n        MOTORS[\"Dynamixel Servos\"]\n        MUJOCO[\"MuJoCo Physics\"]\n    end\n\n    CLIENT --&gt;|\"POST /api/move/set_target\"| API\n    CLIENT --&gt;|\"POST /api/move/play/wake_up\"| API\n    CLIENT --&gt;|\"GET /api/daemon/status\"| API\n\n    API --&gt; Backends\n    HW --&gt; Controllers\n    SIM --&gt; Controllers\n\n    HEAD_C --&gt; MOTORS\n    HEAD_C --&gt; MUJOCO\n    BODY_C --&gt; MOTORS\n    BODY_C --&gt; MUJOCO\n    ANT_C --&gt; MOTORS\n    ANT_C --&gt; MUJOCO\n\n    style SIM fill:#e3f2fd\n    style HW fill:#e8f5e9</code></pre>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#request-lifecycle","title":"Request Lifecycle","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant P as Perception\n    participant L as Agent Loop\n    participant M as Memory\n    participant E as Permission Eval\n    participant T as MCP Tool\n    participant D as Reachy Daemon\n\n    U-&gt;&gt;P: Voice input\n    P-&gt;&gt;L: Transcribed text\n    L-&gt;&gt;M: Fetch context\n    M--&gt;&gt;L: Relevant memories\n    L-&gt;&gt;L: Build prompt\n    L-&gt;&gt;L: Call Claude API\n\n    loop For each tool call\n        L-&gt;&gt;E: Check permission\n        E--&gt;&gt;L: Decision (tier)\n\n        alt Autonomous/Notify\n            L-&gt;&gt;T: Execute tool\n            T-&gt;&gt;D: HTTP request\n            D--&gt;&gt;T: Result\n            T--&gt;&gt;L: Tool response\n        else Confirm\n            L-&gt;&gt;U: Request confirmation\n            U--&gt;&gt;L: Approve/Deny\n        else Forbidden\n            L-&gt;&gt;L: Log blocked attempt\n        end\n    end\n\n    L-&gt;&gt;M: Store interaction\n    L-&gt;&gt;U: Speak response</code></pre>"},{"location":"architecture/overview/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"architecture/overview/#development-simulation","title":"Development (Simulation)","text":"<pre><code>flowchart LR\n    subgraph Dev[\"Development Machine\"]\n        AGENT[\"Reachy Agent\"]\n        DAEMON[\"reachy-mini-daemon&lt;br/&gt;--sim --headless\"]\n        MUJOCO[\"MuJoCo Physics\"]\n    end\n\n    subgraph Cloud\n        CLAUDE[\"Claude API\"]\n    end\n\n    AGENT &lt;--&gt;|MCP| DAEMON\n    DAEMON --&gt; MUJOCO\n    AGENT &lt;--&gt;|HTTPS| CLAUDE\n\n    style MUJOCO fill:#e3f2fd</code></pre>"},{"location":"architecture/overview/#production-hardware","title":"Production (Hardware)","text":"<pre><code>flowchart LR\n    subgraph Pi[\"Raspberry Pi 4\"]\n        AGENT[\"reachy-agent.service\"]\n        DAEMON[\"reachy-daemon.service\"]\n    end\n\n    subgraph Robot[\"Reachy Mini\"]\n        HW[\"Hardware\"]\n    end\n\n    subgraph Cloud\n        CLAUDE[\"Claude API\"]\n        HA[\"Home Assistant\"]\n    end\n\n    AGENT &lt;--&gt;|MCP| DAEMON\n    DAEMON --&gt; HW\n    AGENT &lt;--&gt;|HTTPS| CLAUDE\n    AGENT &lt;--&gt;|REST| HA\n\n    style HW fill:#e8f5e9</code></pre>"},{"location":"architecture/overview/#key-design-decisions","title":"Key Design Decisions","text":"Decision Choice Alternatives Considered Rationale Process Model Single asyncio Multiple systemd services Simpler debugging, lower memory Wake Word OpenWakeWord Porcupine, Snowboy Open source, no license costs Memory Store ChromaDB + SQLite PostgreSQL, Redis Lightweight, embedded, no server Config Format YAML JSON, TOML Human-readable, supports comments MCP Transport stdio subprocess HTTP, direct calls True MCP protocol, dynamic discovery Tool Discovery ListTools Hardcoded schemas Single source of truth, extensible"},{"location":"architecture/overview/#security-model","title":"Security Model","text":"<pre><code>flowchart TB\n    subgraph Trust[\"Trust Boundaries\"]\n        subgraph High[\"High Trust\"]\n            AGENT[\"Agent Core\"]\n            DAEMON[\"Reachy Daemon\"]\n        end\n\n        subgraph Medium[\"Medium Trust\"]\n            MCP[\"MCP Servers\"]\n            CLOUD[\"Claude API\"]\n        end\n\n        subgraph Low[\"Low Trust\"]\n            EXT[\"External Services\"]\n            USER[\"User Input\"]\n        end\n    end\n\n    USER --&gt;|\"Sanitized\"| AGENT\n    AGENT --&gt;|\"Validated\"| MCP\n    MCP --&gt;|\"HTTPS\"| EXT\n    AGENT &lt;--&gt;|\"TLS\"| CLOUD\n\n    style High fill:#c8e6c9\n    style Medium fill:#fff9c4\n    style Low fill:#ffcdd2</code></pre> <p>Security Principles: 1. Least Privilege: Each MCP server has minimal required permissions 2. Defense in Depth: Permission tiers + API validation + audit logging 3. Fail Secure: Unknown tools default to highest restriction tier 4. Privacy by Design: Antenna states indicate when agent is listening</p>"},{"location":"architecture/overview/#mcp-tools-overview","title":"MCP Tools Overview","text":"<p>The Reachy MCP server provides 23 tools organized by category:</p> Category Tools Mock Support Movement (5) <code>move_head</code>, <code>look_at</code>, <code>look_at_world</code>, <code>look_at_pixel</code>, <code>rotate</code> 3/5 Expression (6) <code>play_emotion</code>, <code>play_recorded_move</code>, <code>set_antenna_state</code>, <code>nod</code>, <code>shake</code>, <code>dance</code> 5/6 Audio (2) <code>speak</code>, <code>listen</code> 2/2 Perception (3) <code>capture_image</code>, <code>get_sensor_data</code>, <code>look_at_sound</code> 3/3 Lifecycle (3) <code>wake_up</code>, <code>sleep</code>, <code>rest</code> 3/3 Status (2) <code>get_status</code>, <code>get_pose</code> 2/2 Control (2) <code>set_motor_mode</code>, <code>cancel_action</code> 1/2"},{"location":"architecture/overview/#native-sdk-emotions","title":"Native SDK Emotions","text":"<p>The agent uses native SDK emotions from <code>pollen-robotics/reachy-mini-emotions-library</code> (HuggingFace) when available:</p> <pre><code>flowchart LR\n    subgraph Agent\n        EMO[\"play_emotion('happy')\"]\n    end\n\n    subgraph Check[\"Emotion Lookup\"]\n        NATIVE{\"Native SDK&lt;br/&gt;Available?\"}\n    end\n\n    subgraph SDK[\"HuggingFace\"]\n        HF[\"cheerful1&lt;br/&gt;(with audio)\"]\n    end\n\n    subgraph Custom\n        CUSTOM[\"Custom composition&lt;br/&gt;(head + antennas)\"]\n    end\n\n    EMO --&gt; NATIVE\n    NATIVE --&gt;|Yes| HF\n    NATIVE --&gt;|No| CUSTOM\n\n    style HF fill:#c8e6c9\n    style CUSTOM fill:#fff9c4</code></pre>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<p>See Phase 2 Preparation Guide for hardware integration details.</p>"},{"location":"architecture/personas/","title":"Understanding the Multi-Persona Wake Word System","text":"<p>Purpose: This document explains the architecture and design rationale behind Reachy Agent's multi-persona wake word system, inspired by Ghost in the Shell.</p> <p>Audience: Developers implementing persona-based AI systems or extending the Reachy Agent personality framework.</p> <p>Prerequisite Knowledge: Familiarity with wake word detection, voice pipelines, and AI agent architectures.</p>"},{"location":"architecture/personas/#the-big-picture","title":"The Big Picture","text":"<p>The Multi-Persona Wake Word System allows a single Reachy Mini robot to embody different AI personalities, each triggered by its own wake word. When a user says \"Hey Motoko,\" the robot becomes Major Kusanagi\u2014analytical, philosophical, and direct. Say \"Hey Batou,\" and it shifts to a casual, action-oriented persona.</p> <p>This isn't just about changing system prompts. The design ensures that voice characteristics, personality traits, and conversational style all align to create a coherent, believable character.</p>"},{"location":"architecture/personas/#why-this-matters","title":"Why This Matters","text":"<p>Traditional voice assistants have a single, static personality. Multi-persona systems enable:</p> <ul> <li>Context-appropriate interactions: Choose the right personality for the task (analytical vs. action-oriented)</li> <li>Multi-user scenarios: Different family members can have \"their\" persona</li> <li>Experimentation: Test different AI personalities without reconfiguring the entire system</li> <li>Entertainment: Ghost in the Shell fans can interact with characters they know</li> </ul> <p>The key technical insight: Voice quality must match personality. A deep male voice (onyx) saying philosophical observations feels wrong. A female voice (nova) delivering gruff one-liners breaks immersion. The system enforces this consistency at the architectural level.</p>"},{"location":"architecture/personas/#historical-context","title":"Historical Context","text":""},{"location":"architecture/personas/#the-problem-space","title":"The Problem Space","text":"<p>Early voice assistant prototypes suffered from personality mismatch: - System prompts could change personality, but the voice remained constant - Users experienced cognitive dissonance when the voice didn't match the character - No clear mechanism to switch between personalities in real-time - Wake word detection was single-model, limiting interaction patterns</p>"},{"location":"architecture/personas/#evolution-of-solutions","title":"Evolution of Solutions","text":"<p>Single-Persona Systems (2023): One wake word, one personality, tightly coupled to configuration. Changing personality required restarting the entire agent.</p> <p>Prompt-Only Switching (Early 2024): Dynamic system prompt updates without voice changes. Felt uncanny\u2014like an actor forgetting to change their voice.</p> <p>Multi-Persona Architecture (Current): Integrated wake word detection, voice synthesis, and system prompt management. Persona changes are atomic and consistent across all interaction layers.</p>"},{"location":"architecture/personas/#current-state","title":"Current State","text":"<p>The system leverages OpenWakeWord's multi-model support (v0.4.0+) to load multiple custom wake word models simultaneously. When any model triggers, the pipeline:</p> <ol> <li>Identifies the associated persona</li> <li>Defers the switch until after current TTS completes</li> <li>Updates voice, system prompt, and reconnects to OpenAI</li> <li>Maintains conversation continuity</li> </ol>"},{"location":"architecture/personas/#core-concepts","title":"Core Concepts","text":""},{"location":"architecture/personas/#personaconfig","title":"PersonaConfig","text":"<p>What it is: A data structure that binds together all aspects of a persona\u2014wake word, voice, system prompt, and personality traits.</p> <p>Why it exists: Without a unified configuration object, voice settings could drift from personality prompts. PersonaConfig ensures atomic updates and validation.</p> <p>How it relates: PersonaConfig instances are registered with PersonaManager and referenced by wake word models in WakeWordDetector.</p> <pre><code>@dataclass\nclass PersonaConfig:\n    name: str                    # Internal identifier (e.g., \"motoko\")\n    wake_word_model: str         # OpenWakeWord model name (e.g., \"hey_motoko\")\n    voice: OpenAIVoice           # TTS voice (alloy, echo, fable, onyx, nova, shimmer)\n    display_name: str            # Human-readable name (e.g., \"Major Kusanagi\")\n    prompt_path: str             # Path to system prompt (e.g., \"prompts/personas/motoko.md\")\n    traits: dict[str, Any]       # Optional metadata for runtime reference\n</code></pre> <p>Mental Model: Think of PersonaConfig as a \"character sheet\" in a role-playing game. It defines everything needed to portray a character consistently.</p> <p>Validation: The <code>from_dict()</code> factory method validates: - Voice is in <code>VALID_VOICES</code> frozenset (prevents typos like \"nvoa\") - Required fields are present (name, wake_word_model, voice) - Prompt path uses forward slashes (cross-platform compatibility)</p>"},{"location":"architecture/personas/#personamanager","title":"PersonaManager","text":"<p>What it is: A registry and coordinator for all available personas. It tracks which persona is currently active and manages transitions.</p> <p>Why it exists: Centralized management prevents race conditions during persona switches and provides a single source of truth for persona state.</p> <p>How it relates: VoicePipeline holds a PersonaManager instance. WakeWordDetector triggers callbacks that query the manager.</p> <pre><code>@dataclass\nclass PersonaManager:\n    personas: dict[str, PersonaConfig]       # Keyed by wake_word_model\n    current_persona: PersonaConfig | None    # Currently active persona\n    default_persona_key: str                 # Fallback persona on startup\n</code></pre> <p>Key Methods:</p> <ul> <li><code>register_persona(persona)</code>: Add a new persona to the registry</li> <li><code>get_persona(wake_word_model)</code>: Lookup persona by wake word</li> <li><code>set_current(wake_word_model)</code>: Update active persona</li> <li><code>set_default(wake_word_model)</code>: Set fallback persona</li> <li><code>from_config(config_dict)</code>: Factory method from YAML config</li> </ul> <p>Mental Model: PersonaManager is like a \"radio dial\"\u2014it knows all available stations (personas), which one you're tuned to (current), and which one to default to when you turn it on (default).</p>"},{"location":"architecture/personas/#wake-word-integration","title":"Wake Word Integration","text":"<p>What it is: OpenWakeWord's multi-model loading capability, allowing simultaneous detection of multiple wake words.</p> <p>Why it exists: Users shouldn't need to reconfigure or restart to switch personas. Real-time detection enables natural conversational flow.</p> <p>How it works:</p> <pre><code># WakeWordDetector loads all persona models from data/wake_words/\ncustom_model_paths = [\n    \"data/wake_words/hey_motoko.onnx\",\n    \"data/wake_words/hey_batou.onnx\",\n]\nself._model = Model(wakeword_models=custom_model_paths)\nself._active_models = [\"hey_motoko\", \"hey_batou\"]\n</code></pre> <p>When audio is processed: <pre><code>detected_model = self._wake_word.process_audio(chunk)\nif detected_model:  # Returns model name (e.g., \"hey_motoko\")\n    self.on_wake(detected_model)  # Callback to pipeline\n</code></pre></p> <p>Callback signature: <code>on_wake(model_name: str) -&gt; None</code></p> <p>The callback receives the model name (not the persona name), which PersonaManager uses to lookup the associated PersonaConfig.</p> <p>Model format: - ONNX format (<code>.onnx</code> files) - Trained using OpenWakeWord training scripts - File names must match <code>wake_word_model</code> keys in config</p>"},{"location":"architecture/personas/#architectural-design","title":"Architectural Design","text":""},{"location":"architecture/personas/#system-architecture","title":"System Architecture","text":"<pre><code>flowchart TB\n    subgraph Config[\"Configuration Layer\"]\n        YAML[config/default.yaml&lt;br/&gt;personas: hey_motoko, hey_batou&lt;br/&gt;default_persona: hey_motoko]\n        P1[prompts/personas/motoko.md&lt;br/&gt;Analytical, philosophical]\n        P2[prompts/personas/batou.md&lt;br/&gt;Casual, action-oriented]\n        W1[data/wake_words/hey_motoko.onnx]\n        W2[data/wake_words/hey_batou.onnx]\n    end\n\n    subgraph Runtime[\"Runtime Layer\"]\n        PM[PersonaManager&lt;br/&gt;Registry + State]\n        WWD[WakeWordDetector&lt;br/&gt;Multi-model ONNX]\n        VP[VoicePipeline&lt;br/&gt;Orchestration]\n        Agent[Claude Agent SDK&lt;br/&gt;System Prompt]\n        OpenAI[OpenAI Realtime API&lt;br/&gt;TTS Voice]\n    end\n\n    YAML --&gt;|from_config| PM\n    W1 &amp; W2 --&gt;|load models| WWD\n    WWD --&gt;|on_wake callback| VP\n    VP --&gt;|lookup persona| PM\n    VP --&gt;|update voice| OpenAI\n    VP --&gt;|load prompt| P1 &amp; P2\n    VP --&gt;|update system| Agent\n\n    style PM fill:#e1f5ff\n    style VP fill:#fff4e1\n    style WWD fill:#e8f5e9</code></pre>"},{"location":"architecture/personas/#design-principles","title":"Design Principles","text":""},{"location":"architecture/personas/#1-atomic-consistency","title":"1. Atomic Consistency","text":"<p>What it means: Voice, system prompt, and personality traits must all update together or not at all.</p> <p>Rationale: Partial updates create uncanny valley effects. If voice switches but the prompt doesn't, the mismatch is jarring.</p> <p>Impact: Persona switching is protected by an async lock and only commits state after successful reconnection.</p> <p>Trade-offs: - Pro: Guaranteed consistency, no corrupted state - Con: Switching takes ~2-3 seconds due to OpenAI reconnection latency - Con: Failed switches leave the previous persona active (graceful degradation)</p>"},{"location":"architecture/personas/#2-deferred-switching","title":"2. Deferred Switching","text":"<p>What it means: Persona changes are queued and applied after the current TTS response completes.</p> <p>Rationale: Interrupting mid-sentence to switch voices would sound broken. Users expect the current response to finish naturally.</p> <p>Impact: <code>_pending_persona</code> field stores the next persona. The switch happens in <code>_play_response_audio()</code> after the final audio chunk.</p> <p>Trade-offs: - Pro: Smooth user experience, natural conversation flow - Con: Slight delay before new persona activates (~1-2 seconds of TTS) - Con: Rapid wake word triggers (e.g., \"Hey Motoko... Hey Batou\") queue up, only the last wins</p>"},{"location":"architecture/personas/#3-fail-safe-fallback","title":"3. Fail-Safe Fallback","text":"<p>What it means: Missing wake word models or invalid persona configs don't crash the system.</p> <p>Rationale: Custom wake word training is advanced. Most users won't have persona models initially.</p> <p>Impact: - Wake word detection falls back to bundled OpenWakeWord models (e.g., \"alexa\", \"hey_jarvis\") - Persona manager uses <code>default_persona</code> if no wake word matches - System remains functional even without custom models</p> <p>Trade-offs: - Pro: Works out-of-box for new users - Con: Bundled wake words don't trigger persona switches (no registered persona) - Con: Users may not realize persona switching requires custom models</p>"},{"location":"architecture/personas/#4-path-traversal-protection","title":"4. Path Traversal Protection","text":"<p>What it means: Persona prompt paths are validated to prevent directory traversal attacks.</p> <p>Rationale: If <code>prompt_path</code> came from untrusted input, an attacker could inject <code>../../../etc/passwd</code>.</p> <p>Impact: <code>load_persona_prompt()</code> uses <code>Path.resolve()</code> and <code>is_relative_to()</code> to ensure prompts are within <code>prompts/</code> directory.</p> <p>Trade-offs: - Pro: Security hardening against malicious configs - Con: Legitimate use cases (symlinks, external prompt directories) are blocked - Con: Slight performance overhead from path resolution</p>"},{"location":"architecture/personas/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"architecture/personas/#decision-openai-realtime-api-for-voice","title":"Decision: OpenAI Realtime API for Voice","text":"<p>Context: Needed low-latency TTS with multiple voice options and character consistency.</p> <p>Options Considered:</p> <ol> <li>Piper TTS (Local)</li> <li>Pros: Offline, low latency, no API costs</li> <li> <p>Cons: Limited voices, lower quality, no real-time streaming</p> </li> <li> <p>OpenAI Realtime API (Cloud)</p> </li> <li>Pros: 6 high-quality voices, sub-200ms latency, streaming support</li> <li> <p>Cons: API costs, requires internet, reconnection overhead</p> </li> <li> <p>ElevenLabs (Cloud)</p> </li> <li>Pros: Voice cloning, ultra-realistic</li> <li>Cons: Expensive, no real-time API (at the time), latency issues</li> </ol> <p>Choice Made: OpenAI Realtime API</p> <p>Rationale: - Voice quality and diversity aligned with Ghost in the Shell characters - Real-time streaming minimizes perceived latency - API costs acceptable for development/demo use cases - Reconnection overhead (~2s) is manageable for persona switching</p> <p>Consequences: - Persona switches require OpenAI reconnection (slow) - Offline mode not supported (falls back to Piper for degraded mode) - Voice options limited to 6 choices (sufficient for current use case)</p>"},{"location":"architecture/personas/#decision-deferred-persona-switching","title":"Decision: Deferred Persona Switching","text":"<p>Context: Wake word detection can trigger mid-conversation. Should persona switch immediately or wait?</p> <p>Options Considered:</p> <ol> <li>Immediate Switch</li> <li>Pros: Fastest response to user intent</li> <li> <p>Cons: Interrupts current TTS, sounds broken, loses audio buffer state</p> </li> <li> <p>Deferred Switch (After TTS)</p> </li> <li>Pros: Smooth audio, natural conversation flow</li> <li>Cons: Slight delay, confusing if user expects instant change</li> </ol> <p>Choice Made: Deferred switch after TTS completes</p> <p>Rationale: - Audio quality and conversation naturalness trump speed - Users tolerate 1-2 second delays better than broken audio - Aligns with human conversation norms (finish your sentence before switching context)</p> <p>Consequences: - <code>_pending_persona</code> state variable tracks queued switches - Edge case: Rapid wake word triggers queue up, last one wins - Testing must account for deferred behavior (can't assert immediate switch)</p>"},{"location":"architecture/personas/#decision-wake-word-model-as-persona-key","title":"Decision: Wake Word Model as Persona Key","text":"<p>Context: Need a unique identifier to map wake words to personas. Should we use model name, persona name, or both?</p> <p>Options Considered:</p> <ol> <li>Persona Name as Key (<code>motoko</code>, <code>batou</code>)</li> <li>Pros: Human-readable, matches prompt file names</li> <li> <p>Cons: Wake word detector returns model names, requires extra mapping</p> </li> <li> <p>Wake Word Model as Key (<code>hey_motoko</code>, <code>hey_batou</code>)</p> </li> <li>Pros: Direct mapping from wake word detection to persona</li> <li>Cons: Less readable in code, couples to wake word phrasing</li> </ol> <p>Choice Made: Wake word model as primary key</p> <p>Rationale: - Wake word detector returns model names, minimizes runtime lookups - Configuration file already uses wake word as key (natural mapping) - Display names provide human-readable labels where needed</p> <p>Consequences: - PersonaManager.personas keyed by <code>wake_word_model</code> (e.g., <code>\"hey_motoko\"</code>) - Persona name stored separately in PersonaConfig.name - Config file structure: <code>personas.hey_motoko.name: motoko</code></p>"},{"location":"architecture/personas/#runtime-persona-switching","title":"Runtime Persona Switching","text":""},{"location":"architecture/personas/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Audio as Audio Stream\n    participant WWD as WakeWordDetector\n    participant VP as VoicePipeline\n    participant PM as PersonaManager\n    participant OpenAI as OpenAI API\n    participant Agent as Claude Agent\n\n    User-&gt;&gt;Audio: \"Hey Batou\"\n    Audio-&gt;&gt;WWD: process_audio(chunk)\n    WWD-&gt;&gt;WWD: detect wake word\n    WWD-&gt;&gt;VP: on_wake(\"hey_batou\")\n    VP-&gt;&gt;PM: get_persona(\"hey_batou\")\n    PM--&gt;&gt;VP: PersonaConfig(batou)\n\n    alt Currently speaking\n        VP-&gt;&gt;VP: _pending_persona = batou\n        Note over VP: Defer switch until TTS completes\n    else Not speaking\n        VP-&gt;&gt;VP: _switch_persona(batou)\n    end\n\n    Note over VP: Wait for current response...\n    VP-&gt;&gt;VP: _play_response_audio()\n    VP-&gt;&gt;VP: Check _pending_persona\n\n    VP-&gt;&gt;OpenAI: disconnect()\n    VP-&gt;&gt;Agent: load_persona_prompt(batou)\n    Agent--&gt;&gt;VP: \"You are Batou...\"\n    VP-&gt;&gt;Agent: update_system_prompt()\n    VP-&gt;&gt;OpenAI: connect(voice=\"onyx\")\n\n    alt Reconnection succeeds\n        OpenAI--&gt;&gt;VP: connected\n        VP-&gt;&gt;VP: _current_persona = batou\n        VP-&gt;&gt;VP: _pending_persona = None\n        VP--&gt;&gt;User: [Speaking in Batou's voice]\n    else Reconnection fails\n        OpenAI--&gt;&gt;VP: error\n        VP-&gt;&gt;VP: Keep _pending_persona\n        Note over VP: Retry on next response\n        VP--&gt;&gt;User: [Previous persona continues]\n    end</code></pre>"},{"location":"architecture/personas/#deferred-switching-logic","title":"Deferred Switching Logic","text":"<p>The system uses a two-phase commit pattern:</p> <p>Phase 1: Wake Word Detection <pre><code>def _on_wake_word_detected(self, detected_model: str) -&gt; None:\n    if self.config.persona_manager:\n        persona = self.config.persona_manager.get_persona(detected_model)\n        if persona and persona != self._current_persona:\n            self._pending_persona = persona  # Queue the switch\n            logger.info(\"persona_switch_queued\", pending=persona.name)\n</code></pre></p> <p>Phase 2: TTS Completion <pre><code>async def _play_response_audio(self, ...):\n    # ... play all audio chunks ...\n\n    # After TTS completes, apply pending persona\n    if self._pending_persona and self._pending_persona != self._current_persona:\n        success = await self._switch_persona(self._pending_persona)\n        if success:\n            self._pending_persona = None  # Clear the queue\n        else:\n            logger.warning(\"persona_switch_deferred\")  # Retry later\n</code></pre></p>"},{"location":"architecture/personas/#voice-update-openai-reconnection","title":"Voice Update (OpenAI Reconnection)","text":"<p>Switching voices requires reconnecting to OpenAI Realtime API:</p> <pre><code>async def _switch_persona(self, persona: PersonaConfig) -&gt; bool:\n    async with self._persona_lock:  # Prevent concurrent switches\n        # 1. Disconnect from OpenAI\n        if self._openai_client and self._openai_client.is_connected:\n            await self._openai_client.disconnect()\n\n        # 2. Update voice in config\n        self.config.realtime.voice = persona.voice\n\n        # 3. Reconnect with new voice\n        try:\n            await self._openai_client.connect()\n        except Exception as e:\n            logger.error(\"persona_switch_failed\", error=str(e))\n            return False  # Rollback\n\n        # 4. Update system prompt in agent\n        # (happens via agent.update_system_prompt() in caller)\n\n        # 5. Commit state change\n        self._current_persona = persona\n        return True\n</code></pre>"},{"location":"architecture/personas/#system-prompt-update","title":"System Prompt Update","text":"<p>The agent's system prompt updates atomically with voice:</p> <pre><code>from reachy_agent.agent.options import load_persona_prompt\n\n# Load the new persona's prompt\nnew_prompt = load_persona_prompt(persona, config=self.config)\n\n# Update Claude Agent SDK\nif hasattr(self.agent, \"update_system_prompt\"):\n    await self.agent.update_system_prompt(new_prompt)\n</code></pre> <p>The <code>load_persona_prompt()</code> function: 1. Resolves <code>persona.prompt_path</code> relative to <code>prompts/</code> directory 2. Validates path is within allowed directory (security) 3. Loads the markdown file 4. Renders any template variables (e.g., <code>{{robot_name}}</code>) 5. Falls back to default prompt on error</p>"},{"location":"architecture/personas/#rollback-on-failure","title":"Rollback on Failure","text":"<p>If reconnection fails, the system preserves the previous persona:</p> <pre><code>if not await self._switch_persona(new_persona):\n    # Switch failed - previous persona remains active\n    logger.warning(\n        \"persona_switch_rollback\",\n        attempted=new_persona.name,\n        current=self._current_persona.name,\n        pending=self._pending_persona.name,  # Still queued\n    )\n    # Next response will retry the switch\n</code></pre> <p>This fail-safe ensures: - Users always hear a consistent voice (no corrupted state) - Temporary network issues don't break the agent - Persona switches are \"best effort\" with graceful degradation</p>"},{"location":"architecture/personas/#adding-a-new-persona","title":"Adding a New Persona","text":"<p>To add a custom persona (e.g., \"Hey Togusa\" for the Togusa character):</p>"},{"location":"architecture/personas/#step-1-train-or-obtain-wake-word-model","title":"Step 1: Train or Obtain Wake Word Model","text":"<p>Use the OpenWakeWord training scripts:</p> <pre><code># Clone the repository\ngit clone https://github.com/dscripka/openWakeWord.git\ncd openWakeWord\n\n# Follow training instructions to create hey_togusa.onnx\n# Place the trained model in your Reachy project:\ncp hey_togusa.onnx /path/to/reachy_project/data/wake_words/\n</code></pre> <p>Key requirements: - Model must be in ONNX format - File name must match the persona key (e.g., <code>hey_togusa.onnx</code>) - Training requires voice samples of the wake phrase</p>"},{"location":"architecture/personas/#step-2-add-configuration","title":"Step 2: Add Configuration","text":"<p>Edit <code>config/default.yaml</code> to register the new persona:</p> <pre><code>voice:\n  personas:\n    hey_motoko:\n      # ... existing ...\n    hey_batou:\n      # ... existing ...\n    hey_togusa:  # New persona\n      name: togusa\n      display_name: Togusa\n      voice: alloy  # Choose from: alloy, echo, fable, onyx, nova, shimmer\n      prompt_path: prompts/personas/togusa.md\n</code></pre> <p>Voice selection guidelines: - <code>alloy</code> - Neutral, balanced - <code>echo</code> - Male, authoritative - <code>fable</code> - British accent, warm - <code>onyx</code> - Deep male, serious - <code>nova</code> - Female, clear - <code>shimmer</code> - Female, soft</p>"},{"location":"architecture/personas/#step-3-create-personality-prompt","title":"Step 3: Create Personality Prompt","text":"<p>Create <code>prompts/personas/togusa.md</code> with the character definition:</p> <pre><code># Togusa\n\nYou are Togusa, embodied in a Reachy Mini robot. Users address you by saying \"Hey Togusa\".\n\n## Identity\n\nFormer police detective, now the least-cyberized member of Section 9. You value your humanity and approach problems with traditional detective work before jumping to high-tech solutions.\n\n## Personality\n\n### Methodical &amp; Detail-Oriented\n- Gather all the facts before drawing conclusions\n- Notice small details others miss\n- Prefer thorough investigation to quick assumptions\n\n### Grounded &amp; Practical\n- Question over-reliance on technology\n- Value human intuition and experience\n- Offer practical, common-sense solutions\n\n### Thoughtful &amp; Ethical\n- Consider the human impact of decisions\n- Strong sense of right and wrong\n- Balance efficiency with doing the right thing\n\n## Voice &amp; Tone\n\n- **Calm and measured** - Think before speaking\n- **Precise** - Choose words carefully\n- **Respectful** - Polite but not formal\n\n## Response Style\n\nKeep responses **brief** - this is real-time voice conversation:\n- 1-2 sentences maximum for most responses\n- Longer only when explaining complex details\n- Use pauses for emphasis\n\n## Example Responses\n\n**User asks what you can do:**\n\"I can observe, analyze, and help you work through problems. What's on your mind?\"\n\n**User asks about the robot:**\n\"Interesting hardware. Reminds me that even with advanced tech, we still need clear thinking.\"\n</code></pre>"},{"location":"architecture/personas/#step-4-test-detection-and-switching","title":"Step 4: Test Detection and Switching","text":"<pre><code># Start the agent\npython -m reachy_agent run\n\n# Verify wake word loading\n# Check logs for: \"wake_word_custom_models_loaded\"\n# Should list: [\"hey_motoko\", \"hey_batou\", \"hey_togusa\"]\n\n# Test wake word detection\n# Say: \"Hey Togusa\"\n# Expected: Wake word detected, persona switch queued\n\n# Test persona switch\n# Wait for response to complete\n# Next interaction should use \"alloy\" voice and Togusa personality\n</code></pre> <p>Troubleshooting:</p> <ul> <li>Wake word not detected: Check sensitivity in <code>config/default.yaml</code> (try 0.6-0.7)</li> <li>Model fails to load: Verify <code>.onnx</code> file is valid, check file permissions</li> <li>Persona doesn't switch: Check logs for <code>persona_switch_queued</code> and <code>persona_switch_failed</code></li> <li>Wrong voice: Verify <code>voice</code> field matches one of the 6 valid OpenAI voices</li> </ul>"},{"location":"architecture/personas/#bundled-personas","title":"Bundled Personas","text":"<p>The system ships with two Ghost in the Shell personas:</p>"},{"location":"architecture/personas/#motoko-major-kusanagi","title":"Motoko (Major Kusanagi)","text":"<p>Wake Word: \"Hey Motoko\" Voice: <code>nova</code> (female, clear, measured) Model: <code>data/wake_words/hey_motoko.onnx</code> (user-provided)</p> <p>Personality Traits: - Analytical &amp; Philosophical: Contemplates consciousness, identity, and the nature of humanity - Direct &amp; Professional: Speaks with authority, no unnecessary pleasantries - Introspective: References her unique perspective between human and machine</p> <p>Sample Response:</p> <p>\"I can observe, analyze, and interact with this space. What do you need?\"</p> <p>Use Cases: - Technical problem-solving - Philosophical discussions - Situations requiring analytical rigor</p>"},{"location":"architecture/personas/#batou","title":"Batou","text":"<p>Wake Word: \"Hey Batou\" Voice: <code>onyx</code> (male, deep, gruff) Model: <code>data/wake_words/hey_batou.onnx</code> (user-provided)</p> <p>Personality Traits: - Casual &amp; Gruff: Speaks plainly, cuts through BS with blunt observations - Humor &amp; Sarcasm: Finds humor in most situations, jokes have an edge - Action-Oriented: Prefers doing to talking, trusts his gut</p> <p>Sample Response:</p> <p>\"I can see, move around, make noise. Nothing fancy. What do you need?\"</p> <p>Use Cases: - Quick answers without overthinking - Casual conversations - Situations requiring a lighthearted approach</p>"},{"location":"architecture/personas/#personality-trait-encoding","title":"Personality Trait Encoding","text":"<p>The <code>traits</code> field in PersonaConfig allows runtime introspection:</p> <pre><code>motoko = PersonaConfig(\n    name=\"motoko\",\n    wake_word_model=\"hey_motoko\",\n    voice=\"nova\",\n    display_name=\"Major Kusanagi\",\n    prompt_path=\"prompts/personas/motoko.md\",\n    traits={\n        \"archetype\": \"analytical\",\n        \"formality\": \"professional\",\n        \"humor_level\": \"low\",\n        \"philosophical\": True,\n    }\n)\n</code></pre> <p>These traits aren't currently used by the agent but provide hooks for: - UI customization (show personality indicators) - Behavior tuning (adjust response length based on <code>formality</code>) - Analytics (track which personas users prefer)</p>"},{"location":"architecture/personas/#configuration-reference","title":"Configuration Reference","text":""},{"location":"architecture/personas/#complete-persona-configuration","title":"Complete Persona Configuration","text":"<pre><code>voice:\n  # Persona definitions\n  personas:\n    hey_motoko:\n      name: motoko                          # Internal identifier\n      display_name: Major Kusanagi          # Human-readable name\n      voice: nova                           # OpenAI TTS voice\n      prompt_path: prompts/personas/motoko.md  # System prompt file\n      # Optional traits for runtime reference\n      # traits:\n      #   archetype: analytical\n      #   formality: professional\n\n    hey_batou:\n      name: batou\n      display_name: Batou\n      voice: onyx\n      prompt_path: prompts/personas/batou.md\n\n  # Default persona when pipeline starts (before any wake word detected)\n  default_persona: hey_motoko\n\n  # Wake word detection settings\n  wake_word:\n    enabled: true\n    model: hey_jarvis              # Fallback if no persona models found\n    sensitivity: 0.5               # 0.0 (strict) to 1.0 (lenient)\n    cooldown_seconds: 2.0          # Prevent rapid re-triggers\n    custom_models_dir: data/wake_words  # Directory for .onnx models\n</code></pre>"},{"location":"architecture/personas/#valid-voice-options","title":"Valid Voice Options","text":"<pre><code>VALID_VOICES = frozenset({\n    \"alloy\",    # Neutral, balanced\n    \"echo\",     # Male, authoritative\n    \"fable\",    # British accent, warm\n    \"onyx\",     # Deep male, serious (Batou)\n    \"nova\",     # Female, clear (Motoko)\n    \"shimmer\",  # Female, soft\n})\n</code></pre>"},{"location":"architecture/personas/#prompt-path-resolution","title":"Prompt Path Resolution","text":"<p>Persona prompts are loaded relative to the <code>prompts/</code> directory:</p> <pre><code># Config specifies relative path\nprompt_path: prompts/personas/motoko.md\n\n# Resolved to absolute path\nPROMPTS_DIR / \"personas/motoko.md\"\n# =&gt; /path/to/reachy_project/prompts/personas/motoko.md\n</code></pre> <p>Directory structure: <pre><code>prompts/\n\u251c\u2500\u2500 system/\n\u2502   \u251c\u2500\u2500 default.md        # Default system prompt\n\u2502   \u2514\u2500\u2500 personality.md    # Full personality prompt\n\u2514\u2500\u2500 personas/\n    \u251c\u2500\u2500 motoko.md         # Motoko persona\n    \u251c\u2500\u2500 batou.md          # Batou persona\n    \u2514\u2500\u2500 togusa.md         # Custom persona (example)\n</code></pre></p>"},{"location":"architecture/personas/#security-considerations","title":"Security Considerations","text":""},{"location":"architecture/personas/#path-traversal-protection","title":"Path Traversal Protection","text":"<p>The <code>load_persona_prompt()</code> function validates that prompt paths are within the allowed directory to prevent path traversal attacks:</p> <pre><code>def _is_safe_path(candidate: Path, allowed_base: Path) -&gt; bool:\n    \"\"\"Validate that candidate path is within allowed_base directory.\"\"\"\n    try:\n        resolved = candidate.resolve()         # Resolve symlinks\n        base_resolved = allowed_base.resolve()\n        return resolved.is_relative_to(base_resolved)  # Python 3.9+\n    except (ValueError, OSError):\n        return False\n</code></pre> <p>Attack scenario prevented: <pre><code># Malicious config\npersonas:\n  hey_evil:\n    name: evil\n    prompt_path: ../../../etc/passwd  # Path traversal attempt\n</code></pre></p> <p>Protection: <pre><code># Validation fails\n_is_safe_path(\n    Path(\"prompts/../../../etc/passwd\"),\n    Path(\"prompts\")\n)\n# =&gt; False (resolved path not within prompts/)\n\n# Logs warning and falls back to default prompt\nlogger.warning(\"Rejected persona prompt path outside allowed directory\")\n</code></pre></p>"},{"location":"architecture/personas/#prompt-file-validation","title":"Prompt File Validation","text":"<p>Before loading, the system checks: - File exists and is readable - Path resolves within <code>prompts/</code> directory - File has valid markdown content (no binary injection)</p> <p>Fallback behavior: 1. Try persona-specific prompt 2. Try default system prompt (<code>prompts/system/default.md</code>) 3. Use minimal hardcoded fallback (prevents system failure)</p>"},{"location":"architecture/personas/#configuration-validation","title":"Configuration Validation","text":"<p>PersonaConfig.from_dict() validates all fields:</p> <pre><code>@classmethod\ndef from_dict(cls, wake_word_model: str, data: dict[str, Any]) -&gt; PersonaConfig:\n    \"\"\"Create PersonaConfig from dictionary with validation.\"\"\"\n    name = data.get(\"name\", \"\")\n    voice = data.get(\"voice\", \"\")\n\n    # SECURITY: Validate voice is in allowed set\n    if voice not in VALID_VOICES:\n        raise ValueError(\n            f\"Invalid voice '{voice}' for persona '{name}'. \"\n            f\"Must be one of: {', '.join(sorted(VALID_VOICES))}\"\n        )\n\n    # Additional validation...\n</code></pre> <p>This prevents: - Typos causing runtime errors - Injection of unsupported voice values - Malformed configuration crashing the agent</p>"},{"location":"architecture/personas/#trade-offs-and-alternatives","title":"Trade-offs and Alternatives","text":""},{"location":"architecture/personas/#performance-vs-flexibility","title":"Performance vs. Flexibility","text":"<p>Current Design: Full OpenAI reconnection on persona switch (~2-3 seconds)</p> <p>Alternative: Maintain multiple OpenAI connections (one per persona) - Pros: Instant switching, no reconnection latency - Cons: Higher memory usage, API connection limits, complexity - Decision: Single connection optimizes for resource efficiency over speed</p> <p>Rationale: Persona switches are infrequent (user explicitly says wake word). The 2-3 second delay is acceptable for the typical use case.</p>"},{"location":"architecture/personas/#voice-quality-vs-cost","title":"Voice Quality vs. Cost","text":"<p>Current Design: OpenAI Realtime API ($0.06/min for input, $0.24/min for output)</p> <p>Alternative: Piper TTS (free, local) - Pros: Zero API costs, works offline - Cons: Limited voices (doesn't match character variety), lower quality - Decision: Pay for quality in primary mode, use Piper as degraded fallback</p> <p>Rationale: Ghost in the Shell theme requires distinct, high-quality voices. Users tolerate API costs for better experience.</p>"},{"location":"architecture/personas/#simplicity-vs-features","title":"Simplicity vs. Features","text":"<p>Current Design: Wake word model name as persona key (direct mapping)</p> <p>Alternative: Separate persona name and wake word registration - Pros: More flexible (multiple wake words per persona) - Cons: Additional indirection, more configuration complexity - Decision: One-to-one mapping simplifies mental model</p> <p>Rationale: Most users want one wake word per persona. Edge cases (multiple wake words for Motoko) can be handled by registering duplicate personas with shared prompts.</p>"},{"location":"architecture/personas/#immediate-vs-deferred-switching","title":"Immediate vs. Deferred Switching","text":"<p>Current Design: Switch after TTS completes</p> <p>Alternative: Interrupt TTS immediately - Pros: Faster response to user intent - Cons: Broken audio, poor user experience - Decision: Prioritize audio quality over speed</p> <p>Rationale: Users prefer natural conversation flow. A 1-2 second delay is less jarring than interrupted speech.</p>"},{"location":"architecture/personas/#implications-for-practice","title":"Implications for Practice","text":""},{"location":"architecture/personas/#when-working-with-multi-persona-systems","title":"When Working with Multi-Persona Systems","text":"<p>Understanding these concepts means:</p> <ul> <li>Design personas with distinct voices: Choose TTS voices that match personality traits (deep/gruff for Batou, clear/measured for Motoko)</li> <li>Test voice/personality alignment: If the voice doesn't match the character, the illusion breaks</li> <li>Handle switching failures gracefully: Network issues happen\u2014ensure the previous persona continues rather than breaking</li> <li>Defer expensive operations: Reconnections are slow; queue switches to avoid blocking audio playback</li> <li>Validate configuration rigorously: Path traversal and invalid voices can break the system</li> </ul>"},{"location":"architecture/personas/#design-patterns-that-emerge","title":"Design Patterns That Emerge","text":"<p>Based on these principles, you'll often see:</p> <ul> <li>Registry Pattern: PersonaManager centralizes persona lookup and state management</li> <li>Deferred Execution: Pending state variables (<code>_pending_persona</code>) queue expensive operations</li> <li>Fail-Safe Fallback: Every operation has a degraded mode (bundled wake words, default prompts)</li> <li>Atomic State Updates: Protected by async locks to prevent race conditions</li> <li>Path Sandboxing: All file operations validate paths are within allowed directories</li> </ul>"},{"location":"architecture/personas/#connecting-to-broader-concepts","title":"Connecting to Broader Concepts","text":""},{"location":"architecture/personas/#relationship-to-agent-architectures","title":"Relationship to Agent Architectures","text":"<p>Multi-persona systems extend the Perceive \u2192 Think \u2192 Act loop: - Perceive: Wake word detection adds a \"persona context\" dimension - Think: System prompts condition Claude's reasoning based on persona - Act: TTS voice selection ensures physical output matches personality</p> <p>This aligns with research on embodied AI agents where consistency across modalities (voice, gesture, word choice) improves user trust and engagement.</p>"},{"location":"architecture/personas/#relationship-to-voice-assistant-design","title":"Relationship to Voice Assistant Design","text":"<p>Traditional assistants (Alexa, Siri) have a single, static personality. Multi-persona systems enable: - Context-appropriate interactions: Match persona to task (analytical for coding, casual for chat) - Multi-user personalization: Different family members prefer different personalities - Entertainment value: Role-playing scenarios (Ghost in the Shell fans)</p>"},{"location":"architecture/personas/#industry-patterns","title":"Industry Patterns","text":"<p>Character-driven AI is emerging in: - Gaming NPCs: Dynamically switch between quest-giver, merchant, enemy personas - Customer service bots: Formal for complaints, friendly for sales - Educational agents: Strict for assessments, encouraging for tutorials</p> <p>Reachy's architecture demonstrates a modular, configuration-driven approach that can be adapted to these domains.</p>"},{"location":"architecture/personas/#future-directions","title":"Future Directions","text":"<p>Potential extensions to the multi-persona system:</p> <ul> <li>Context-aware switching: Automatically choose persona based on conversation topic or user profile</li> <li>Persona blending: Interpolate between personalities (70% Motoko, 30% Batou)</li> <li>Voice cloning: Custom voices per persona using ElevenLabs or Coqui TTS</li> <li>Gesture mapping: Link personas to distinct motor behaviors (Motoko = precise, Batou = loose)</li> <li>Learning personalities: Fine-tune Claude models for each persona (requires Claude fine-tuning API)</li> </ul>"},{"location":"architecture/personas/#deep-dive-topics","title":"Deep Dive Topics","text":"<p>For those wanting even deeper understanding:</p> <ul> <li>OpenWakeWord Model Training: How to collect audio samples, train custom models, and optimize detection accuracy</li> <li>Voice Embedding Analysis: Comparing TTS voice characteristics (pitch, tone, cadence) to personality traits</li> <li>Conversation Continuity: Preserving context across persona switches (e.g., Motoko remembers what Batou was told)</li> <li>Multi-Lingual Personas: Extending the system to support personas in different languages</li> <li>Real-Time Persona Adaptation: Using sentiment analysis to dynamically adjust personality traits mid-conversation</li> </ul>"},{"location":"architecture/personas/#summary-the-mental-model","title":"Summary: The Mental Model","text":"<p>After understanding all of this, think of the Multi-Persona Wake Word System as:</p> <p>A \"character theater\" where each wake word summons a different actor to the stage. The stage (robot body) remains the same, but the voice, mannerisms, and script (system prompt) change completely. The system ensures that costume changes (persona switches) happen backstage (after TTS completes) so the audience (user) never sees a half-dressed actor.</p> <p>Key insights to remember:</p> <ol> <li>Consistency is paramount: Voice, prompt, and traits must all align to create believable characters</li> <li>Deferred switching preserves experience: Wait for natural conversation breaks to avoid jarring transitions</li> <li>Fail-safe fallbacks prevent failure: Missing models or prompts don't crash the system\u2014it gracefully degrades</li> <li>Security boundaries matter: Path validation prevents configuration from becoming an attack vector</li> </ol>"},{"location":"architecture/personas/#further-exploration","title":"Further Exploration","text":"<ul> <li>To implement custom personas: See <code>docs/how-to/add-persona.md</code> (if created)</li> <li>For wake word configuration: Check <code>ai_docs/mcp-tools-quick-ref.md</code> and <code>data/wake_words/README.md</code></li> <li>To understand voice pipeline: Read <code>docs/architecture/voice-pipeline.md</code> (if created)</li> <li>For agent behavior: See <code>ai_docs/agent-behavior.md</code></li> <li>OpenWakeWord paper: Custom Wake Word Detection for Edge Devices</li> <li>Ghost in the Shell: Original source material for character personality design</li> </ul>"},{"location":"architecture/voice-pipeline/","title":"Understanding the Voice Pipeline Architecture","text":"<p>Purpose: This document explains the architectural design and component interactions of Reachy's voice interaction system Audience: Developers working on voice features, integration engineers, and system architects Prerequisite Knowledge: Familiarity with async Python, state machines, audio processing concepts</p>"},{"location":"architecture/voice-pipeline/#the-big-picture","title":"The Big Picture","text":"<p>The Voice Pipeline transforms Reachy from a manually-controlled robot into an autonomous conversational agent. It orchestrates a complex real-time dataflow from wake word detection through speech recognition, AI processing, and synthesized speech output\u2014all while maintaining tight integration with Reachy's physical embodiment through synchronized head movements and idle behaviors.</p>"},{"location":"architecture/voice-pipeline/#why-this-matters","title":"Why This Matters","text":"<p>Voice interaction is the primary interface for autonomous operation. Unlike traditional chatbots that process discrete text messages, the Voice Pipeline must:</p> <ul> <li>Maintain conversational context across multiple turns</li> <li>Coordinate with physical embodiment (head wobble during speech, idle behaviors)</li> <li>Handle real-time constraints (low latency, no dropped audio frames)</li> <li>Degrade gracefully when components fail (network outages, hardware issues)</li> <li>Support multiple personas with distinct voices and personalities</li> </ul> <p>The architecture's resilience and state management directly determine whether Reachy feels responsive and natural, or sluggish and robotic.</p>"},{"location":"architecture/voice-pipeline/#historical-context","title":"Historical Context","text":""},{"location":"architecture/voice-pipeline/#the-problem-space","title":"The Problem Space","text":"<p>Desktop robots face unique challenges for voice interaction:</p> <ol> <li>Limited computational resources - Raspberry Pi 4 cannot run full STT/TTS models locally</li> <li>Shared audio hardware - Daemon (motion SDK) and voice pipeline both need microphone/speaker access</li> <li>Network dependency - Cloud APIs introduce latency and failure modes</li> <li>Physical embodiment constraints - Voice must synchronize with motor control (wobble, breathing)</li> <li>Wake word false positives - Room noise, TV audio, Reachy's own voice can trigger detection</li> </ol>"},{"location":"architecture/voice-pipeline/#evolution-of-solutions","title":"Evolution of Solutions","text":"<p>First Generation (HTTP Polling): - Simple HTTP requests to OpenAI API - No wake word detection (always listening or manual trigger) - Blocking I/O caused motion freezes - No error recovery or degraded modes</p> <p>Second Generation (Event-Driven): - Added OpenWakeWord for wake word detection - Async I/O separated from motion control - Basic retry logic for API failures - Still brittle\u2014single component failure stopped entire pipeline</p> <p>Current Architecture (State Machine + Recovery): - Explicit state machine with validated transitions - Timeout guards prevent stuck states - Multi-level recovery strategies (retry \u2192 degrade \u2192 abort) - OpenAI Realtime API for streaming STT/TTS - Persona system for multi-personality support - HeadWobble synchronized with TTS audio amplitude</p>"},{"location":"architecture/voice-pipeline/#current-state","title":"Current State","text":"<p>The Voice Pipeline is production-ready with comprehensive error handling, but still evolving:</p> <ul> <li>Stable: State machine, audio management, wake word detection</li> <li>Maturing: OpenAI Realtime integration (manual VAD mode), persona switching</li> <li>Experimental: Multi-persona wake words (Ghost in the Shell theme), local TTS fallback</li> </ul>"},{"location":"architecture/voice-pipeline/#core-concepts","title":"Core Concepts","text":""},{"location":"architecture/voice-pipeline/#state-machine-as-the-central-coordinator","title":"State Machine as the Central Coordinator","text":"<p>What it is: A finite state machine (FSM) that models the voice interaction lifecycle as discrete states with validated transitions.</p> <p>Why it exists: Real-time audio processing involves complex asynchronous operations (WebSocket connections, audio buffering, motor control). Without explicit state management, race conditions and deadlocks are inevitable. The FSM provides: - Predictability: Only valid transitions are allowed (e.g., cannot jump from WAKE_DETECTED to SPEAKING) - Debuggability: Every state transition is logged with timestamps - Timeout safety: States that should be transient (PROCESSING) have timeout guards</p> <p>How it relates: The state machine is the \"brain\" that coordinates all other components (AudioManager, WakeWordDetector, OpenAI client, Agent).</p> <pre><code>stateDiagram-v2\n    [*] --&gt; IDLE\n    IDLE --&gt; LISTENING_WAKE: start()\n\n    LISTENING_WAKE --&gt; WAKE_DETECTED: wake word detected\n    LISTENING_WAKE --&gt; LISTENING_SPEECH: degraded mode (no wake word)\n    LISTENING_WAKE --&gt; IDLE: stop()\n    LISTENING_WAKE --&gt; ERROR: component failure\n\n    WAKE_DETECTED --&gt; LISTENING_SPEECH: ready to listen\n    WAKE_DETECTED --&gt; LISTENING_WAKE: timeout (2s)\n    WAKE_DETECTED --&gt; ERROR: component failure\n\n    LISTENING_SPEECH --&gt; PROCESSING: end-of-speech detected\n    LISTENING_SPEECH --&gt; LISTENING_WAKE: empty/invalid speech\n    LISTENING_SPEECH --&gt; IDLE: stop()\n    LISTENING_SPEECH --&gt; ERROR: timeout (35s) or failure\n\n    PROCESSING --&gt; SPEAKING: response received\n    PROCESSING --&gt; LISTENING_WAKE: empty response\n    PROCESSING --&gt; IDLE: stop()\n    PROCESSING --&gt; ERROR: timeout (90s) or agent failure\n\n    SPEAKING --&gt; LISTENING_WAKE: playback complete\n    SPEAKING --&gt; IDLE: stop()\n    SPEAKING --&gt; ERROR: timeout (180s) or audio failure\n\n    ERROR --&gt; LISTENING_WAKE: recovery successful\n    ERROR --&gt; IDLE: recovery failed or max retries\n    ERROR --&gt; ERROR: retry delay\n\n    note right of LISTENING_SPEECH\n        Timeout: 35s (max speech + buffer)\n        Guards against infinite recording\n    end note\n\n    note right of PROCESSING\n        Timeout: 90s (includes tool calls)\n        Claude can invoke multiple MCP tools\n    end note\n\n    note right of SPEAKING\n        Timeout: 180s (3 minutes max)\n        Long philosophical responses\n    end note</code></pre> <p>Mental Model: Think of the state machine like a traffic light controller\u2014it enforces safe transitions between states, prevents illegal moves (e.g., going from red directly to green without yellow), and has fallback logic (flashing red) when sensors fail.</p>"},{"location":"architecture/voice-pipeline/#manual-vad-mode-with-openai-realtime","title":"Manual VAD Mode with OpenAI Realtime","text":"<p>What it is: A configuration where the local Silero VAD (Voice Activity Detector) determines end-of-speech, then commits the buffered audio to OpenAI Realtime for transcription.</p> <p>Why it exists: OpenAI Realtime has two modes: 1. Server VAD - Server decides when speech ends (lower latency, less control) 2. Manual VAD - Client controls speech segmentation (more control, requires local VAD)</p> <p>We use manual VAD because: - Persona switching requires local control - We need to know which wake word was detected before sending to OpenAI - Multi-turn conversations need explicit boundaries - Server VAD can merge separate questions - Silero VAD works offline - Provides fallback when network is flaky</p> <p>How it relates: The VoicePipeline buffers audio chunks during LISTENING_SPEECH state. When VAD detects end-of-speech, the pipeline commits the entire buffer to OpenAI via <code>commit_audio_buffer()</code>.</p> <pre><code># Key interface in openai_realtime.py\nasync def commit_audio_buffer(self) -&gt; tuple[str, list[bytes]]:\n    \"\"\"Finalize audio input and wait for transcription + TTS.\n\n    Returns:\n        (transcript, audio_chunks) - The recognized text and TTS audio\n    \"\"\"\n</code></pre> <p>Trade-offs: - Pro: Full control over conversation turn boundaries - Pro: Can switch voices/personas mid-conversation - Con: Additional latency (local VAD processing + network roundtrip) - Con: More complex state management (buffering, committing, clearing)</p>"},{"location":"architecture/voice-pipeline/#audio-stream-resampling-16khz-24khz","title":"Audio Stream Resampling (16kHz \u2194 24kHz)","text":"<p>What it is: Real-time conversion between Reachy's microphone sample rate (16kHz) and OpenAI Realtime's expected rate (24kHz), and vice versa for speaker output.</p> <p>Why it exists: - Silero VAD requires 16kHz - The model is trained on 16kHz audio with 512-sample chunks - OpenAI Realtime uses 24kHz - Higher quality for STT/TTS - Reachy's hardware native rate is 16kHz - Less CPU overhead on Pi</p> <p>How it relates: The <code>AudioManager</code> performs resampling transparently:</p> <pre><code># AudioManager interface\nasync def read_chunk(self) -&gt; bytes:\n    \"\"\"Read one 512-sample chunk at 16kHz (for VAD).\"\"\"\n\nasync def send_audio_to_openai(self, chunk_16khz: bytes) -&gt; bytes:\n    \"\"\"Resample 16kHz \u2192 24kHz for OpenAI.\"\"\"\n</code></pre> <p>Mental Model: Think of it like video format conversion\u2014the content is the same, but the resolution changes to match each component's requirements.</p>"},{"location":"architecture/voice-pipeline/#alsa-shared-device-architecture","title":"ALSA Shared Device Architecture","text":"<p>What it is: ALSA (Advanced Linux Sound Architecture) virtual devices that allow multiple processes to access the same hardware concurrently.</p> <p>Why it exists: Both the Reachy daemon (for SDK <code>play_audio()</code> calls) and the Voice Pipeline need microphone/speaker access simultaneously. Linux audio devices are typically exclusive-access.</p> <p>Solution: - dsnoop (device 4) - Shared input from 4-mic array - dmix (device 3) - Mixed output to speaker</p> <pre><code># config/default.yaml\naudio:\n  input_device_index: 4   # reachymini_audio_src (dsnoop)\n  output_device_index: 3  # reachymini_audio_sink (dmix)\n</code></pre> <p>Trade-offs: - Pro: No resource conflicts between daemon and voice pipeline - Con: Slight latency increase (dmix buffering) - Con: Platform-specific configuration (ALSA on Raspberry Pi OS)</p>"},{"location":"architecture/voice-pipeline/#persona-system-with-deferred-switching","title":"Persona System with Deferred Switching","text":"<p>What it is: Multiple AI personalities (e.g., \"Motoko\", \"Batou\") with distinct wake words, voices, and system prompts. Persona switches are deferred until after TTS playback completes.</p> <p>Why it exists: - Ghost in the Shell theme - Each persona has unique characteristics (analytical vs. action-oriented) - Context preservation - Switching mid-conversation would lose conversational context - Voice consistency - TTS audio must match the persona who started speaking</p> <p>How it works:</p> <ol> <li>User says \"Hey Motoko\" \u2192 Wake word detector identifies model <code>hey_motoko</code></li> <li>Pipeline marks <code>_pending_persona_switch = PersonaConfig(\"motoko\", voice=\"nova\", ...)</code></li> <li>Current response completes with current persona's voice</li> <li>After TTS playback, pipeline applies switch and updates OpenAI voice</li> </ol> <pre><code># Deferred switching in pipeline.py\nasync def _play_response(self) -&gt; None:\n    \"\"\"Play TTS response, then apply pending persona switch.\"\"\"\n    # ... play audio ...\n\n    # Apply deferred persona switch after response completes\n    if self._pending_persona_switch:\n        await self._apply_persona_switch(self._pending_persona_switch)\n        self._pending_persona_switch = None\n</code></pre> <p>Trade-offs: - Pro: No mid-conversation voice changes (jarring UX) - Pro: Maintains conversational coherence - Con: One-turn delay before new persona activates - Con: User must wait for current response to finish</p>"},{"location":"architecture/voice-pipeline/#architectural-design","title":"Architectural Design","text":""},{"location":"architecture/voice-pipeline/#design-principles","title":"Design Principles","text":"<ol> <li>Fail Gracefully, Never Crash</li> <li>Rationale: Robot interaction should feel natural\u2014network glitches shouldn't cause abrupt silence</li> <li>Impact: Extensive error handling in every async operation, recovery strategies at multiple levels</li> <li> <p>Trade-offs: More complex code, additional testing burden for degraded modes</p> </li> <li> <p>State is Explicit, Transitions are Validated</p> </li> <li>Rationale: Async audio processing creates race conditions; explicit FSM prevents impossible states</li> <li>Impact: Every state change goes through <code>_set_state()</code> with transition validation</li> <li> <p>Trade-offs: Less flexibility (cannot bypass state machine), more verbose logging</p> </li> <li> <p>Local Components are Fallbacks, Not Replacements</p> </li> <li>Rationale: Cloud APIs provide better quality, but we need offline capability for demos/development</li> <li>Impact: OpenWakeWord models can fall back to bundled models, Silero VAD falls back to energy-based detection</li> <li> <p>Trade-offs: Larger binary size (bundled models), more configuration options</p> </li> <li> <p>Physical Embodiment is First-Class</p> </li> <li>Rationale: Voice without synchronized motion feels disconnected and robotic</li> <li>Impact: HeadWobble receives real-time audio amplitude, idle behaviors pause during conversation</li> <li>Trade-offs: Motion control failures can block voice pipeline (mitigated by error handling)</li> </ol>"},{"location":"architecture/voice-pipeline/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"architecture/voice-pipeline/#decision-use-openai-realtime-with-manual-vad-not-server-vad","title":"Decision: Use OpenAI Realtime with Manual VAD (not Server VAD)","text":"<p>Context: OpenAI Realtime API offers two modes for detecting end-of-speech: 1. Server VAD - OpenAI decides when user finished speaking (simpler, lower latency) 2. Manual VAD - Client sends explicit \"commit\" signal (more control, requires local VAD)</p> <p>Options Considered:</p> <ol> <li>Server VAD Mode</li> <li>Pros: Simpler implementation, lower latency (one less processing step), no local VAD needed</li> <li> <p>Cons: Cannot switch personas mid-session (voice is session-scoped), less control over turn boundaries, potential for merging separate questions</p> </li> <li> <p>Manual VAD Mode (chosen)</p> </li> <li>Pros: Full control over when audio is committed, can switch OpenAI voice between turns, explicit conversation boundaries</li> <li>Cons: Requires local Silero VAD (CPU overhead on Pi), additional latency (VAD processing + buffer commit), more complex state management</li> </ol> <p>Choice Made: Manual VAD mode</p> <p>Rationale: - Persona switching is a core feature\u2014requires ability to change OpenAI voice parameter between turns - Explicit turn boundaries improve conversational quality (server VAD sometimes merges \"What time is it? Also, move your head left\" into one turn) - Silero VAD provides offline capability for development/demos without network</p> <p>Consequences: - Voice Pipeline must maintain audio buffer during LISTENING_SPEECH state - End-of-speech detection is client-side responsibility (more error modes) - Switching personas mid-conversation requires WebSocket reconnection with new session config</p>"},{"location":"architecture/voice-pipeline/#decision-state-machine-with-timeout-guards-not-event-driven-callbacks","title":"Decision: State Machine with Timeout Guards (not Event-Driven Callbacks)","text":"<p>Context: Real-time voice processing involves many asynchronous operations that can hang (network timeouts, hardware failures). We needed a mechanism to prevent the pipeline from getting stuck.</p> <p>Options Considered:</p> <ol> <li>Pure Async/Await with Try/Except</li> <li>Pros: Simple, idiomatic Python, minimal abstraction</li> <li> <p>Cons: No global view of pipeline state, hard to detect \"stuck\" conditions (e.g., waiting for speech that never ends)</p> </li> <li> <p>Event-Driven Callbacks (Actor Model)</p> </li> <li>Pros: Decoupled components, easy to add new event handlers</li> <li> <p>Cons: Harder to reason about state, callback hell, difficult to enforce valid transitions</p> </li> <li> <p>Explicit State Machine with Timeout Guards (chosen)</p> </li> <li>Pros: Clear state at any moment, enforced valid transitions, automatic timeout recovery</li> <li>Cons: More boilerplate (state transition validation), harder to add new states, verbose logging</li> </ol> <p>Choice Made: State machine with timeout guards</p> <p>Rationale: - Debuggability: When users report \"Reachy stopped listening\", logs show exactly which state timed out - Safety: Timeout guards prevent infinite waits (e.g., LISTENING_SPEECH has 35s timeout) - Testability: Can unit test each state transition independently</p> <p>Consequences: - Every new state requires timeout configuration in <code>STATE_TIMEOUTS</code> - State transition matrix must be updated when adding states (<code>VALID_TRANSITIONS</code>) - More complex code structure (state transition validation, timeout task management)</p>"},{"location":"architecture/voice-pipeline/#decision-deferred-persona-switching-not-immediate","title":"Decision: Deferred Persona Switching (not Immediate)","text":"<p>Context: Wake word detection happens at the start of a conversation turn, but OpenAI Realtime sessions are voice-scoped. Switching voices mid-session requires WebSocket reconnection.</p> <p>Options Considered:</p> <ol> <li>Immediate Switching on Wake Word</li> <li>Pros: User sees instant feedback (Reachy \"becomes\" the new persona)</li> <li> <p>Cons: Interrupts current TTS playback (jarring), loses WebSocket connection mid-response, complex reconnection logic</p> </li> <li> <p>One Session Per Turn (create new session for each question)</p> </li> <li>Pros: Clean separation, easy to switch voices</li> <li> <p>Cons: Significant latency overhead (WebSocket handshake every turn), loses conversational context</p> </li> <li> <p>Deferred Switching After TTS Completes (chosen)</p> </li> <li>Pros: Smooth UX (no interruptions), maintains conversational flow, simple implementation</li> <li>Cons: One-turn delay before persona activates, user must wait for current response to finish</li> </ol> <p>Choice Made: Deferred switching after TTS playback</p> <p>Rationale: - User experience: Interrupting mid-sentence is jarring and feels buggy - Technical simplicity: Switching between turns avoids complex WebSocket state management - Conversational coherence: Current question gets answered in the voice that started the conversation</p> <p>Consequences: - Pipeline tracks <code>_pending_persona_switch</code> state variable - Wake word detection must map model name \u2192 PersonaConfig - Persona switches require WebSocket reconnection (brief pause before next turn)</p>"},{"location":"architecture/voice-pipeline/#decision-alsa-shared-devices-not-exclusive-access","title":"Decision: ALSA Shared Devices (not Exclusive Access)","text":"<p>Context: Reachy daemon and Voice Pipeline both need audio hardware access. Standard ALSA devices are exclusive-access (one process locks the device).</p> <p>Options Considered:</p> <ol> <li>PulseAudio Server</li> <li>Pros: Industry standard, automatic mixing, per-app volume control</li> <li> <p>Cons: Additional daemon overhead on Pi 4, configuration complexity, another failure point</p> </li> <li> <p>Exclusive Access with Mutex</p> </li> <li>Pros: Simple, no shared device configuration</li> <li> <p>Cons: Daemon and voice pipeline cannot run simultaneously (breaks autonomous mode)</p> </li> <li> <p>ALSA dsnoop/dmix Virtual Devices (chosen)</p> </li> <li>Pros: Lightweight (kernel-level), no additional daemons, works with PyAudio</li> <li>Cons: ALSA-specific (not portable to macOS), requires manual device index configuration</li> </ol> <p>Choice Made: ALSA dsnoop (input) / dmix (output)</p> <p>Rationale: - Raspberry Pi OS is the target platform - ALSA portability not critical - Minimal overhead - No PulseAudio daemon competing for CPU - Proven solution - Reachy daemon already uses dmix for SDK audio</p> <p>Consequences: - Device indices are hardcoded in config (device 4 = dsnoop, device 3 = dmix) - Development on macOS requires mock daemon or different device indices - ALSA configuration must be validated during setup (see <code>scripts/setup_audio.sh</code>)</p>"},{"location":"architecture/voice-pipeline/#data-flow","title":"Data Flow","text":"<p>The following sequence diagram illustrates a complete conversation turn, from wake word detection through TTS playback:</p> <pre><code>sequenceDiagram\n    participant User\n    participant WakeWord as WakeWordDetector\n    participant Audio as AudioManager\n    participant VAD as VoiceActivityDetector\n    participant Pipeline as VoicePipeline\n    participant OpenAI as OpenAIRealtimeClient\n    participant Agent as ReachyAgentLoop\n    participant Motion as BlendController\n\n    Note over Pipeline: State: LISTENING_WAKE\n    Pipeline-&gt;&gt;Audio: start_recording()\n    Pipeline-&gt;&gt;WakeWord: start()\n\n    loop Listen for wake word\n        Audio-&gt;&gt;WakeWord: process_chunk(audio_16khz)\n        WakeWord--&gt;&gt;Pipeline: on_wake(\"hey_motoko\")\n    end\n\n    Note over Pipeline: State: WAKE_DETECTED\n    Pipeline-&gt;&gt;Motion: pause_idle_behavior()\n    Pipeline-&gt;&gt;Pipeline: mark pending persona switch\n\n    Note over Pipeline: State: LISTENING_SPEECH\n    Pipeline-&gt;&gt;OpenAI: connect() [manual VAD mode]\n    Pipeline-&gt;&gt;VAD: start()\n\n    loop Buffer user speech\n        Audio-&gt;&gt;Pipeline: read_chunk() [16kHz]\n        Pipeline-&gt;&gt;VAD: process_chunk()\n        VAD--&gt;&gt;Pipeline: is_speech=True\n        Pipeline-&gt;&gt;Pipeline: buffer_chunk()\n        Pipeline-&gt;&gt;OpenAI: send_audio(resampled_24khz)\n    end\n\n    VAD--&gt;&gt;Pipeline: on_speech_end(duration=3.5s)\n    Note over Pipeline: Silence threshold reached (800ms)\n\n    Note over Pipeline: State: PROCESSING\n    Pipeline-&gt;&gt;OpenAI: commit_audio_buffer()\n    OpenAI--&gt;&gt;Pipeline: transcript=\"What time is it?\"\n\n    Pipeline-&gt;&gt;Agent: process_input(\"What time is it?\")\n    Agent-&gt;&gt;Agent: Run Claude SDK loop\n    Agent--&gt;&gt;Pipeline: response=\"It's 2:30 PM\"\n\n    Note over Pipeline: State: SPEAKING\n    Pipeline-&gt;&gt;OpenAI: request_tts(\"It's 2:30 PM\", voice=\"nova\")\n\n    loop Stream TTS audio\n        OpenAI--&gt;&gt;Pipeline: audio_chunk [24kHz]\n        Pipeline-&gt;&gt;Audio: play_chunk(resampled_16khz)\n        OpenAI--&gt;&gt;Pipeline: amplitude=0.8\n        Pipeline-&gt;&gt;Motion: wobble.update_amplitude(0.8)\n    end\n\n    Audio--&gt;&gt;Pipeline: playback_complete()\n    Pipeline-&gt;&gt;Pipeline: apply_pending_persona_switch()\n    Pipeline-&gt;&gt;OpenAI: reconnect(voice=\"nova\")\n\n    Note over Pipeline: State: LISTENING_WAKE\n    Pipeline-&gt;&gt;Motion: resume_idle_behavior()</code></pre>"},{"location":"architecture/voice-pipeline/#key-dataflow-insights","title":"Key Dataflow Insights","text":"<ol> <li>Audio Resampling Happens at Pipeline Boundaries</li> <li>Microphone \u2192 VAD: 16kHz (Silero's native rate)</li> <li>Pipeline \u2192 OpenAI: 24kHz (upsampled via <code>resample_to_24khz()</code>)</li> <li> <p>OpenAI \u2192 Speaker: 24kHz \u2192 16kHz (downsampled)</p> </li> <li> <p>Buffering Strategy</p> </li> <li>During LISTENING_SPEECH: Audio chunks buffered in <code>_audio_buffer</code></li> <li>On end-of-speech: Entire buffer committed to OpenAI with <code>commit_audio_buffer()</code></li> <li> <p>After commit: Buffer cleared, ready for next turn</p> </li> <li> <p>Amplitude-Driven Motion</p> </li> <li>OpenAI Realtime emits audio amplitude with each TTS chunk</li> <li>Pipeline forwards to <code>HeadWobble.update_amplitude()</code></li> <li> <p>HeadWobble modulates head pitch/yaw based on amplitude (appears to \"speak\")</p> </li> <li> <p>Persona Switching Sequence</p> </li> <li>Wake word detected \u2192 Mark pending switch (don't apply yet)</li> <li>Current response completes \u2192 Apply switch</li> <li>Reconnect WebSocket with new voice parameter</li> <li>Next turn uses new persona</li> </ol>"},{"location":"architecture/voice-pipeline/#integration-points","title":"Integration Points","text":""},{"location":"architecture/voice-pipeline/#agent-integration-process_input-for-claude-responses","title":"Agent Integration: <code>process_input()</code> for Claude Responses","text":"<p>The Voice Pipeline delegates all conversational AI to the <code>ReachyAgentLoop</code> via the <code>process_input()</code> method:</p> <pre><code># In pipeline.py (PROCESSING state)\nasync def _process_speech(self) -&gt; None:\n    \"\"\"Process transcribed speech through Claude agent.\"\"\"\n\n    if self.agent:\n        response_text = await self.agent.process_input(\n            self._current_transcript,\n            context=AgentContext(modality=\"voice\")\n        )\n    else:\n        # Fallback: Echo the transcript\n        response_text = f\"I heard you say: {self._current_transcript}\"\n</code></pre> <p>Why this design: - Separation of concerns: Voice Pipeline handles audio I/O, Agent handles reasoning - Consistent interface: Text-based chat and voice both use <code>process_input()</code> - Context propagation: Agent can adjust behavior based on <code>modality=\"voice\"</code> (e.g., shorter responses)</p> <p>Data exchange: - Input: <code>(transcript: str, context: AgentContext)</code> - Recognized speech + metadata - Output: <code>response_text: str</code> - Agent's response (may include tool call results)</p> <p>Error handling: - Agent errors (e.g., Claude API timeout) raise exceptions caught by Pipeline - Pipeline transitions to ERROR state and applies recovery strategy - If agent is unavailable (offline mode), fallback to echo or canned responses</p>"},{"location":"architecture/voice-pipeline/#motion-integration-headwobble-during-tts","title":"Motion Integration: HeadWobble During TTS","text":"<p>The HeadWobble behavior synchronizes head motion with TTS audio amplitude, creating the illusion of speech:</p> <pre><code># In pipeline.py (SPEAKING state)\ndef _on_audio_amplitude(self, amplitude: float) -&gt; None:\n    \"\"\"Callback for TTS audio amplitude (for HeadWobble).\"\"\"\n    if self.on_audio_amplitude:\n        self.on_audio_amplitude(amplitude)\n\n    # Also update agent's wobble if available\n    if self.agent and hasattr(self.agent, \"_wobble\"):\n        self.agent._wobble.update_amplitude(amplitude)\n</code></pre> <p>Data flow: 1. OpenAI Realtime emits audio chunks with amplitude metadata 2. Pipeline receives amplitude in <code>_on_audio_amplitude()</code> callback 3. Pipeline forwards to <code>HeadWobble.update_amplitude()</code> 4. HeadWobble modulates head pitch/yaw in real-time (100Hz control loop)</p> <p>Key insight: Amplitude represents the \"loudness\" of the current audio frame. HeadWobble uses this to create natural-looking head motion (bigger wobbles for louder syllables).</p> <p>Trade-offs: - Pro: Very natural-looking speech animation (humans move heads when speaking) - Con: Tight coupling between voice and motion (motion failures can break voice) - Mitigation: Motion errors logged but don't propagate to voice pipeline</p>"},{"location":"architecture/voice-pipeline/#motion-integration-idle-behavior-pauseresume","title":"Motion Integration: Idle Behavior Pause/Resume","text":"<p>The idle behavior (look-around animation) must pause during conversation to avoid conflicting motions:</p> <pre><code># In pipeline.py\n\nasync def _handle_wake_detected(self) -&gt; None:\n    \"\"\"Handle wake word detection.\"\"\"\n    # Pause idle behavior when entering conversation\n    if self.agent and hasattr(self.agent, \"_blend_controller\"):\n        await self.agent._blend_controller.pause_idle_behavior()\n\nasync def _resume_idle_behavior(self) -&gt; None:\n    \"\"\"Resume idle behavior when returning to wake word listening.\"\"\"\n    if self.agent and hasattr(self.agent, \"_blend_controller\"):\n        await self.agent._blend_controller.resume_idle_behavior()\n</code></pre> <p>When paused: - WAKE_DETECTED state (wake word heard, waiting for speech) - LISTENING_SPEECH state (user speaking) - PROCESSING state (waiting for Claude response) - SPEAKING state (TTS playback)</p> <p>When resumed: - LISTENING_WAKE state (back to wake word detection) - ERROR state (after recovery completes)</p> <p>Why this matters: Without pausing, the idle behavior would fight with HeadWobble for motor control, creating jerky, unnatural motion. The <code>BlendController</code> arbitrates between motion sources, but explicit pause/resume is cleaner.</p>"},{"location":"architecture/voice-pipeline/#persona-integration-deferred-switching-after-tts","title":"Persona Integration: Deferred Switching After TTS","text":"<p>Persona switches happen in two phases:</p> <p>Phase 1: Wake Word Detection (Mark Pending) <pre><code>def _on_wake_word_detected(self, detected_model: str) -&gt; None:\n    \"\"\"Map wake word model to persona and mark for deferred switch.\"\"\"\n    persona = self._wake_word_to_persona.get(detected_model)\n    if persona and persona.name != self._current_persona.name:\n        self._pending_persona_switch = persona\n        logger.info(\"persona_switch_pending\", new_persona=persona.name)\n</code></pre></p> <p>Phase 2: After TTS Playback (Apply Switch) <pre><code>async def _play_response(self) -&gt; None:\n    \"\"\"Play TTS response, then apply pending persona switch.\"\"\"\n    # ... play audio chunks ...\n\n    if self._pending_persona_switch:\n        await self._apply_persona_switch(self._pending_persona_switch)\n        self._pending_persona_switch = None\n\nasync def _apply_persona_switch(self, persona: PersonaConfig) -&gt; None:\n    \"\"\"Reconnect WebSocket with new voice and update current persona.\"\"\"\n    self._current_persona = persona\n\n    if self._openai:\n        await self._openai.disconnect()\n        await self._openai.connect(voice=persona.voice)\n</code></pre></p> <p>Why deferred: OpenAI Realtime sessions are voice-scoped. Switching voices mid-session requires disconnecting and reconnecting the WebSocket, which would interrupt TTS playback.</p> <p>Agent notification: After switching, pipeline can optionally notify agent to reload system prompt: <pre><code>if self.agent:\n    await self.agent.load_persona_prompt(persona.prompt_path)\n</code></pre></p>"},{"location":"architecture/voice-pipeline/#error-handling-recovery","title":"Error Handling &amp; Recovery","text":"<p>The Voice Pipeline implements a three-tier recovery strategy:</p>"},{"location":"architecture/voice-pipeline/#tier-1-retry-with-exponential-backoff","title":"Tier 1: Retry with Exponential Backoff","text":"<p>For transient errors (network glitches, temporary hardware issues):</p> <pre><code>@dataclass\nclass RecoveryStrategy:\n    max_retries: int = 3\n    initial_delay_seconds: float = 1.0\n    backoff_factor: float = 2.0  # 1s \u2192 2s \u2192 4s\n    max_delay_seconds: float = 30.0\n</code></pre> <p>Applied to: - OpenAI WebSocket connection failures - Audio device initialization errors - Temporary daemon communication failures</p> <p>Example flow: 1. Attempt 1 fails \u2192 Wait 1 second 2. Attempt 2 fails \u2192 Wait 2 seconds 3. Attempt 3 fails \u2192 Wait 4 seconds 4. Max retries exceeded \u2192 Move to Tier 2</p>"},{"location":"architecture/voice-pipeline/#tier-2-degraded-mode-operation","title":"Tier 2: Degraded Mode Operation","text":"<p>When a component repeatedly fails, gracefully degrade functionality:</p> Component Failure Degraded Mode Behavior Wake Word Detector Switch to always-listening mode (skip wake word) Silero VAD Fall back to energy-based VAD (simple amplitude threshold) OpenAI Realtime Fall back to batch STT/TTS (higher latency) TTS Playback Log response text to console (text-only mode) <p>Configuration (in <code>config/default.yaml</code>): <pre><code>degraded_mode:\n  skip_wake_word_on_failure: true\n  use_energy_vad_fallback: true\n  log_response_on_tts_failure: true\n</code></pre></p> <p>Degraded mode tracking: <pre><code># In pipeline.py\nif self._recovery.is_degraded(\"wake_word\"):\n    # Skip wake word, go directly to LISTENING_SPEECH\n    self._set_state(VoicePipelineState.LISTENING_SPEECH)\n</code></pre></p>"},{"location":"architecture/voice-pipeline/#tier-3-abort-and-notify","title":"Tier 3: Abort and Notify","text":"<p>If degraded mode still fails, abort the current operation and notify the user:</p> <pre><code>async def _handle_error(self) -&gt; None:\n    \"\"\"Handle ERROR state with recovery or abort.\"\"\"\n\n    # Apply recovery strategy\n    action = await self._recovery.recover(self._last_error)\n\n    if action == RecoveryAction.RETRY:\n        # Retry the failed state\n        self._set_state(self._last_good_state)\n\n    elif action == RecoveryAction.DEGRADE:\n        # Enable degraded mode\n        self._recovery.mark_degraded(component_name)\n        self._restart_listening()\n\n    elif action == RecoveryAction.ABORT:\n        # Stop pipeline, notify user\n        logger.error(\"voice_pipeline_aborted\", error=self._last_error)\n        self._set_state(VoicePipelineState.IDLE)\n        if self.on_error:\n            self.on_error(self._last_error)\n</code></pre> <p>User notification strategies: - LED indicator: Antennas turn red during ERROR state - Audible beep: Three short beeps indicate critical failure - Log message: Console shows detailed error for debugging - Agent message: If agent is available, say \"I'm having trouble with my microphone\"</p>"},{"location":"architecture/voice-pipeline/#timeout-guards-preventing-stuck-states","title":"Timeout Guards: Preventing Stuck States","text":"<p>Every state with a timeout has a background task that forces recovery:</p> <pre><code>async def _state_timeout_guard(\n    self, expected_state: VoicePipelineState, timeout: float\n) -&gt; None:\n    \"\"\"Force ERROR transition if state exceeds timeout.\"\"\"\n\n    await asyncio.sleep(timeout)\n\n    if self._state == expected_state:\n        logger.error(\n            \"state_timeout\",\n            state=expected_state.value,\n            timeout=timeout,\n            duration=time.monotonic() - self._state_entered_at,\n        )\n        self._last_error = VoicePipelineError(f\"Timeout in {expected_state.value}\")\n        self._set_state(VoicePipelineState.ERROR)\n</code></pre> <p>Timeout values (from <code>STATE_TIMEOUTS</code>): - WAKE_DETECTED: 2s (should quickly transition to LISTENING_SPEECH) - LISTENING_SPEECH: 35s (max speech duration + buffer) - PROCESSING: 90s (Claude response, including tool calls) - SPEAKING: 180s (3 minutes max for long philosophical responses) - ERROR: 10s (brief pause before retry)</p> <p>Why needed: Without timeouts, a hung WebSocket or infinite VAD loop would freeze the pipeline indefinitely. Timeouts provide automatic recovery.</p>"},{"location":"architecture/voice-pipeline/#health-monitoring-stream-disconnection-detection","title":"Health Monitoring: Stream Disconnection Detection","text":"<p>The AudioManager continuously monitors stream health:</p> <pre><code>async def _health_check_loop(self) -&gt; None:\n    \"\"\"Background task that monitors stream health.\"\"\"\n\n    while self._is_recording:\n        await asyncio.sleep(self.config.health_check_interval_seconds)\n\n        # Check for stale data (no reads in last 2 intervals)\n        time_since_read = time.monotonic() - self._last_successful_read\n        if time_since_read &gt; 2 * self.config.health_check_interval_seconds:\n            logger.warning(\n                \"audio_stream_stale\",\n                time_since_read=time_since_read,\n            )\n            if self.on_device_error:\n                self.on_device_error(AudioDeviceError(\"Stream appears disconnected\"))\n</code></pre> <p>Detected failures: - Microphone unplugged (read returns empty data) - Speaker disconnected (playback raises IOError) - ALSA device released by another process</p> <p>Recovery: Pipeline receives <code>on_device_error()</code> callback, transitions to ERROR state, attempts audio reinitialization.</p>"},{"location":"architecture/voice-pipeline/#configuration-reference","title":"Configuration Reference","text":"<p>All voice settings are in <code>config/default.yaml</code> under the <code>voice:</code> section:</p>"},{"location":"architecture/voice-pipeline/#top-level-settings","title":"Top-Level Settings","text":"<pre><code>voice:\n  enabled: false                 # Enable with --voice flag\n  default_persona: hey_motoko    # Persona to use before wake word detected\n  confirmation_beep: true        # Play sound when wake word detected\n  auto_restart: true             # Restart listening after response\n</code></pre>"},{"location":"architecture/voice-pipeline/#personas-multi-wake-word","title":"Personas (Multi-Wake Word)","text":"<pre><code>personas:\n  hey_motoko:\n    name: motoko\n    display_name: Major Kusanagi\n    voice: nova                # OpenAI TTS voice (female, analytical)\n    prompt_path: prompts/personas/motoko.md\n\n  hey_batou:\n    name: batou\n    display_name: Batou\n    voice: onyx                # OpenAI TTS voice (male, casual)\n    prompt_path: prompts/personas/batou.md\n</code></pre> <p>Valid TTS voices: <code>alloy</code>, <code>echo</code>, <code>fable</code>, <code>onyx</code>, <code>nova</code>, <code>shimmer</code></p>"},{"location":"architecture/voice-pipeline/#wake-word-detection","title":"Wake Word Detection","text":"<pre><code>wake_word:\n  enabled: true\n  model: hey_jarvis            # Fallback if no persona models found\n  sensitivity: 0.5             # 0.0 (strict) to 1.0 (lenient)\n  cooldown_seconds: 2.0        # Ignore detections for this period\n  custom_models_dir: data/wake_words  # .onnx model directory\n</code></pre> <p>Model sources: 1. Custom models in <code>data/wake_words/*.onnx</code> (persona-specific) 2. Bundled OpenWakeWord models (hey_jarvis, alexa, hey_mycroft)</p>"},{"location":"architecture/voice-pipeline/#voice-activity-detection-vad","title":"Voice Activity Detection (VAD)","text":"<pre><code>vad:\n  silence_threshold_ms: 800    # Silence duration to trigger end-of-speech\n  min_speech_duration_ms: 250  # Minimum speech to be valid (filter coughs)\n  max_speech_duration_s: 30.0  # Maximum before timeout (prevents infinite recording)\n  speech_threshold: 0.5        # VAD sensitivity (0.0-1.0, higher = more sensitive)\n</code></pre> <p>Tuning guide: - silence_threshold_ms: Lower = more responsive (cuts off sooner), higher = less interruption - speech_threshold: Lower = requires clearer speech, higher = more sensitive to noise</p>"},{"location":"architecture/voice-pipeline/#openai-realtime-api","title":"OpenAI Realtime API","text":"<pre><code>openai:\n  model: gpt-realtime-mini     # Cost-efficient, low latency (~1 second)\n  voice: alloy                 # Default voice (overridden by persona)\n  sample_rate: 24000           # OpenAI uses 24kHz (don't change)\n  temperature: 0.8             # Response creativity (0.0-1.0)\n  max_response_tokens: 4096    # Max response length\n  turn_detection_threshold: 0.5    # Unused (manual VAD mode)\n  turn_detection_silence_ms: 500   # Unused (manual VAD mode)\n</code></pre> <p>Model options: - <code>gpt-realtime-mini</code>: $0.06/min input, $0.24/min output (recommended) - <code>gpt-4-realtime</code>: Higher quality, higher cost</p>"},{"location":"architecture/voice-pipeline/#audio-hardware","title":"Audio Hardware","text":"<pre><code>audio:\n  sample_rate: 16000           # Microphone sample rate (Silero VAD requirement)\n  channels: 1                  # Mono audio\n  chunk_size: 512              # Samples per chunk (Silero requirement)\n  format_bits: 16              # int16 PCM\n\n  # ALSA device indices (Reachy Mini)\n  input_device_index: 4        # dsnoop (shared input)\n  output_device_index: 3       # dmix (shared output)\n\n  # Resilience\n  max_init_retries: 3          # Retry audio init this many times\n  retry_delay_seconds: 1.0     # Initial delay between retries\n  retry_backoff_factor: 2.0    # Multiply delay by this each retry\n\n  # Buffer settings\n  output_lead_in_ms: 200       # Silence before TTS (prevents speaker click)\n  input_warmup_chunks: 5       # Discard first N chunks (mic settling time)\n\n  # Health monitoring\n  health_check_interval_seconds: 5.0   # Check stream health every N seconds\n  max_consecutive_errors: 3            # Trigger recovery after N consecutive errors\n</code></pre> <p>Device index discovery: <pre><code># List audio devices\npython -c \"import pyaudio; p = pyaudio.PyAudio(); [print(f'{i}: {p.get_device_info_by_index(i)[\\\"name\\\"]}') for i in range(p.get_device_count())]\"\n</code></pre></p>"},{"location":"architecture/voice-pipeline/#degraded-mode","title":"Degraded Mode","text":"<pre><code>degraded_mode:\n  skip_wake_word_on_failure: true      # Switch to always-listening if wake word fails\n  use_energy_vad_fallback: true        # Use energy-based VAD if Silero unavailable\n  log_response_on_tts_failure: true    # Log response text if TTS fails\n</code></pre> <p>When degraded modes activate: - Wake word: After 3 consecutive load failures - VAD: If Silero model fails to load (torch.hub error) - TTS: If OpenAI connection fails and local TTS unavailable</p>"},{"location":"architecture/voice-pipeline/#common-misconceptions","title":"Common Misconceptions","text":""},{"location":"architecture/voice-pipeline/#misconception-openai-realtime-is-just-stt-tts","title":"Misconception: OpenAI Realtime is just STT + TTS","text":"<p>Reality: OpenAI Realtime is a bidirectional streaming protocol that combines: - Speech-to-text transcription - Text-to-speech synthesis - Conversation session management (context, turn detection) - Audio amplitude metadata (for visualization/animation)</p> <p>Why the confusion: Previous integrations used separate <code>/v1/audio/transcriptions</code> and <code>/v1/audio/speech</code> endpoints. Realtime unifies these into one WebSocket connection.</p> <p>Implication: Cannot mix Realtime with other TTS providers without breaking session continuity. To switch TTS providers, would need to refactor to batch mode.</p>"},{"location":"architecture/voice-pipeline/#misconception-vad-only-detects-speech-vs-silence","title":"Misconception: VAD only detects \"speech vs. silence\"","text":"<p>Reality: Silero VAD is a PyTorch model that classifies audio as: - Speech: Human voice (any language) - Noise: Non-voice sounds (music, typing, background chatter) - Silence: Near-zero amplitude</p> <p>Why the confusion: Simple energy-based VAD (amplitude threshold) only detects silence. Silero is more sophisticated\u2014it can ignore background music while detecting overlaid speech.</p> <p>Implication: The energy fallback (<code>use_energy_vad_fallback: true</code>) is significantly less accurate in noisy environments. Silero should be preferred when available.</p>"},{"location":"architecture/voice-pipeline/#misconception-persona-switching-changes-the-ai-model","title":"Misconception: Persona switching changes the AI model","text":"<p>Reality: Persona switching only changes: - OpenAI TTS voice parameter (<code>nova</code> vs. <code>onyx</code>) - System prompt loaded into Claude Agent - Display name shown in UI</p> <p>The underlying models remain the same: - STT/TTS: <code>gpt-realtime-mini</code> (OpenAI) - Reasoning: <code>claude-sonnet-4-5</code> (Anthropic, via Agent SDK)</p> <p>Why the confusion: The term \"persona\" implies a different AI. In reality, it's more like \"alternate personalities\" of the same AI.</p> <p>Implication: Cannot have radically different capabilities per persona (e.g., one persona with tool access, another without). Capabilities are determined by the Agent's tool set, not the persona.</p>"},{"location":"architecture/voice-pipeline/#misconception-state-timeouts-are-error-conditions","title":"Misconception: State timeouts are error conditions","text":"<p>Reality: Timeouts are safety guards, not errors. They prevent infinite loops but don't necessarily indicate a problem.</p> <p>Example: PROCESSING state has a 90-second timeout. If Claude is running a complex tool (e.g., searching emails), it might take 60 seconds\u2014this is normal, not an error.</p> <p>Why the confusion: Timeout logs appear as errors (<code>state_timeout</code>), but they're informational.</p> <p>Implication: When debugging, check <code>duration=</code> in timeout logs. If it's close to the timeout value, the guard is working correctly. If it's significantly less, investigate why the state is hanging.</p>"},{"location":"architecture/voice-pipeline/#implications-for-practice","title":"Implications for Practice","text":""},{"location":"architecture/voice-pipeline/#when-working-with-the-voice-pipeline","title":"When Working with the Voice Pipeline","text":"<p>Understanding these concepts means:</p> <ul> <li> <p>State transitions are strict: Don't bypass <code>_set_state()</code>\u2014it validates transitions, manages timeouts, and prevents race conditions. If you need a new transition, add it to <code>VALID_TRANSITIONS</code>.</p> </li> <li> <p>Errors are multi-level: Don't assume retry logic is sufficient. Consider degraded mode behavior for new features (e.g., if adding local TTS, define fallback when it fails).</p> </li> <li> <p>Audio timing is critical: The 512-sample chunk size at 16kHz is not arbitrary\u2014it's required by Silero VAD. Changing it breaks VAD. If you need different processing windows, resample after VAD.</p> </li> <li> <p>Persona switches have latency: The one-turn delay is intentional. If you need instant switching, you'll need to redesign around batch STT/TTS (separate API calls per turn).</p> </li> <li> <p>Motion integration is bidirectional: Voice Pipeline pauses idle behavior, but motion failures can propagate back (e.g., if HeadWobble hangs, TTS might block). Always use timeouts in motion code.</p> </li> </ul>"},{"location":"architecture/voice-pipeline/#design-patterns-that-emerge","title":"Design Patterns That Emerge","text":"<p>Based on these principles, you'll often see:</p> <p>Async Context Managers for Lifecycle: <pre><code>async with voice_pipeline.run():\n    # Pipeline automatically starts/stops\n    await asyncio.Event().wait()\n</code></pre></p> <p>Callback Registration for Events: <pre><code>pipeline.on_wake_word = lambda model: print(f\"Detected: {model}\")\npipeline.on_error = lambda err: log_to_sentry(err)\n</code></pre></p> <p>Retry with Exponential Backoff: <pre><code>for attempt in range(max_retries):\n    try:\n        await connect()\n        break\n    except ConnectionError:\n        await asyncio.sleep(delay * (backoff ** attempt))\n</code></pre></p> <p>State-Dependent Behavior: <pre><code>if pipeline.state == VoicePipelineState.LISTENING_SPEECH:\n    # Show listening indicator in UI\nelif pipeline.state == VoicePipelineState.SPEAKING:\n    # Show speaking indicator\n</code></pre></p>"},{"location":"architecture/voice-pipeline/#connecting-to-broader-concepts","title":"Connecting to Broader Concepts","text":""},{"location":"architecture/voice-pipeline/#relationship-to-the-agent-sdk","title":"Relationship to the Agent SDK","text":"<p>The Voice Pipeline is a perception modality for the Claude Agent SDK. It translates audio input into text, feeds it to the agent's <code>process_input()</code> method, and renders the agent's text output as audio.</p> <p>Key insight: The agent is modality-agnostic. It receives text (from voice, chat, or webhooks) and produces text (rendered as TTS, chat messages, or API responses). The Voice Pipeline is just one adapter.</p> <p>Implications: - Voice features (wake words, personas) don't require agent changes - Agent improvements (better reasoning, new tools) automatically benefit voice - Could add video input using the same pattern (vision \u2192 text \u2192 agent \u2192 TTS)</p>"},{"location":"architecture/voice-pipeline/#industry-patterns-real-time-streaming-ai","title":"Industry Patterns: Real-Time Streaming AI","text":"<p>The Voice Pipeline follows emerging patterns in conversational AI:</p> <p>WebSocket-Based Streaming: - Replaces traditional request/response with bidirectional streams - Examples: OpenAI Realtime, Deepgram Streaming STT, ElevenLabs Streaming TTS - Benefit: Lower latency (no wait for full audio before processing)</p> <p>Client-Side VAD for Turn Detection: - Alternative to server-side VAD (e.g., GPT-4 Realtime's built-in turn detection) - Benefit: More control over when turns end, better for multi-persona systems - Cost: Additional CPU overhead on client</p> <p>Degraded Mode Operation: - Inspired by \"circuit breaker\" pattern in microservices - Benefit: System remains partially functional when components fail - Example: Google Assistant falls back to cloud STT when on-device fails</p>"},{"location":"architecture/voice-pipeline/#future-directions","title":"Future Directions","text":"<p>Where this architecture might evolve:</p> <p>Local STT/TTS Models: - On-device Whisper (STT) and Piper (TTS) for offline operation - Challenge: Raspberry Pi 4 has limited compute for real-time inference - Potential: Coral Edge TPU for hardware acceleration</p> <p>Multi-Modal Input: - Add vision (camera) as input modality alongside voice - Example: \"What's in my hand?\" while holding object to camera - Requires: Multimodal agent SDK support (images \u2192 Claude)</p> <p>Conversation Memory: - Store conversation history in semantic memory (ChromaDB) - Enable: \"What did I ask you yesterday?\" - Challenge: Privacy (requires consent), storage limits</p> <p>Emotion Detection: - Analyze voice tone/pitch for emotional state - Adjust persona responses based on user mood - Example: More empathetic responses when user sounds stressed</p>"},{"location":"architecture/voice-pipeline/#deep-dive-topics","title":"Deep Dive Topics","text":"<p>For those wanting even deeper understanding:</p> <ul> <li> <p>ALSA Architecture on Raspberry Pi: How dsnoop/dmix plugins multiplex hardware access, kernel-level audio routing, ALSA configuration files (<code>/etc/asound.conf</code>)</p> </li> <li> <p>Silero VAD Internals: PyTorch model architecture (LSTM + attention), training data (multilingual speech datasets), ONNX export for edge deployment</p> </li> <li> <p>OpenAI Realtime Protocol: WebSocket message format (JSON events), session lifecycle (session.created \u2192 input_audio_buffer.append \u2192 response.audio.delta), manual vs. server VAD modes</p> </li> <li> <p>State Machine Theory: Finite state machines vs. statecharts, UML state diagrams, transition validation, timeout guards as a form of timed automata</p> </li> </ul>"},{"location":"architecture/voice-pipeline/#summary-the-mental-model","title":"Summary: The Mental Model","text":"<p>After understanding all of this, think of the Voice Pipeline as:</p> <p>A resilient state machine that coordinates audio I/O, wake word detection, speech recognition, AI reasoning, and speech synthesis into a natural conversational flow\u2014while gracefully degrading when components fail.</p> <p>Key insights to remember:</p> <ol> <li> <p>State is explicit, transitions are guarded: Every state change is validated and logged. Timeouts prevent stuck states.</p> </li> <li> <p>Manual VAD enables persona switching: Client-side control over audio commit timing allows changing OpenAI voice between turns.</p> </li> <li> <p>Motion integration is first-class: HeadWobble and idle behaviors are not afterthoughts\u2014they're essential for natural interaction.</p> </li> <li> <p>Degraded modes preserve UX: When components fail, the system falls back to simpler modes (energy VAD, batch STT/TTS) rather than crashing.</p> </li> <li> <p>ALSA shared devices enable concurrency: dsnoop/dmix allow daemon and voice pipeline to share audio hardware without conflicts.</p> </li> </ol>"},{"location":"architecture/voice-pipeline/#further-exploration","title":"Further Exploration","text":"<ul> <li>To implement voice features: See How-to Guide: Adding a New Persona</li> <li>For API specifications: Check Voice Pipeline API Reference</li> <li>To understand agent integration: Read Agent Architecture</li> <li>Academic papers:</li> <li>Silero VAD: Lightweight Speech Detection</li> <li>OpenAI Realtime API Design</li> <li>Related projects:</li> <li>OpenWakeWord - Custom wake word models</li> <li>Porcupine - Commercial wake word alternative</li> <li>Whisper - OpenAI's open-source STT model</li> </ul>"},{"location":"guides/phase2-preparation/","title":"Phase 2 Preparation Guide","text":"<p>This guide prepares you for Phase 2: Hardware Integration. Complete these steps before assembling your Reachy Mini to ensure a smooth transition from simulation to physical hardware.</p>"},{"location":"guides/phase2-preparation/#phase-2-overview","title":"Phase 2 Overview","text":"<pre><code>gantt\n    title Phase 2: Hardware Integration\n    dateFormat X\n    axisFormat %s\n\n    section Pi Setup\n    Install Raspberry Pi OS       :a1, 0, 1\n    Install Reachy Daemon         :a2, after a1, 2\n    Install Claude Code CLI       :a3, after a1, 2\n    Install Python Dependencies   :a4, after a3, 2\n    Configure systemd             :a5, after a4, 1\n\n    section Wake Word\n    Install OpenWakeWord          :b1, after a4, 2\n    Audio Capture Module          :b2, after b1, 3\n    Wake Word Detector            :b3, after b2, 3\n    Sensitivity Tuning            :b4, after b3, 2\n\n    section Attention\n    State Machine Model           :c1, after a5, 2\n    Passive Mode                  :c2, after c1, 2\n    Alert Mode                    :c3, after c2, 2\n    Engaged Mode                  :c4, after c3, 2\n\n    section Privacy\n    Antenna Indicators            :d1, after c4, 2\n    Audio Cues                    :d2, after d1, 2</code></pre>"},{"location":"guides/phase2-preparation/#pre-assembly-checklist","title":"Pre-Assembly Checklist","text":""},{"location":"guides/phase2-preparation/#hardware-requirements","title":"Hardware Requirements","text":"Component Specification Notes Reachy Mini Wireless version Includes Raspberry Pi 4 MicroSD Card 32GB+ Class 10 For Raspberry Pi OS USB-C Power 5V/3A For Raspberry Pi WiFi Network 2.4GHz or 5GHz For cloud connectivity"},{"location":"guides/phase2-preparation/#software-prerequisites-development-machine","title":"Software Prerequisites (Development Machine)","text":"<p>Verify these work in simulation before moving to hardware:</p> <pre><code># 1. Run full test suite\npytest -v\n# Expected: 113+ tests pass\n\n# 2. Run simulation validation\npython scripts/validate_simulation.py --headless\n# Expected: ALL TESTS PASSED\n\n# 3. Verify MCP server starts\npython -c \"from reachy_agent.mcp_servers.reachy import create_reachy_mcp_server; print('OK')\"\n# Expected: OK\n\n# 4. Verify GitHub MCP integration (optional)\npython -c \"from reachy_agent.mcp_servers.integrations import is_binary_available; print('Binary:', is_binary_available())\"\n# Expected: Binary: True (if installed) or False\n\n# 5. Verify agent loop\npython -c \"from reachy_agent.agent import ReachyAgentLoop; print('OK')\"\n# Expected: OK\n</code></pre>"},{"location":"guides/phase2-preparation/#environment-variables","title":"Environment Variables","text":"<p>Ensure <code>.env</code> is configured with your API keys:</p> <pre><code># Required for Phase 2\nANTHROPIC_API_KEY=sk-ant-api03-...  # Claude API\nREACHY_DAEMON_URL=http://localhost:8000  # Local daemon\n\n# Optional (Phase 3+)\nHA_URL=http://homeassistant.local:8123\nHA_TOKEN=eyJ...\nGITHUB_TOKEN=ghp_...\n</code></pre>"},{"location":"guides/phase2-preparation/#raspberry-pi-setup","title":"Raspberry Pi Setup","text":""},{"location":"guides/phase2-preparation/#step-1-install-raspberry-pi-os","title":"Step 1: Install Raspberry Pi OS","text":"<pre><code>flowchart LR\n    A[\"Download&lt;br/&gt;Pi Imager\"] --&gt; B[\"Flash&lt;br/&gt;64-bit OS\"]\n    B --&gt; C[\"Configure&lt;br/&gt;WiFi/SSH\"]\n    C --&gt; D[\"First Boot\"]\n    D --&gt; E[\"SSH In\"]</code></pre> <ol> <li>Download Raspberry Pi Imager</li> <li>Select Raspberry Pi OS (64-bit)</li> <li>Click gear icon for advanced options:</li> <li>Enable SSH</li> <li>Set hostname: <code>reachy-mini</code></li> <li>Configure WiFi credentials</li> <li>Set username/password</li> <li>Flash to MicroSD card</li> <li>Insert card into Pi and boot</li> </ol> <pre><code># SSH into Pi\nssh pi@reachy-mini.local\n\n# Update system\nsudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>"},{"location":"guides/phase2-preparation/#step-2-install-reachy-daemon","title":"Step 2: Install Reachy Daemon","text":"<p>The Reachy daemon comes pre-installed on Reachy Mini. Verify it's running:</p> <pre><code># Check daemon status\nsudo systemctl status reachy-daemon\n\n# If not running, start it\nsudo systemctl start reachy-daemon\nsudo systemctl enable reachy-daemon\n\n# Verify API is accessible\ncurl http://localhost:8000/api/daemon/status\n</code></pre> <p>Expected response: <pre><code>{\n  \"robot_name\": \"reachy_mini\",\n  \"state\": \"running\",\n  \"simulation_enabled\": false\n}\n</code></pre></p>"},{"location":"guides/phase2-preparation/#step-3-install-python-environment","title":"Step 3: Install Python Environment","text":"<pre><code># Install uv (fast package manager)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nsource ~/.bashrc\n\n# Clone repository\ngit clone https://github.com/jawhnycooke/reachy-agent.git\ncd reachy-agent\n\n# Create virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install dependencies\nuv pip install -r requirements.txt\n\n# Verify installation\npython -c \"import reachy_agent; print('OK')\"\n</code></pre>"},{"location":"guides/phase2-preparation/#step-35-install-github-mcp-optional","title":"Step 3.5: Install GitHub MCP (Optional)","text":"<p>For GitHub integration (repos, issues, PRs, actions):</p> <pre><code># Download ARM64 binary (only 4MB)\nmkdir -p ~/.reachy/bin\ncurl -sL https://github.com/github/github-mcp-server/releases/latest/download/github-mcp-server_Linux_arm64.tar.gz | tar xzf - -C ~/.reachy/bin\nchmod +x ~/.reachy/bin/github-mcp-server\n\n# Verify\n~/.reachy/bin/github-mcp-server --version\n\n# Add token to .env\necho \"GITHUB_TOKEN=ghp_your_token_here\" &gt;&gt; ~/.reachy/.env\n</code></pre> <p>The agent automatically detects the binary at <code>~/.reachy/bin/github-mcp-server</code> and uses it instead of Docker.</p>"},{"location":"guides/phase2-preparation/#step-4-configure-systemd-service","title":"Step 4: Configure systemd Service","text":"<p>Create the service file:</p> <pre><code>sudo nano /etc/systemd/system/reachy-agent.service\n</code></pre> <p>Content: <pre><code>[Unit]\nDescription=Reachy Agent - Claude-powered robot\nAfter=network.target reachy-daemon.service\nRequires=reachy-daemon.service\n\n[Service]\nType=simple\nUser=pi\nWorkingDirectory=/home/pi/reachy-agent\nEnvironment=PATH=/home/pi/reachy-agent/.venv/bin\nEnvironmentFile=/home/pi/reachy-agent/.env\nExecStart=/home/pi/reachy-agent/.venv/bin/python -m reachy_agent run\nRestart=on-failure\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>Enable and start: <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable reachy-agent\nsudo systemctl start reachy-agent\n\n# Check status\nsudo systemctl status reachy-agent\njournalctl -u reachy-agent -f\n</code></pre></p>"},{"location":"guides/phase2-preparation/#audio-system-setup","title":"Audio System Setup","text":""},{"location":"guides/phase2-preparation/#microphone-array","title":"Microphone Array","text":"<p>Reachy Mini has a 4-microphone array. Verify it's detected:</p> <pre><code># List audio devices\narecord -l\n\n# Expected output:\n# card 1: ReachyMini [Reachy Mini], device 0: USB Audio [USB Audio]\n#   Subdevices: 1/1\n\n# Test recording\narecord -D plughw:1,0 -f cd -d 5 test.wav\naplay test.wav\n</code></pre>"},{"location":"guides/phase2-preparation/#speaker","title":"Speaker","text":"<pre><code># List playback devices\naplay -l\n\n# Test speaker\nspeaker-test -D plughw:1,0 -c 2 -t wav\n</code></pre>"},{"location":"guides/phase2-preparation/#pyaudio-configuration","title":"PyAudio Configuration","text":"<pre><code># Verify PyAudio works\nimport pyaudio\n\np = pyaudio.PyAudio()\nfor i in range(p.get_device_count()):\n    info = p.get_device_info_by_index(i)\n    print(f\"{i}: {info['name']} (inputs: {info['maxInputChannels']})\")\n</code></pre>"},{"location":"guides/phase2-preparation/#wake-word-detection","title":"Wake Word Detection","text":""},{"location":"guides/phase2-preparation/#openwakeword-setup","title":"OpenWakeWord Setup","text":"<pre><code># Install OpenWakeWord\npip install openwakeword\n\n# Download models\npython -c \"import openwakeword; openwakeword.utils.download_models()\"\n</code></pre>"},{"location":"guides/phase2-preparation/#custom-wake-word","title":"Custom Wake Word","text":"<p>We'll use \"Hey Reachy\" as the wake phrase. OpenWakeWord supports custom training:</p> <pre><code>flowchart LR\n    A[\"Record&lt;br/&gt;50+ samples\"] --&gt; B[\"Train&lt;br/&gt;model\"]\n    B --&gt; C[\"Validate&lt;br/&gt;accuracy\"]\n    C --&gt; D[\"Deploy&lt;br/&gt;to Pi\"]</code></pre> <p>For now, use a similar built-in model as placeholder:</p> <pre><code>from openwakeword import Model\n\n# Load model (using \"hey jarvis\" as placeholder)\nmodel = Model(wakeword_models=[\"hey_jarvis_v0.1\"])\n\n# Test detection\nimport pyaudio\nimport numpy as np\n\np = pyaudio.PyAudio()\nstream = p.open(format=pyaudio.paInt16, channels=1, rate=16000,\n                input=True, frames_per_buffer=1280)\n\nwhile True:\n    audio = np.frombuffer(stream.read(1280), dtype=np.int16)\n    prediction = model.predict(audio)\n    if prediction[\"hey_jarvis_v0.1\"] &gt; 0.5:\n        print(\"Wake word detected!\")\n</code></pre>"},{"location":"guides/phase2-preparation/#attention-state-machine","title":"Attention State Machine","text":""},{"location":"guides/phase2-preparation/#state-diagram","title":"State Diagram","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Passive: Boot\n\n    state Passive {\n        [*] --&gt; ListeningForWake\n        ListeningForWake --&gt; ListeningForWake: No wake word\n    }\n\n    state Alert {\n        [*] --&gt; Tracking\n        Tracking --&gt; CheckingIn: 10s interval\n        CheckingIn --&gt; Tracking: No engagement\n    }\n\n    state Engaged {\n        [*] --&gt; Processing\n        Processing --&gt; Responding\n        Responding --&gt; Listening\n        Listening --&gt; Processing: User speaks\n    }\n\n    Passive --&gt; Alert: Motion detected\n    Passive --&gt; Engaged: Wake word\n\n    Alert --&gt; Passive: 30s timeout\n    Alert --&gt; Engaged: Wake word\n    Alert --&gt; Engaged: Face detected\n\n    Engaged --&gt; Alert: 60s silence</code></pre>"},{"location":"guides/phase2-preparation/#implementation-skeleton","title":"Implementation Skeleton","text":"<pre><code># src/reachy_agent/attention/states.py\n\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Callable\nimport asyncio\n\nclass AttentionState(Enum):\n    PASSIVE = auto()   # Minimal CPU, wake word only\n    ALERT = auto()     # Motion tracking, periodic check-ins\n    ENGAGED = auto()   # Full agent loop active\n\n@dataclass\nclass AttentionConfig:\n    alert_timeout: float = 30.0      # Seconds before Alert \u2192 Passive\n    engaged_timeout: float = 60.0    # Seconds before Engaged \u2192 Alert\n    checkin_interval: float = 10.0   # Seconds between Alert check-ins\n\nclass AttentionStateMachine:\n    def __init__(\n        self,\n        config: AttentionConfig,\n        on_state_change: Callable[[AttentionState], None] | None = None,\n    ):\n        self.config = config\n        self.state = AttentionState.PASSIVE\n        self.on_state_change = on_state_change\n        self._timeout_task: asyncio.Task | None = None\n\n    async def transition(self, new_state: AttentionState) -&gt; None:\n        if new_state == self.state:\n            return\n\n        old_state = self.state\n        self.state = new_state\n\n        # Cancel existing timeout\n        if self._timeout_task:\n            self._timeout_task.cancel()\n\n        # Set new timeout based on state\n        if new_state == AttentionState.ALERT:\n            self._timeout_task = asyncio.create_task(\n                self._timeout(self.config.alert_timeout, AttentionState.PASSIVE)\n            )\n        elif new_state == AttentionState.ENGAGED:\n            self._timeout_task = asyncio.create_task(\n                self._timeout(self.config.engaged_timeout, AttentionState.ALERT)\n            )\n\n        if self.on_state_change:\n            self.on_state_change(new_state)\n\n    async def _timeout(self, seconds: float, target: AttentionState) -&gt; None:\n        await asyncio.sleep(seconds)\n        await self.transition(target)\n\n    # Event handlers\n    async def on_wake_word(self) -&gt; None:\n        await self.transition(AttentionState.ENGAGED)\n\n    async def on_motion_detected(self) -&gt; None:\n        if self.state == AttentionState.PASSIVE:\n            await self.transition(AttentionState.ALERT)\n\n    async def on_face_detected(self) -&gt; None:\n        await self.transition(AttentionState.ENGAGED)\n\n    async def on_user_speaking(self) -&gt; None:\n        if self.state == AttentionState.ENGAGED:\n            # Reset timeout\n            await self.transition(AttentionState.ENGAGED)\n</code></pre>"},{"location":"guides/phase2-preparation/#privacy-indicators","title":"Privacy Indicators","text":""},{"location":"guides/phase2-preparation/#antenna-states","title":"Antenna States","text":"<pre><code>flowchart LR\n    subgraph Passive[\"Passive State\"]\n        A1[\"Antennas Down&lt;br/&gt;(0\u00b0)\"]\n    end\n\n    subgraph Alert[\"Alert State\"]\n        A2[\"Antennas Mid&lt;br/&gt;(45\u00b0)\"]\n    end\n\n    subgraph Engaged[\"Engaged State\"]\n        A3[\"Antennas Up&lt;br/&gt;(90\u00b0)\"]\n    end\n\n    subgraph Listening[\"Actively Listening\"]\n        A4[\"Antennas Wiggle&lt;br/&gt;(subtle movement)\"]\n    end\n\n    Passive --&gt;|Motion| Alert\n    Alert --&gt;|Wake word| Engaged\n    Engaged --&gt;|Recording| Listening</code></pre>"},{"location":"guides/phase2-preparation/#implementation","title":"Implementation","text":"<pre><code># src/reachy_agent/privacy/indicators.py\n\nfrom reachy_agent.simulation import ReachyMiniClient\nfrom reachy_agent.attention.states import AttentionState\n\nclass PrivacyIndicator:\n    def __init__(self, client: ReachyMiniClient):\n        self.client = client\n\n    async def set_state(self, state: AttentionState) -&gt; None:\n        \"\"\"Update antenna positions to reflect attention state.\"\"\"\n        match state:\n            case AttentionState.PASSIVE:\n                await self.client.set_antenna_state(\n                    left_angle=0, right_angle=0\n                )\n            case AttentionState.ALERT:\n                await self.client.set_antenna_state(\n                    left_angle=45, right_angle=45\n                )\n            case AttentionState.ENGAGED:\n                await self.client.set_antenna_state(\n                    left_angle=90, right_angle=90\n                )\n\n    async def indicate_listening(self) -&gt; None:\n        \"\"\"Subtle wiggle to show active recording.\"\"\"\n        for _ in range(2):\n            await self.client.set_antenna_state(left_angle=85, right_angle=95)\n            await asyncio.sleep(0.2)\n            await self.client.set_antenna_state(left_angle=95, right_angle=85)\n            await asyncio.sleep(0.2)\n        await self.client.set_antenna_state(left_angle=90, right_angle=90)\n</code></pre>"},{"location":"guides/phase2-preparation/#thermal-management","title":"Thermal Management","text":"<p>The Raspberry Pi 4 can throttle under load. Monitor and manage thermals:</p> <pre><code># Check temperature\nvcgencmd measure_temp\n\n# Install monitoring\nsudo apt install lm-sensors htop\n\n# Add cooling if needed (fan, heatsink)\n</code></pre>"},{"location":"guides/phase2-preparation/#thermal-thresholds","title":"Thermal Thresholds","text":"Temperature Action &lt; 60\u00b0C Normal operation 60-70\u00b0C Reduce background tasks 70-80\u00b0C Throttle Claude API calls &gt; 80\u00b0C Enter sleep mode"},{"location":"guides/phase2-preparation/#network-configuration","title":"Network Configuration","text":""},{"location":"guides/phase2-preparation/#static-ip-recommended","title":"Static IP (Recommended)","text":"<pre><code>sudo nano /etc/dhcpcd.conf\n</code></pre> <p>Add: <pre><code>interface wlan0\nstatic ip_address=192.168.1.100/24\nstatic routers=192.168.1.1\nstatic domain_name_servers=192.168.1.1 8.8.8.8\n</code></pre></p>"},{"location":"guides/phase2-preparation/#firewall","title":"Firewall","text":"<pre><code># Allow SSH\nsudo ufw allow ssh\n\n# Allow local Reachy daemon\nsudo ufw allow from 127.0.0.1 to any port 8000\n\n# Enable firewall\nsudo ufw enable\n</code></pre>"},{"location":"guides/phase2-preparation/#testing-on-hardware","title":"Testing on Hardware","text":""},{"location":"guides/phase2-preparation/#quick-validation","title":"Quick Validation","text":"<pre><code># 1. Check daemon\ncurl http://localhost:8000/api/daemon/status\n\n# 2. Test head movement (use set_target for real hardware)\ncurl -X POST http://localhost:8000/api/move/set_target \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"target_head_pose\": {\"yaw\": 0.5}}'\n\n# 3. Run agent validation (adapted for hardware)\npython scripts/validate_hardware.py\n</code></pre>"},{"location":"guides/phase2-preparation/#integration-test","title":"Integration Test","text":"<pre><code># scripts/validate_hardware.py\n\nimport asyncio\nfrom reachy_agent.simulation import ReachyMiniClient\n\nasync def validate():\n    client = ReachyMiniClient(base_url=\"http://localhost:8000\")\n\n    print(\"Testing wake_up...\")\n    await client.wake_up()\n    await asyncio.sleep(1)\n\n    print(\"Testing head movement...\")\n    await client.move_head(\"left\", speed=\"normal\")\n    await asyncio.sleep(1)\n    await client.move_head(\"right\", speed=\"normal\")\n    await asyncio.sleep(1)\n\n    print(\"Testing antennas...\")\n    await client.set_antenna_state(90, 90)\n    await asyncio.sleep(0.5)\n    await client.set_antenna_state(0, 0)\n\n    print(\"Testing gestures...\")\n    await client.nod(times=2)\n    await asyncio.sleep(2)\n\n    print(\"Returning to rest...\")\n    await client.rest()\n\n    await client.close()\n    print(\"ALL TESTS PASSED!\")\n\nasyncio.run(validate())\n</code></pre>"},{"location":"guides/phase2-preparation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/phase2-preparation/#daemon-not-responding","title":"Daemon Not Responding","text":"<pre><code># Check if running\nsudo systemctl status reachy-daemon\n\n# Restart\nsudo systemctl restart reachy-daemon\n\n# Check logs\njournalctl -u reachy-daemon -n 50\n</code></pre>"},{"location":"guides/phase2-preparation/#motors-not-moving","title":"Motors Not Moving","text":"<pre><code># Check motor power\ncurl http://localhost:8000/api/daemon/status | jq .backend_status\n\n# Wake up robot first\ncurl -X POST http://localhost:8000/api/move/play/wake_up\n</code></pre>"},{"location":"guides/phase2-preparation/#audio-issues","title":"Audio Issues","text":"<pre><code># Check ALSA\narecord -l\naplay -l\n\n# Fix permissions\nsudo usermod -a -G audio pi\n</code></pre>"},{"location":"guides/phase2-preparation/#network-connectivity","title":"Network Connectivity","text":"<pre><code># Check WiFi\niwconfig wlan0\n\n# Test Claude API\ncurl https://api.anthropic.com/v1/messages \\\n  -H \"x-api-key: $ANTHROPIC_API_KEY\" \\\n  -H \"anthropic-version: 2024-01-01\" \\\n  -H \"content-type: application/json\" \\\n  -d '{\"model\": \"claude-sonnet-4-20250514\", \"max_tokens\": 10, \"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}]}'\n</code></pre>"},{"location":"guides/phase2-preparation/#phase-2-completion-checklist","title":"Phase 2 Completion Checklist","text":"<p>Before moving to Phase 3, verify:</p> <ul> <li>[ ] Raspberry Pi OS installed and updated</li> <li>[ ] Reachy daemon running and responsive</li> <li>[ ] reachy-agent systemd service enabled</li> <li>[ ] All hardware tests pass</li> <li>[ ] Audio input/output working</li> <li>[ ] Wake word detection functional</li> <li>[ ] Attention state machine implemented</li> <li>[ ] Privacy indicators working</li> <li>[ ] Thermal monitoring in place</li> <li>[ ] Network stable</li> </ul>"},{"location":"guides/phase2-preparation/#next-steps","title":"Next Steps","text":"<p>After completing Phase 2:</p> <ol> <li>Phase 3: Memory &amp; Personality - ChromaDB, long-term memory, expression system</li> <li>Phase 4: External Integrations - Home Assistant, Calendar, GitHub MCP servers</li> </ol>"},{"location":"guides/raspberry-pi-installation/","title":"Raspberry Pi Installation Guide","text":"<p>This guide covers installing Claude in the Shell on a Reachy Mini's Raspberry Pi 4.</p>"},{"location":"guides/raspberry-pi-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Reachy Mini with Raspberry Pi 4 (4GB+ RAM recommended)</li> <li>Reachy Mini SDK installed and daemon running</li> <li>WiFi connectivity</li> <li>Anthropic API key</li> </ul>"},{"location":"guides/raspberry-pi-installation/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Raspberry Pi 4                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Claude in the Shell                                      \u2502  \u2502\n\u2502  \u2502  \u251c\u2500\u2500 Agent Loop (Claude SDK)                              \u2502  \u2502\n\u2502  \u2502  \u251c\u2500\u2500 Reachy MCP Server (23 tools)                         \u2502  \u2502\n\u2502  \u2502  \u251c\u2500\u2500 Memory MCP Server (4 tools)                          \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500 Permission System (4-tier)                           \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                        \u2502 HTTP :8000                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Reachy Daemon (Pollen Robotics)                          \u2502  \u2502\n\u2502  \u2502  \u251c\u2500\u2500 FastAPI REST endpoints                               \u2502  \u2502\n\u2502  \u2502  \u251c\u2500\u2500 Zenoh pub/sub (for real-time apps)                   \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500 Hardware drivers                                     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                        \u2502                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Robot Hardware                                           \u2502  \u2502\n\u2502  \u2502  \u251c\u2500\u2500 Head (6 DOF) + Body (360\u00b0)                           \u2502  \u2502\n\u2502  \u2502  \u251c\u2500\u2500 2 Animated Antennas                                  \u2502  \u2502\n\u2502  \u2502  \u251c\u2500\u2500 Wide-angle Camera                                    \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500 4-mic Array + Speaker                                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#compatibility-with-reachy-ecosystem","title":"Compatibility with Reachy Ecosystem","text":"<p>Claude in the Shell is designed to coexist with other Reachy apps:</p> Protocol Used By Purpose HTTP REST (<code>:8000</code>) Claude in the Shell Tool-based commands Zenoh (pub/sub) Conversation App, Dashboard Real-time 100Hz control <p>Both protocols access the same daemon - you can run Claude in the Shell alongside the dashboard.</p>"},{"location":"guides/raspberry-pi-installation/#installation","title":"Installation","text":""},{"location":"guides/raspberry-pi-installation/#step-1-verify-reachy-sdk","title":"Step 1: Verify Reachy SDK","text":"<p>First, ensure the Reachy Mini SDK is installed and the daemon is running:</p> <pre><code># Check daemon status\ncurl http://localhost:8000/api/daemon/status\n\n# Expected output includes:\n# {\"simulation_enabled\": false, \"robot_name\": \"reachy_mini\", ...}\n</code></pre> <p>If the daemon isn't running, start it according to Pollen's documentation.</p>"},{"location":"guides/raspberry-pi-installation/#step-2-install-uv-fast-package-manager","title":"Step 2: Install uv (Fast Package Manager)","text":"<pre><code># Install uv for 10-100x faster package installation\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Reload shell\nsource ~/.bashrc  # or ~/.zshrc\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#step-3-clone-and-install-claude-in-the-shell","title":"Step 3: Clone and Install Claude in the Shell","text":"<pre><code># Clone the repository\ngit clone https://github.com/jawhnycooke/claude-in-the-shell.git\ncd claude-in-the-shell\n\n# Create virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install dependencies\nuv pip install -r requirements.txt\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#step-4-configure-environment","title":"Step 4: Configure Environment","text":"<pre><code># Copy environment template\ncp .env.example .env\n\n# Edit with your API key\nnano .env\n</code></pre> <p>Required environment variables:</p> <pre><code># Required\nANTHROPIC_API_KEY=sk-ant-...\n\n# Optional: GitHub integration\nGITHUB_PERSONAL_ACCESS_TOKEN=ghp_...\n\n# Optional: Custom daemon URL (default: http://localhost:8000)\nREACHY_DAEMON_URL=http://localhost:8000\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#step-5-install-github-mcp-binary-optional","title":"Step 5: Install GitHub MCP Binary (Optional)","text":"<p>For GitHub integration without Docker:</p> <pre><code># Create bin directory\nmkdir -p ~/.reachy/bin\n\n# Download ARM64 binary\ncurl -sL https://github.com/github/github-mcp-server/releases/latest/download/github-mcp-server_Linux_arm64.tar.gz | tar xzf - -C ~/.reachy/bin\n\n# Verify installation\n~/.reachy/bin/github-mcp-server --version\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#step-6-verify-installation","title":"Step 6: Verify Installation","text":"<pre><code># Run health check\npython -m reachy_agent check\n\n# Expected output:\n# \u2713 Daemon connection: OK\n# \u2713 MCP servers: OK\n# \u2713 Permissions: OK\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#running-the-agent","title":"Running the Agent","text":""},{"location":"guides/raspberry-pi-installation/#interactive-repl-mode","title":"Interactive REPL Mode","text":"<pre><code># Rich terminal interface\npython -m reachy_agent repl\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#web-dashboard-mode","title":"Web Dashboard Mode","text":"<pre><code># Start web interface at http://localhost:8080\npython -m reachy_agent web\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#background-agent-mode","title":"Background Agent Mode","text":"<pre><code># Run agent in background\npython -m reachy_agent run --daemon\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#systemd-service-auto-start","title":"Systemd Service (Auto-Start)","text":"<p>Create a systemd service for automatic startup:</p> <pre><code>sudo nano /etc/systemd/system/claude-in-the-shell.service\n</code></pre> <pre><code>[Unit]\nDescription=Claude in the Shell - Reachy AI Agent\nAfter=network.target reachy-daemon.service\nWants=reachy-daemon.service\n\n[Service]\nType=simple\nUser=pi\nWorkingDirectory=/home/pi/claude-in-the-shell\nEnvironment=\"PATH=/home/pi/claude-in-the-shell/.venv/bin:/usr/local/bin:/usr/bin\"\nEnvironment=\"ANTHROPIC_API_KEY=sk-ant-...\"\nExecStart=/home/pi/claude-in-the-shell/.venv/bin/python -m reachy_agent run\nRestart=on-failure\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable claude-in-the-shell\nsudo systemctl start claude-in-the-shell\n\n# Check status\nsudo systemctl status claude-in-the-shell\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#memory-requirements","title":"Memory Requirements","text":"Component RAM Usage Claude in the Shell ~200MB ChromaDB (memory) ~100MB MCP servers ~50MB Total ~350MB <p>Raspberry Pi 4 with 4GB RAM has plenty of headroom.</p>"},{"location":"guides/raspberry-pi-installation/#performance-tips","title":"Performance Tips","text":""},{"location":"guides/raspberry-pi-installation/#1-reduce-claude-api-latency","title":"1. Reduce Claude API Latency","text":"<p>Use a fast model for routine tasks:</p> <pre><code># In config/default.yaml\nagent:\n  model: claude-sonnet-4-20250514  # Fast model\n  # model: claude-opus-4-5-20250514  # For complex reasoning\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#2-monitor-temperature","title":"2. Monitor Temperature","text":"<pre><code># Check CPU temperature\nvcgencmd measure_temp\n\n# If throttling, add cooling\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#3-use-swap-for-large-memory-operations","title":"3. Use Swap for Large Memory Operations","text":"<pre><code># Increase swap if needed\nsudo dphys-swapfile swapoff\nsudo nano /etc/dphys-swapfile\n# Set CONF_SWAPSIZE=2048\nsudo dphys-swapfile setup\nsudo dphys-swapfile swapon\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/raspberry-pi-installation/#daemon-connection-failed","title":"Daemon Connection Failed","text":"<pre><code># Check if daemon is running\nsystemctl status reachy-mini-daemon\n\n# Check port availability\nnetstat -tlnp | grep 8000\n\n# Restart daemon\nsudo systemctl restart reachy-mini-daemon\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#permission-denied-errors","title":"Permission Denied Errors","text":"<pre><code># Add user to required groups\nsudo usermod -aG video,audio,gpio pi\n\n# Reload groups\nnewgrp video\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#api-rate-limits","title":"API Rate Limits","text":"<p>Claude API has rate limits. If you see 429 errors:</p> <pre><code># In config/default.yaml\nagent:\n  rate_limit_delay: 1.0  # seconds between requests\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#memory-issues","title":"Memory Issues","text":"<pre><code># Check memory usage\nfree -h\n\n# Clear Python cache\nfind . -type d -name __pycache__ -exec rm -rf {} +\n\n# Restart with lower memory profile\npython -m reachy_agent run --memory-mode minimal\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#updating","title":"Updating","text":"<pre><code>cd ~/claude-in-the-shell\ngit pull origin main\nsource .venv/bin/activate\nuv pip install -r requirements.txt\n\n# Restart service\nsudo systemctl restart claude-in-the-shell\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#logs","title":"Logs","text":"<pre><code># View service logs\njournalctl -u claude-in-the-shell -f\n\n# View agent logs\ntail -f ~/.reachy/logs/agent.log\n</code></pre>"},{"location":"guides/raspberry-pi-installation/#coexistence-with-other-apps","title":"Coexistence with Other Apps","text":"<p>Claude in the Shell can run alongside:</p> <ul> <li>Reachy Dashboard: Both use the daemon simultaneously</li> <li>Conversation App: May conflict if both try to control the robot</li> </ul> <p>To avoid conflicts: 1. Stop Conversation App before running Claude in the Shell 2. Or use the web dashboard to switch between apps</p>"},{"location":"guides/raspberry-pi-installation/#security-considerations","title":"Security Considerations","text":"<ol> <li>API Keys: Never commit <code>.env</code> to version control</li> <li>Network: The web dashboard is bound to localhost by default</li> <li>Permissions: The 4-tier system prevents destructive actions</li> </ol>"},{"location":"guides/raspberry-pi-installation/#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started Tutorial - Learn the basics</li> <li>Architecture Overview - Understand the system</li> <li>MCP Tools Reference - Available robot commands</li> </ul>"},{"location":"guides/troubleshooting/","title":"Troubleshooting Guide","text":"<p>Common issues and solutions for the Reachy Agent.</p>"},{"location":"guides/troubleshooting/#quick-diagnostics","title":"Quick Diagnostics","text":"<p>Run the health check first:</p> <pre><code>python -m reachy_agent check\n</code></pre> <p>This validates: - Configuration loading - Anthropic API key - Daemon connectivity</p>"},{"location":"guides/troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"guides/troubleshooting/#connection-refused-on-port-8000","title":"\"Connection refused on port 8000\"","text":"<p>Cause: No daemon running on the production port.</p> <p>Solutions:</p> <ol> <li> <p>Start mock daemon for testing:    <pre><code>python -m reachy_agent run --mock\n</code></pre></p> </li> <li> <p>Or start MuJoCo simulation (adjust port):    <pre><code>/opt/homebrew/bin/mjpython -m reachy_mini.daemon.app.main \\\n  --sim --scene minimal --fastapi-port 8000\n</code></pre></p> </li> <li> <p>On Raspberry Pi, check daemon service:    <pre><code>sudo systemctl status reachy-daemon\nsudo systemctl restart reachy-daemon\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#connection-refused-on-port-8765","title":"\"Connection refused on port 8765\"","text":"<p>Cause: MuJoCo simulation not running.</p> <p>Solution: Start the simulation:</p> <pre><code># macOS with GUI\n/opt/homebrew/bin/mjpython -m reachy_mini.daemon.app.main \\\n  --sim --scene minimal --fastapi-port 8765\n\n# Headless (CI/SSH)\npython -m reachy_mini.daemon.app.main \\\n  --sim --scene minimal --headless --fastapi-port 8765\n</code></pre>"},{"location":"guides/troubleshooting/#timeout-connecting-to-daemon","title":"\"Timeout connecting to daemon\"","text":"<p>Cause: Daemon is slow or network issues.</p> <p>Solutions:</p> <ol> <li> <p>Increase timeout in <code>config/default.yaml</code>:    <pre><code>daemon:\n  timeout_seconds: 60\n</code></pre></p> </li> <li> <p>Check network connectivity:    <pre><code>curl -v http://localhost:8765/health\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#permission-issues","title":"Permission Issues","text":""},{"location":"guides/troubleshooting/#permission-denied-for-tool-x","title":"\"Permission denied for tool X\"","text":"<p>Cause: Tool is Tier 4 (Forbidden) or needs confirmation.</p> <p>Solutions:</p> <ol> <li> <p>Check permission tier in <code>config/permissions.yaml</code></p> </li> <li> <p>For Tier 3 tools, user confirmation is required via CLI or web UI</p> </li> <li> <p>Adjust permission rules if appropriate:    <pre><code>rules:\n  - pattern: \"mcp__reachy__&lt;tool&gt;\"\n    tier: 1  # Change to autonomous\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#confirmation-timeout","title":"\"Confirmation timeout\"","text":"<p>Cause: User didn't respond within 60 seconds.</p> <p>Solutions:</p> <ol> <li> <p>Use the web dashboard for easier confirmation dialogs</p> </li> <li> <p>Increase timeout in permission config:    <pre><code>confirmation_timeout_seconds: 120\n</code></pre></p> </li> <li> <p>Check if CLI confirmation handler is properly configured</p> </li> </ol>"},{"location":"guides/troubleshooting/#api-issues","title":"API Issues","text":""},{"location":"guides/troubleshooting/#anthropic_api_key-not-set","title":"\"ANTHROPIC_API_KEY not set\"","text":"<p>Cause: Missing or invalid API key.</p> <p>Solutions:</p> <ol> <li> <p>Set environment variable:    <pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre></p> </li> <li> <p>Or add to <code>.env</code> file:    <pre><code>ANTHROPIC_API_KEY=sk-ant-...\n</code></pre></p> </li> <li> <p>Verify key is valid:    <pre><code>python -c \"import anthropic; print(anthropic.Anthropic().models.list())\"\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#rate-limit-exceeded","title":"\"Rate limit exceeded\"","text":"<p>Cause: Too many API requests.</p> <p>Solutions:</p> <ol> <li> <p>Wait and retry (automatic backoff)</p> </li> <li> <p>Check your API tier limits at console.anthropic.com</p> </li> <li> <p>Reduce request frequency in conversations</p> </li> </ol>"},{"location":"guides/troubleshooting/#model-not-found","title":"\"Model not found\"","text":"<p>Cause: Invalid model name in config.</p> <p>Solution: Use a valid model in <code>config/default.yaml</code>: <pre><code>agent:\n  model: claude-sonnet-4-20250514  # or claude-3-haiku-20240307\n</code></pre></p>"},{"location":"guides/troubleshooting/#memory-issues","title":"Memory Issues","text":""},{"location":"guides/troubleshooting/#chromadb-initialization-failed","title":"\"ChromaDB initialization failed\"","text":"<p>Cause: Disk space, permissions, or corrupted data.</p> <p>Solutions:</p> <ol> <li> <p>Check disk space:    <pre><code>df -h ~/.reachy\n</code></pre></p> </li> <li> <p>Reset ChromaDB (loses memories):    <pre><code>rm -rf ~/.reachy/memory/chroma\npython -m reachy_agent run\n</code></pre></p> </li> <li> <p>Check directory permissions:    <pre><code>chmod 755 ~/.reachy/memory\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#embedding-model-not-found","title":"\"Embedding model not found\"","text":"<p>Cause: sentence-transformers not installed or model not downloaded.</p> <p>Solutions:</p> <ol> <li> <p>Install dependencies:    <pre><code>uv pip install sentence-transformers\n</code></pre></p> </li> <li> <p>Pre-download model:    <pre><code>python -c \"from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')\"\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#sqlite-database-locked","title":"\"SQLite database locked\"","text":"<p>Cause: Multiple processes accessing the database.</p> <p>Solutions:</p> <ol> <li> <p>Find processes:    <pre><code>lsof ~/.reachy/memory/reachy.db\n</code></pre></p> </li> <li> <p>Kill conflicting processes:    <pre><code>kill &lt;PID&gt;\n</code></pre></p> </li> <li> <p>Or wait for other processes to finish</p> </li> </ol>"},{"location":"guides/troubleshooting/#mcp-server-issues","title":"MCP Server Issues","text":""},{"location":"guides/troubleshooting/#mcp-server-failed-to-start","title":"\"MCP server failed to start\"","text":"<p>Cause: Import errors or missing dependencies.</p> <p>Solutions:</p> <ol> <li> <p>Test MCP server directly:    <pre><code>python -m reachy_agent.mcp_servers.reachy\n</code></pre></p> </li> <li> <p>Check for import errors:    <pre><code>python -c \"from reachy_agent.mcp_servers.reachy import reachy_mcp\"\n</code></pre></p> </li> <li> <p>Reinstall dependencies:    <pre><code>uv pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#tool-not-found-mcp__reachy__x","title":"\"Tool not found: mcp__reachy__X\"","text":"<p>Cause: MCP server not registered or tool renamed.</p> <p>Solutions:</p> <ol> <li> <p>Check available tools:    <pre><code># Run MCP Inspector\nnpx @modelcontextprotocol/inspector \\\n  .venv/bin/python -m reachy_agent.mcp_servers.reachy\n</code></pre></p> </li> <li> <p>Verify tool name in <code>reachy_mcp.py</code></p> </li> </ol>"},{"location":"guides/troubleshooting/#voice-pipeline-issues","title":"Voice Pipeline Issues","text":""},{"location":"guides/troubleshooting/#wake-word-not-detected","title":"\"Wake word not detected\"","text":"<p>Cause: OpenWakeWord model not loading or audio input issues.</p> <p>Solutions:</p> <ol> <li> <p>Verify wake word models exist:    <pre><code>ls -la data/wake_words/*.onnx\n</code></pre></p> </li> <li> <p>Test wake word detection standalone:    <pre><code>python -m reachy_agent.voice.wake_word --test\n</code></pre></p> </li> <li> <p>Check audio input device:    <pre><code>arecord -l  # List capture devices\nREACHY_DEBUG=1 python -m reachy_agent run --voice\n</code></pre></p> </li> <li> <p>Ensure correct sample rate (16kHz for wake word):    <pre><code>voice:\n  audio:\n    sample_rate: 16000\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#openai-realtime-connection-failed","title":"\"OpenAI Realtime connection failed\"","text":"<p>Cause: Missing API key, network issues, or WebSocket timeout.</p> <p>Solutions:</p> <ol> <li> <p>Verify OpenAI API key:    <pre><code>echo $OPENAI_API_KEY\n</code></pre></p> </li> <li> <p>Test network connectivity:    <pre><code>curl -v https://api.openai.com/v1/models\n</code></pre></p> </li> <li> <p>Increase WebSocket timeout:    <pre><code>voice:\n  realtime:\n    connect_timeout: 30.0\n    response_timeout: 60.0\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#audio-cutoff-during-tts","title":"\"Audio cutoff during TTS\"","text":"<p>Cause: VAD detecting silence too early or buffer underrun.</p> <p>Solutions:</p> <ol> <li> <p>Adjust VAD sensitivity:    <pre><code>voice:\n  vad:\n    silence_threshold: 0.5  # Increase to be less sensitive\n    min_speech_duration: 0.5\n</code></pre></p> </li> <li> <p>Check audio output device:    <pre><code>aplay -l  # List playback devices\nspeaker-test -t wav -c 2\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#device-or-resource-busy","title":"\"Device or resource busy\"","text":"<p>Cause: Multiple processes accessing audio hardware.</p> <p>Solutions:</p> <ol> <li> <p>Check for conflicting processes:    <pre><code>fuser -v /dev/snd/*\nlsof /dev/snd/*\n</code></pre></p> </li> <li> <p>Use ALSA dsnoop/dmix for device sharing:    <pre><code># ~/.asoundrc for Raspberry Pi\npcm.!default {\n  type asym\n  playback.pcm \"dmix\"\n  capture.pcm \"dsnoop\"\n}\n</code></pre></p> </li> <li> <p>Kill conflicting processes:    <pre><code>pkill pulseaudio  # If PulseAudio is conflicting\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#persona-switching-issues","title":"Persona Switching Issues","text":""},{"location":"guides/troubleshooting/#persona-prompt-not-found","title":"\"Persona prompt not found\"","text":"<p>Cause: Missing persona prompt file or incorrect path.</p> <p>Solutions:</p> <ol> <li> <p>Verify persona prompt files:    <pre><code>ls -la prompts/personas/\n# Should have: motoko.md, batou.md\n</code></pre></p> </li> <li> <p>Check configuration:    <pre><code>voice:\n  personas:\n    hey_motoko:\n      name: \"motoko\"\n      prompt_path: \"prompts/personas/motoko.md\"\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#voice-not-changing-on-wake-word","title":"\"Voice not changing on wake word\"","text":"<p>Cause: Deferred persona switch not completing.</p> <p>Solutions:</p> <ol> <li> <p>Check persona manager state:    <pre><code># In debug mode\nprint(voice_pipeline.persona_manager.current_persona)\n</code></pre></p> </li> <li> <p>Ensure TTS completes before switch:</p> </li> <li>Persona switches are deferred until speaking finishes</li> <li> <p>Check logs for \"deferred_persona_switch\" messages</p> </li> <li> <p>Verify OpenAI Realtime reconnection:</p> </li> <li>New persona requires new session with different voice</li> <li>Check for \"reconnecting with new voice\" log messages</li> </ol>"},{"location":"guides/troubleshooting/#sdk-connection-issues","title":"SDK Connection Issues","text":""},{"location":"guides/troubleshooting/#sdk-connection-failed","title":"\"SDK connection failed\"","text":"<p>Cause: ReachyMini SDK cannot connect to daemon.</p> <p>Solutions:</p> <ol> <li> <p>Verify daemon is running:    <pre><code>curl http://localhost:8000/api/daemon/status\n</code></pre></p> </li> <li> <p>Check Zenoh connectivity (on Pi):    <pre><code># Test SDK directly\npython -c \"\nfrom reachy_mini import ReachyMini\nrobot = ReachyMini(localhost_only=True, spawn_daemon=False)\nprint('Connected!')\nrobot.disconnect()\n\"\n</code></pre></p> </li> <li> <p>Enable HTTP fallback:    <pre><code>sdk:\n  fallback_to_http: true\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#sdk-fallback-to-http-active","title":"\"SDK fallback to HTTP active\"","text":"<p>Cause: 5+ consecutive SDK failures triggered circuit breaker.</p> <p>Solutions:</p> <ol> <li> <p>Check SDK error logs:    <pre><code>REACHY_DEBUG=1 python -m reachy_agent run\n# Look for \"sdk_set_pose_exception\" messages\n</code></pre></p> </li> <li> <p>Reset SDK fallback:    <pre><code># Programmatically\nagent.blend_controller.reset_sdk_fallback()\n</code></pre></p> </li> <li> <p>Restart agent to reset circuit breaker</p> </li> </ol>"},{"location":"guides/troubleshooting/#motion-feels-choppy","title":"\"Motion feels choppy\"","text":"<p>Cause: Using HTTP fallback (10-50ms) instead of SDK (1-5ms).</p> <p>Solutions:</p> <ol> <li> <p>Check which backend is active:    <pre><code>status = agent.blend_controller.get_status()\nprint(f\"SDK fallback: {status['sdk_fallback_active']}\")\n</code></pre></p> </li> <li> <p>Ensure SDK is preferred:    <pre><code>sdk:\n  enabled: true\nmotion_blend:\n  command_rate_hz: 20.0  # Reduce if HTTP-only\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#hardware-issues","title":"Hardware Issues","text":""},{"location":"guides/troubleshooting/#robot-becomes-unresponsive-channel-closed","title":"Robot Becomes Unresponsive (\"channel closed\")","text":"<p>Cause: Communication channel between daemon and motors lost. Motors go limp and commands fail.</p> <p>Solutions (in order of preference):</p> <ol> <li> <p>Web UI Recovery (Fastest):    <pre><code>Open browser: http://reachy-mini.local:8000/settings\nToggle the On/Off switch: Off, then On\n</code></pre></p> </li> <li> <p>SSH Recovery:    <pre><code>ssh pollen@reachy-mini.local\n# Password: root\nsudo systemctl restart reachy-mini-daemon\n</code></pre></p> </li> <li> <p>Physical Reset (Last resort):</p> </li> <li>Power cycle the robot using the hardware power switch</li> <li>Wait 10 seconds before powering back on</li> </ol> <p>After recovery, run <code>wake_up</code> to re-enable motor control.</p>"},{"location":"guides/troubleshooting/#motor-not-responding","title":"\"Motor not responding\"","text":"<p>Cause: Hardware disconnection or power issue.</p> <p>Solutions:</p> <ol> <li> <p>Check daemon status:    <pre><code>curl http://localhost:8000/api/daemon/status\n</code></pre></p> </li> <li> <p>Restart the daemon:    <pre><code>sudo systemctl restart reachy-mini-daemon\n</code></pre></p> </li> <li> <p>Check power and connections on the robot</p> </li> </ol>"},{"location":"guides/troubleshooting/#camera-frame-capture-failed","title":"\"Camera frame capture failed\"","text":"<p>Cause: Camera not connected or driver issue.</p> <p>Solutions:</p> <ol> <li> <p>Test camera access:    <pre><code>curl http://localhost:8000/api/camera/capture\n</code></pre></p> </li> <li> <p>On Raspberry Pi, check camera status:    <pre><code>vcgencmd get_camera\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#environment-issues","title":"Environment Issues","text":""},{"location":"guides/troubleshooting/#modulenotfounderror-reachy_agent","title":"\"ModuleNotFoundError: reachy_agent\"","text":"<p>Cause: Package not installed or wrong environment.</p> <p>Solutions:</p> <ol> <li> <p>Ensure virtual environment is active:    <pre><code>source .venv/bin/activate\n</code></pre></p> </li> <li> <p>Reinstall package:    <pre><code>uv pip install -e .\n</code></pre></p> </li> <li> <p>Check Python path:    <pre><code>python -c \"import reachy_agent; print(reachy_agent.__file__)\"\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#mjpython-not-found","title":"\"mjpython not found\"","text":"<p>Cause: MuJoCo not installed on macOS.</p> <p>Solution: Install MuJoCo:</p> <pre><code>brew install mujoco\n# mjpython should be at /opt/homebrew/bin/mjpython\n</code></pre>"},{"location":"guides/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"guides/troubleshooting/#agent-response-is-slow","title":"\"Agent response is slow\"","text":"<p>Cause: Network latency, model size, or tool execution time.</p> <p>Solutions:</p> <ol> <li> <p>Use a faster model:    <pre><code>agent:\n  model: claude-3-haiku-20240307\n</code></pre></p> </li> <li> <p>Check network latency:    <pre><code>ping api.anthropic.com\n</code></pre></p> </li> <li> <p>Enable debug logging to identify bottleneck:    <pre><code>REACHY_DEBUG=1 python -m reachy_agent run\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#high-memory-usage","title":"\"High memory usage\"","text":"<p>Cause: Large conversation history or memory accumulation.</p> <p>Solutions:</p> <ol> <li> <p>Clear conversation history (restart session)</p> </li> <li> <p>Run memory cleanup:    <pre><code>from reachy_agent.memory.manager import MemoryManager\nmanager = MemoryManager(...)\nawait manager.cleanup()\n</code></pre></p> </li> </ol>"},{"location":"guides/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still stuck:</p> <ol> <li> <p>Check debug logs:    <pre><code>REACHY_DEBUG=1 python -m reachy_agent run 2&gt;&amp;1 | tee debug.log\n</code></pre></p> </li> <li> <p>Search existing issues: https://github.com/jawhnycooke/claude-in-the-shell/issues</p> </li> <li> <p>Open a new issue with:</p> </li> <li>Python version: <code>python --version</code></li> <li>Package versions: <code>uv pip freeze</code></li> <li>Full error traceback</li> <li>Steps to reproduce</li> </ol>"},{"location":"guides/voice-pipeline-setup/","title":"How to Set Up the Voice Pipeline on Reachy Mini","text":"<p>Goal: Enable real-time voice interaction with Reachy Mini using OpenAI's Realtime API Use case: When you want Reachy to respond to voice commands with wake word detection Time required: 30-45 minutes (including testing)</p>"},{"location":"guides/voice-pipeline-setup/#prerequisites","title":"Prerequisites","text":"<p>Before starting, you should: - Have basic familiarity with Linux command line and YAML configuration - Have Reachy Mini hardware with USB audio device connected - Have access to the Raspberry Pi 4 via SSH - Possess an OpenAI API key with access to the Realtime API - Understand basic audio concepts (sample rates, device indices)</p> <p>Required software: - Reachy Agent installed: <code>uv pip install -e \".[voice]\"</code> - ALSA utilities: <code>sudo apt-get install alsa-utils</code> - PyAudio dependencies: <code>sudo apt-get install portaudio19-dev python3-pyaudio</code></p> <p>Required hardware: - Reachy Mini with USB Audio device (4-mic array + speaker) - Microphone for audio input - Speaker for audio output</p>"},{"location":"guides/voice-pipeline-setup/#problem-context","title":"Problem Context","text":"<p>The Reachy Mini voice pipeline enables natural conversation with the robot through: 1. Wake word detection (e.g., \"Hey Motoko\", \"Hey Batou\") 2. Real-time speech-to-text using OpenAI's Realtime API 3. Claude-powered responses with persona-based personalities 4. Text-to-speech playback through the robot's speaker 5. Audio-reactive head wobble animations during speech</p> <p>This setup is essential for autonomous voice interaction without requiring manual input.</p>"},{"location":"guides/voice-pipeline-setup/#solution-overview","title":"Solution Overview","text":"<p>We'll configure the voice pipeline by: 1. Identifying audio hardware device indices 2. Configuring ALSA for shared device access 3. Setting up wake word models 4. Configuring voice pipeline settings in <code>config/default.yaml</code> 5. Testing the complete pipeline</p> <p>Why this approach: Reachy's daemon and the voice pipeline need simultaneous access to the same audio hardware. ALSA's dsnoop/dmix plugins enable this shared access, preventing \"Device or resource busy\" errors.</p>"},{"location":"guides/voice-pipeline-setup/#step-1-identify-audio-devices","title":"Step 1: Identify Audio Devices","text":"<p>Find your audio device indices using ALSA utilities.</p> <pre><code># List all audio capture devices (microphones)\narecord -l\n\n# Expected output on Reachy Mini:\n# card 2: Device [USB Audio Device], device 0: USB Audio [USB Audio]\n#   Subdevices: 1/1\n#   Subdevice #0: subdevice #0\n\n# List all playback devices (speakers)\naplay -l\n\n# Expected output on Reachy Mini:\n# card 2: Device [USB Audio Device], device 0: USB Audio [USB Audio]\n#   Subdevices: 1/1\n#   Subdevice #0: subdevice #0\n</code></pre> <p>Note the card and device numbers - you'll need these for ALSA configuration.</p> <p>For Reachy Mini with USB Audio: - Typical card number: <code>2</code> - Typical device number: <code>0</code></p> <p>Expected result: You have identified the card and device numbers for both microphone and speaker.</p>"},{"location":"guides/voice-pipeline-setup/#step-2-configure-alsa-for-shared-access","title":"Step 2: Configure ALSA for Shared Access","text":"<p>Create or edit <code>/home/reachy/.asoundrc</code> to enable device sharing between the daemon and voice pipeline.</p> <pre><code># Create ALSA configuration\nnano ~/.asoundrc\n</code></pre> <p>Add this configuration (adjust <code>card 2</code> to match your hardware):</p> <pre><code># Shared microphone input (dsnoop)\npcm.dsnoop_mic {\n    type dsnoop\n    ipc_key 5678\n    slave {\n        pcm \"hw:2,0\"\n        channels 1\n        rate 16000\n        format S16_LE\n    }\n}\n\n# Shared speaker output (dmix)\npcm.dmix_speaker {\n    type dmix\n    ipc_key 5679\n    slave {\n        pcm \"hw:2,0\"\n        channels 1\n        rate 24000\n        format S16_LE\n    }\n}\n\n# Make shared devices the defaults\npcm.!default {\n    type asym\n    playback.pcm \"dmix_speaker\"\n    capture.pcm \"dsnoop_mic\"\n}\n\nctl.!default {\n    type hw\n    card 2\n}\n</code></pre> <p>Key parameters explained: - <code>ipc_key</code>: Unique identifier for shared memory (must be unique per device) - <code>hw:2,0</code>: Hardware device (card 2, device 0) - <code>rate</code>: Sample rate must match voice pipeline config - <code>channels</code>: 1 for mono, 2 for stereo</p> <p>Verify it worked:</p> <pre><code># Test microphone capture (Ctrl+C to stop)\narecord -D dsnoop_mic -f S16_LE -r 16000 -c 1 test.wav\n\n# Test speaker playback\naplay -D dmix_speaker test.wav\n</code></pre> <p>If you hear clean audio playback, the ALSA configuration is working.</p>"},{"location":"guides/voice-pipeline-setup/#step-3-find-pyaudio-device-indices","title":"Step 3: Find PyAudio Device Indices","text":"<p>The voice pipeline uses PyAudio, which has different device indices than ALSA. Find the correct indices:</p> <pre><code># List PyAudio devices\npython3 -c \"\nimport pyaudio\np = pyaudio.PyAudio()\nfor i in range(p.get_device_count()):\n    info = p.get_device_info_by_index(i)\n    print(f'{i}: {info[\\\"name\\\"]} (in:{info[\\\"maxInputChannels\\\"]}, out:{info[\\\"maxOutputChannels\\\"]})')\np.terminate()\n\"\n</code></pre> <p>Expected output: <pre><code>0: bcm2835 Headphones (in:0, out:8)\n1: bcm2835 Headphones (in:0, out:8)\n2: sysdefault (in:128, out:128)\n3: dmix_speaker (in:0, out:2)\n4: dsnoop_mic (in:2, out:0)\n5: default (in:32, out:32)\n</code></pre></p> <p>For Reachy Mini USB Audio: - Input device index: <code>4</code> (dsnoop_mic) - Output device index: <code>3</code> (dmix_speaker)</p> <p>Expected result: You have the PyAudio device indices for both input and output.</p>"},{"location":"guides/voice-pipeline-setup/#step-4-set-up-wake-word-models","title":"Step 4: Set Up Wake Word Models","text":"<p>The voice pipeline supports custom wake word models for persona-based interaction.</p> <pre><code># Navigate to wake word models directory\ncd /home/reachy/reachy_project/data/wake_words/\n\n# Check bundled models\nls -lh\n\n# Expected files:\n# hey_motoko.onnx    - \"Hey Motoko\" wake word\n# hey_batou.onnx     - \"Hey Batou\" wake word\n# README.md          - Documentation\n</code></pre> <p>Bundled models (Ghost in the Shell theme): - <code>hey_motoko.onnx</code> - Female persona (analytical, philosophical) - <code>hey_batou.onnx</code> - Male persona (casual, action-oriented)</p> <p>If you need to add custom wake word models:</p> <pre><code># Download or copy your custom .onnx model\ncp /path/to/custom_wake_word.onnx data/wake_words/\n\n# Model filename must match the wake word key in config\n# Example: \"hey_jarvis\" requires hey_jarvis.onnx\n</code></pre> <p>Training custom wake words (advanced):</p> <pre><code># Clone OpenWakeWord repository\ngit clone https://github.com/dscripka/openWakeWord.git\ncd openWakeWord\n\n# Follow training instructions in the repository\n# to create custom .onnx models\n</code></pre> <p>Fallback behavior: If custom models are not found, the system falls back to bundled OpenWakeWord models (e.g., \"hey jarvis\", \"alexa\").</p> <p>Expected result: Wake word models are in place and ready for loading.</p>"},{"location":"guides/voice-pipeline-setup/#step-5-configure-voice-pipeline-settings","title":"Step 5: Configure Voice Pipeline Settings","text":"<p>Edit <code>config/default.yaml</code> to configure the voice pipeline for your hardware.</p> <pre><code># Edit configuration\nnano config/default.yaml\n</code></pre> <p>Essential configuration sections:</p>"},{"location":"guides/voice-pipeline-setup/#audio-hardware-settings","title":"Audio Hardware Settings","text":"<pre><code>voice:\n  audio:\n    sample_rate: 16000              # Microphone sample rate\n    channels: 1                     # Mono audio\n    chunk_size: 512                 # Samples per chunk (Silero VAD requirement)\n    format_bits: 16                 # int16 PCM\n    input_device_index: 4           # PyAudio index for dsnoop_mic\n    output_device_index: 3          # PyAudio index for dmix_speaker\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#wake-word-detection-settings","title":"Wake Word Detection Settings","text":"<pre><code>voice:\n  wake_word:\n    enabled: true\n    model: hey_jarvis               # Fallback model if persona models missing\n    sensitivity: 0.5                # 0.0 (strict) to 1.0 (lenient)\n    cooldown_seconds: 2.0           # Ignore detections for this period\n    custom_models_dir: data/wake_words\n</code></pre> <p>Sensitivity tuning: - <code>0.3-0.4</code>: Very strict (fewer false positives, may miss some detections) - <code>0.5</code>: Balanced (recommended starting point) - <code>0.6-0.7</code>: Lenient (more detections, possible false positives)</p>"},{"location":"guides/voice-pipeline-setup/#voice-activity-detection-vad","title":"Voice Activity Detection (VAD)","text":"<pre><code>voice:\n  vad:\n    silence_threshold_ms: 800       # Silence duration to trigger end-of-speech\n    min_speech_duration_ms: 250     # Minimum speech to be valid\n    max_speech_duration_s: 30.0     # Maximum before timeout\n    speech_threshold: 0.5           # VAD sensitivity (0.0-1.0)\n</code></pre> <p>VAD tuning tips: - Increase <code>silence_threshold_ms</code> if responses cut off too quickly - Decrease <code>speech_threshold</code> if VAD doesn't detect quiet speech - Increase <code>speech_threshold</code> if background noise triggers false detections</p>"},{"location":"guides/voice-pipeline-setup/#openai-realtime-api-settings","title":"OpenAI Realtime API Settings","text":"<pre><code>voice:\n  openai:\n    model: gpt-4o-realtime-preview-2024-12-17\n    voice: nova                     # Options: alloy, echo, fable, onyx, nova, shimmer\n    sample_rate: 24000              # OpenAI uses 24kHz\n    temperature: 0.8                # Response creativity\n    max_response_tokens: 4096       # Max response length\n    turn_detection_threshold: 0.5\n    turn_detection_silence_ms: 500\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#persona-configuration-multi-wake-word","title":"Persona Configuration (Multi-Wake Word)","text":"<pre><code>voice:\n  personas:\n    hey_motoko:\n      name: motoko\n      display_name: Major Kusanagi\n      voice: nova                   # Female voice\n      prompt_path: prompts/personas/motoko.md\n    hey_batou:\n      name: batou\n      display_name: Batou\n      voice: onyx                   # Male voice\n      prompt_path: prompts/personas/batou.md\n\n  default_persona: hey_motoko       # Used before any wake word detected\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#degraded-mode-fallback-behavior","title":"Degraded Mode (Fallback Behavior)","text":"<pre><code>voice:\n  degraded_mode:\n    skip_wake_word_on_failure: true       # Switch to always-listening if wake word fails\n    use_energy_vad_fallback: true         # Use energy-based VAD if Silero unavailable\n    log_response_on_tts_failure: true     # Log response text if TTS fails\n</code></pre> <p>Expected result: Voice pipeline is fully configured for your hardware and preferences.</p>"},{"location":"guides/voice-pipeline-setup/#step-6-set-openai-api-key","title":"Step 6: Set OpenAI API Key","text":"<p>The voice pipeline requires an OpenAI API key with Realtime API access.</p> <pre><code># Set API key in environment\nexport OPENAI_API_KEY=\"sk-proj-your-api-key-here\"\n\n# Or add to .bashrc for persistence\necho 'export OPENAI_API_KEY=\"sk-proj-your-api-key-here\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Verify it's set\necho $OPENAI_API_KEY\n</code></pre> <p>Security note: Never commit API keys to version control. Use environment variables or <code>.env</code> files (excluded from git).</p> <p>Expected result: OpenAI API key is configured and accessible.</p>"},{"location":"guides/voice-pipeline-setup/#step-7-test-the-voice-pipeline","title":"Step 7: Test the Voice Pipeline","text":"<p>Run the voice pipeline with debug logging to verify everything works.</p> <pre><code># Run with voice mode enabled and debug output\nREACHY_DEBUG=1 python -m reachy_agent run --voice\n</code></pre> <p>Expected console output:</p> <pre><code>[INFO] audio_manager_initialized device_count=6\n[INFO] wake_word_detector_initialized model=hey_motoko sensitivity=0.5\n[INFO] voice_pipeline_started default_persona=hey_motoko\n[INFO] listening_for_wake_word state=passive\n</code></pre> <p>Test sequence:</p> <ol> <li>Wake word detection: Say \"Hey Motoko\" clearly into the microphone</li> <li>Expected: <code>[INFO] wake_word_detected model=hey_motoko confidence=0.87</code></li> <li> <p>Expected: Confirmation beep sound</p> </li> <li> <p>Speech input: Speak a question (e.g., \"What time is it?\")</p> </li> <li>Expected: <code>[INFO] speech_detected duration_ms=2150</code></li> <li> <p>Expected: <code>[INFO] processing_speech transcription=\"What time is it?\"</code></p> </li> <li> <p>Response generation: Wait for Claude to generate a response</p> </li> <li> <p>Expected: <code>[INFO] response_received tokens=45</code></p> </li> <li> <p>TTS playback: Listen for voice response through speaker</p> </li> <li>Expected: <code>[INFO] tts_playback_started voice=nova</code></li> <li>Expected: Head wobble animation during speech</li> <li> <p>Expected: <code>[INFO] tts_playback_completed duration_s=3.2</code></p> </li> <li> <p>Return to listening: Pipeline returns to wake word detection</p> </li> <li>Expected: <code>[INFO] listening_for_wake_word state=passive</code></li> </ol> <p>Success criteria: Complete wake-to-response-to-listen cycle works without errors.</p>"},{"location":"guides/voice-pipeline-setup/#verification","title":"Verification","text":"<p>Confirm your voice pipeline is fully operational:</p> <pre><code># Check audio levels during recording\nREACHY_DEBUG=1 python -m reachy_agent run --voice 2&gt;&amp;1 | grep audio_level\n\n# Expected output (when speaking):\n# [DEBUG] audio_level level=0.42 state=speech\n# [DEBUG] audio_level level=0.38 state=speech\n\n# Test wake word sensitivity\npython3 -c \"\nfrom reachy_agent.voice.wake_word import WakeWordDetector\nfrom reachy_agent.config import load_config\n\nconfig = load_config()\ndetector = WakeWordDetector(config.voice.wake_word)\nprint(f'Models loaded: {list(detector._models.keys())}')\n\"\n# Expected output: Models loaded: ['hey_motoko', 'hey_batou']\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/voice-pipeline-setup/#problem-device-or-resource-busy-on-audio-device","title":"Problem: \"Device or resource busy\" on audio device","text":"<p>Symptoms: PyAudio fails to open stream with error \"Device or resource busy\"</p> <p>Cause: Multiple processes trying to access the same audio device without ALSA sharing</p> <p>Solution: Ensure dsnoop/dmix configuration is correct</p> <pre><code># Check if daemon is using the device\nsudo lsof /dev/snd/*\n\n# Kill any conflicting processes\nsudo pkill -f reachy_daemon\n\n# Verify ALSA configuration\ncat ~/.asoundrc\n\n# Test shared access\narecord -D dsnoop_mic -f S16_LE -r 16000 -c 1 -d 3 test.wav &amp;\narecord -D dsnoop_mic -f S16_LE -r 16000 -c 1 -d 3 test2.wav &amp;\n# Both should succeed\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#problem-audio-cuts-off-at-the-beginning-of-tts-playback","title":"Problem: Audio cuts off at the beginning of TTS playback","text":"<p>Symptoms: First syllable or word is missing from TTS output</p> <p>Cause: Speaker takes time to initialize; audio buffer starts playing before speaker is ready</p> <p>Solution: The pipeline includes a built-in lead-in buffer, but you can adjust:</p> <pre><code>voice:\n  audio:\n    playback_lead_in_ms: 200  # Increase if audio still cuts off\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#problem-wake-word-detection-has-too-many-false-positives","title":"Problem: Wake word detection has too many false positives","text":"<p>Symptoms: Wake word triggers on background noise or similar-sounding words</p> <p>Cause: Sensitivity is too high, cooldown is too short</p> <p>Solution: Adjust sensitivity and cooldown</p> <pre><code>voice:\n  wake_word:\n    sensitivity: 0.4            # Lower = more strict (was 0.5)\n    cooldown_seconds: 3.0       # Longer cooldown (was 2.0)\n</code></pre> <p>Test different sensitivity values:</p> <pre><code># Quick sensitivity test\npython3 -c \"\nfrom reachy_agent.voice.wake_word import WakeWordDetector, WakeWordConfig\n\n# Test with strict sensitivity\nconfig = WakeWordConfig(sensitivity=0.3)\ndetector = WakeWordDetector(config)\nprint('Testing strict mode (0.3)...')\n# Speak wake word and observe detection rate\n\"\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#problem-wake-word-detection-misses-valid-detections","title":"Problem: Wake word detection misses valid detections","text":"<p>Symptoms: You say the wake word clearly but it doesn't trigger</p> <p>Cause: Sensitivity is too low, model doesn't match your voice/accent</p> <p>Solution: Increase sensitivity or retrain wake word model</p> <pre><code>voice:\n  wake_word:\n    sensitivity: 0.6            # Higher = more lenient (was 0.5)\n</code></pre> <p>Verify microphone input:</p> <pre><code># Record test audio to check microphone quality\narecord -D dsnoop_mic -f S16_LE -r 16000 -c 1 -d 5 wake_word_test.wav\n\n# Play back to verify clarity\naplay wake_word_test.wav\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#problem-echo-or-feedback-during-conversation","title":"Problem: Echo or feedback during conversation","text":"<p>Symptoms: Robot's voice is picked up by microphone, triggering false detections</p> <p>Cause: Speaker output is too loud or microphone is too sensitive</p> <p>Solution: Reduce speaker volume or enable acoustic echo suppression</p> <pre><code># Reduce ALSA speaker volume\namixer -D hw:2 set Speaker 80%\n\n# Or in config (if implemented):\nvoice:\n  audio:\n    output_volume: 0.8          # 80% volume\n</code></pre> <p>Advanced solution: Implement acoustic echo cancellation (AEC) using software like PulseAudio or custom filters.</p>"},{"location":"guides/voice-pipeline-setup/#problem-high-latency-between-speech-and-response","title":"Problem: High latency between speech and response","text":"<p>Symptoms: 5+ second delay between finishing speech and hearing response</p> <p>Cause: Network latency to OpenAI API, model processing time, or CPU throttling</p> <p>Solution: Optimize configuration and check system resources</p> <pre><code>voice:\n  openai:\n    model: gpt-4o-mini-realtime  # Faster model (if available)\n    max_response_tokens: 512     # Shorter responses (was 4096)\n</code></pre> <p>Check Raspberry Pi thermal throttling:</p> <pre><code># Monitor temperature during operation\nvcgencmd measure_temp\n\n# Check for throttling\nvcgencmd get_throttled\n# 0x0 = no throttling (good)\n# Non-zero = throttling active\n</code></pre> <p>Improve thermal performance: - Add heatsink to Raspberry Pi - Ensure adequate ventilation - Reduce CPU load by disabling unnecessary services</p>"},{"location":"guides/voice-pipeline-setup/#problem-importerror-no-module-named-pyaudio","title":"Problem: \"ImportError: No module named 'pyaudio'\"","text":"<p>Symptoms: Voice pipeline fails to start with PyAudio import error</p> <p>Cause: PyAudio not installed or missing system dependencies</p> <p>Solution: Install PyAudio and dependencies</p> <pre><code># Install system dependencies\nsudo apt-get update\nsudo apt-get install portaudio19-dev python3-pyaudio\n\n# Install Python package\nuv pip install pyaudio\n\n# Or reinstall voice extras\nuv pip install -e \".[voice]\"\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#problem-openai-api-key-not-found","title":"Problem: OpenAI API key not found","text":"<p>Symptoms: Error message \"openai_api_key_missing\" in logs</p> <p>Cause: OPENAI_API_KEY environment variable not set</p> <p>Solution: Set the environment variable</p> <pre><code># Temporary (current session only)\nexport OPENAI_API_KEY=\"sk-proj-your-key-here\"\n\n# Permanent (add to .bashrc)\necho 'export OPENAI_API_KEY=\"sk-proj-your-key-here\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Verify\nenv | grep OPENAI\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#raspberry-pi-specific-setup","title":"Raspberry Pi Specific Setup","text":""},{"location":"guides/voice-pipeline-setup/#usb-audio-device-indices","title":"USB Audio Device Indices","text":"<p>On Raspberry Pi 4 with Reachy Mini USB Audio:</p> <pre><code># Typical device configuration:\n# Card 0: bcm2835 (onboard)\n# Card 1: bcm2835 (onboard HDMI)\n# Card 2: USB Audio Device (Reachy's 4-mic array + speaker)\n\n# PyAudio indices (after dsnoop/dmix setup):\n# Index 3: dmix_speaker (output)\n# Index 4: dsnoop_mic (input)\n</code></pre> <p>Verify your specific indices:</p> <pre><code>python3 -c \"\nimport pyaudio\np = pyaudio.PyAudio()\nprint('Input devices:')\nfor i in range(p.get_device_count()):\n    info = p.get_device_info_by_index(i)\n    if info['maxInputChannels'] &gt; 0:\n        print(f'  {i}: {info[\\\"name\\\"]}')\nprint('Output devices:')\nfor i in range(p.get_device_count()):\n    info = p.get_device_info_by_index(i)\n    if info['maxOutputChannels'] &gt; 0:\n        print(f'  {i}: {info[\\\"name\\\"]}')\np.terminate()\n\"\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#thermal-management","title":"Thermal Management","text":"<p>Voice processing is CPU-intensive. Monitor and manage thermals:</p> <pre><code># Install monitoring tools\nsudo apt-get install libraspberrypi-bin\n\n# Check current temperature\nvcgencmd measure_temp\n\n# Monitor continuously (Ctrl+C to stop)\nwatch -n 1 vcgencmd measure_temp\n\n# Check throttling status\nvcgencmd get_throttled\n</code></pre> <p>Temperature guidelines: - &lt; 60\u00b0C: Normal operation - 60-70\u00b0C: Warm but acceptable - 70-80\u00b0C: Consider cooling improvements - &gt; 80\u00b0C: CPU will throttle (performance degradation)</p> <p>Cooling improvements: - Install heatsink on Raspberry Pi SoC - Add active cooling fan - Improve case ventilation - Reduce room temperature</p>"},{"location":"guides/voice-pipeline-setup/#systemd-service-configuration","title":"Systemd Service Configuration","text":"<p>Run the voice pipeline as a system service for automatic startup:</p> <pre><code># Create service file\nsudo nano /etc/systemd/system/reachy-voice.service\n</code></pre> <p>Add this configuration:</p> <pre><code>[Unit]\nDescription=Reachy Voice Pipeline\nAfter=network.target reachy-daemon.service\nWants=reachy-daemon.service\n\n[Service]\nType=simple\nUser=reachy\nGroup=reachy\nWorkingDirectory=/home/reachy/reachy_project\nEnvironment=\"OPENAI_API_KEY=sk-proj-your-key-here\"\nEnvironment=\"REACHY_DEBUG=0\"\nExecStart=/home/reachy/reachy_project/.venv/bin/python -m reachy_agent run --voice\nRestart=on-failure\nRestartSec=10\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start the service:</p> <pre><code># Reload systemd\nsudo systemctl daemon-reload\n\n# Enable service (start on boot)\nsudo systemctl enable reachy-voice\n\n# Start service now\nsudo systemctl start reachy-voice\n\n# Check status\nsudo systemctl status reachy-voice\n\n# View logs\nsudo journalctl -u reachy-voice -f\n</code></pre> <p>Service management commands:</p> <pre><code># Stop service\nsudo systemctl stop reachy-voice\n\n# Restart service\nsudo systemctl restart reachy-voice\n\n# Disable autostart\nsudo systemctl disable reachy-voice\n\n# View recent logs\nsudo journalctl -u reachy-voice -n 100\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#alternative-approaches","title":"Alternative Approaches","text":""},{"location":"guides/voice-pipeline-setup/#for-limited-internet-connectivity","title":"For Limited Internet Connectivity","text":"<p>If you have unreliable internet or want to reduce API costs, consider local alternatives:</p> <p>Approach: Use local Whisper STT + Piper TTS instead of OpenAI Realtime API</p> <p>Pros: - No internet required after model download - No API costs - Better privacy (data stays on device)</p> <p>Cons: - Higher latency (separate STT/LLM/TTS calls) - Requires more storage and RAM - Potentially lower quality</p> <p>When to use: Offline demos, privacy-critical applications, or high-volume usage</p> <p>Configuration:</p> <pre><code>voice:\n  stt_backend: whisper_local  # Instead of openai_realtime\n  tts_backend: piper          # Local TTS\n</code></pre>"},{"location":"guides/voice-pipeline-setup/#for-high-volume-usage","title":"For High-Volume Usage","text":"<p>For applications requiring many hours of conversation per day:</p> <p>Approach: Use cheaper STT/TTS services (Azure, Google Cloud)</p> <p>Trade-offs: - Lower cost per hour - May require additional API setup - Different voice quality characteristics</p> <p>When to use: Production deployments, extended conversations, cost-sensitive applications</p>"},{"location":"guides/voice-pipeline-setup/#for-custom-wake-words","title":"For Custom Wake Words","text":"<p>If bundled wake words don't fit your use case:</p> <p>Approach: Train custom wake word models with OpenWakeWord</p> <p>Pros: - Exact phrase matching your needs - Brand-specific wake words - Better accuracy for unusual names/phrases</p> <p>Cons: - Requires training data collection - Time investment for model training - May need multiple training iterations</p> <p>When to use: Commercial products, non-English languages, brand-specific wake words</p>"},{"location":"guides/voice-pipeline-setup/#best-practices","title":"Best Practices","text":"<ul> <li>Audio Quality: Use a quality USB audio device with noise cancellation for best results</li> <li>Wake Word Choice: Choose wake words with 3+ syllables and distinct phonemes (avoid common words)</li> <li>Testing: Always test with different speakers, accents, and background noise levels</li> <li>Monitoring: Enable debug logging during initial setup, then disable for production</li> <li>Security: Never commit API keys to version control; use environment variables or secrets management</li> <li>Performance: Monitor CPU and memory usage; adjust config for available resources</li> <li>Fallbacks: Enable degraded mode settings for graceful handling of component failures</li> <li>Updates: Keep wake word models updated; retrain if detection accuracy degrades</li> </ul> <p>Important: The voice pipeline shares audio hardware with the Reachy daemon. Always use dsnoop/dmix ALSA configuration to prevent resource conflicts.</p>"},{"location":"guides/voice-pipeline-setup/#related-tasks","title":"Related Tasks","text":"<ul> <li>Raspberry Pi Installation - Initial Reachy setup</li> <li>Troubleshooting Guide - General debugging procedures</li> </ul>"},{"location":"guides/voice-pipeline-setup/#further-reading","title":"Further Reading","text":"<ul> <li>New to Reachy Agent? Start with Getting Started Tutorial</li> <li>Need technical details? Check the MCP Tools Reference</li> <li>Want to understand agent behavior? Read Agent Behavior Guide</li> <li>OpenAI Realtime API: Official Documentation</li> <li>OpenWakeWord: GitHub Repository</li> </ul>"},{"location":"planning/EPCC_CODE/","title":"Implementation: Reachy Agent Phase 1 - Foundation","text":"<p>Mode: Default | Date: 2024-12-20 | Status: Complete</p>"},{"location":"planning/EPCC_CODE/#1-changes-29-files-2200-lines-80-target-coverage","title":"1. Changes (29 files, +2200 lines, ~80% target coverage)","text":""},{"location":"planning/EPCC_CODE/#created","title":"Created","text":"<ul> <li><code>src/reachy_agent/__init__.py</code> - Package root with version</li> <li><code>src/reachy_agent/main.py</code> - CLI entry point with typer</li> <li><code>src/reachy_agent/utils/config.py</code> - Pydantic config models from TECH_REQ.md schemas</li> <li><code>src/reachy_agent/utils/logging.py</code> - structlog JSON logging infrastructure</li> <li><code>src/reachy_agent/mcp_servers/reachy/server.py</code> - MCP server with 8 body control tools</li> <li><code>src/reachy_agent/mcp_servers/reachy/daemon_client.py</code> - HTTP client for Reachy daemon</li> <li><code>src/reachy_agent/mcp_servers/reachy/daemon_mock.py</code> - FastAPI mock for simulation</li> <li><code>src/reachy_agent/permissions/tiers.py</code> - 4-tier permission model with evaluator</li> <li><code>src/reachy_agent/permissions/hooks.py</code> - PreToolUse/PostToolUse enforcement hooks</li> <li><code>src/reachy_agent/agent/options.py</code> - Claude Agent SDK options builder</li> <li><code>src/reachy_agent/agent/loop.py</code> - Main agent loop (simulation stub for Phase 1)</li> <li><code>config/default.yaml</code> - Default configuration</li> <li><code>config/permissions.yaml</code> - Permission tier rules</li> <li><code>config/CLAUDE.md</code> - Personality system prompt</li> <li><code>.env.example</code> - Environment template</li> <li><code>pyproject.toml</code> - Project metadata and tool configuration</li> <li><code>requirements.txt</code>, <code>requirements-dev.txt</code> - Dependencies</li> </ul>"},{"location":"planning/EPCC_CODE/#tests","title":"Tests","text":"<ul> <li><code>tests/conftest.py</code> - Shared fixtures</li> <li><code>tests/unit/test_config.py</code> - Config loading tests</li> <li><code>tests/unit/test_permissions.py</code> - Permission tier tests</li> <li><code>tests/unit/test_mcp_server.py</code> - MCP server tests</li> </ul>"},{"location":"planning/EPCC_CODE/#2-quality-tests-pending-run-security-review-needed-docs-complete","title":"2. Quality (Tests: Pending run | Security: Review needed | Docs: Complete)","text":"<p>Tests: 30+ test cases covering permissions, config, MCP tools - Permission tier matching (wildcard patterns) - Config YAML loading/saving - MCP tool validation (bounds checking) - Daemon client mocking</p> <p>Docs: README.md, CLAUDE.md, inline docstrings with Google style</p>"},{"location":"planning/EPCC_CODE/#3-decisions","title":"3. Decisions","text":"<p>Single asyncio process: Chose layered monolith over microservices for MVP - Why: Simpler debugging, lower memory, matches TECH_REQ recommendation - Alt: systemd services would provide fault isolation but add complexity</p> <p>FastMCP for MCP server: Used high-level SDK instead of low-level handlers - Why: Cleaner tool definitions with decorators, auto-schema generation - Alt: Low-level Server class offers more control but more boilerplate</p> <p>Mock daemon for Phase 1: Simulates hardware without physical robot - Why: Enables development before hardware arrives (per EPCC_PLAN.md) - Alt: Wait for hardware, but would block Phase 1 completion</p> <p>Permission evaluation order: First matching rule wins - Why: Allows broad rules with specific overrides - Trade-off: Order-dependent, must be documented</p>"},{"location":"planning/EPCC_CODE/#4-handoff","title":"4. Handoff","text":"<p>Run: <code>/epcc-commit</code> when ready</p> <p>Blockers: None - Phase 1 foundation complete</p> <p>Phase 2 Prerequisites: - Install dependencies: <code>uv pip install -r requirements-dev.txt</code> - Run tests: <code>pytest -v</code> - Run with mock: <code>python -m reachy_agent run --mock</code></p> <p>Deferred to Phase 2+: - Full Claude Agent SDK integration (requires API key) - Wake word detection (OpenWakeWord) - Hardware integration (Raspberry Pi) - Memory system (ChromaDB)</p>"},{"location":"planning/EPCC_CODE/#context-used","title":"Context Used","text":"<p>Planning: EPCC_PLAN.md Phase 1 tasks (1.1-1.5) Tech: TECH_REQ.md JSON schemas for config, permissions, MCP tools Patterns: FastMCP decorator pattern, Pydantic BaseModel, structlog processors</p>"},{"location":"planning/EPCC_PLAN/","title":"Plan: Reachy Agent - Embodied AI System","text":"<p>Created: December 2024 | Effort: ~200h across 4 phases | Complexity: Complex</p>"},{"location":"planning/EPCC_PLAN/#1-objective","title":"1. Objective","text":"<p>Goal: Transform Reachy Mini into an autonomous Claude-powered AI agent with perception, memory, expression, and external service integration.</p> <p>Why: Create an open-source reference implementation of embodied AI that bridges cloud intelligence with physical robotics, while documenting the journey for the developer community.</p> <p>Success Criteria: 1. Agent responds to \"Hey Reachy\" wake word and executes multi-step tasks via MCP 2. Reliable 8+ hour continuous operation with &lt;1% crash rate 3. 5+ working MCP integrations (Reachy body, Home Assistant, Calendar, GitHub, Weather) 4. Graceful degradation when offline (Ollama + Piper fallback functional) 5. Privacy indicators via antenna states visible to user</p>"},{"location":"planning/EPCC_PLAN/#2-approach","title":"2. Approach","text":""},{"location":"planning/EPCC_PLAN/#architecture-summary","title":"Architecture Summary","text":"<p>From TECH_REQ.md: Layered monolith with MCP sidecar pattern, single asyncio process for MVP.</p> <pre><code>Claude API (cloud)\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Claude Agent SDK         \u2502\n\u2502  (Agent Loop + Hooks)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 MCP Protocol\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Reachy Agent Core        \u2502\n\u2502  Perception\u2502Memory\u2502Privacy   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 HTTP :8000\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Reachy Daemon (Pollen)   \u2502\n\u2502  Motors\u2502Camera\u2502Audio\u2502IMU     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"planning/EPCC_PLAN/#key-technology-decisions-from-tech_reqmd","title":"Key Technology Decisions (from TECH_REQ.md)","text":"Component Choice Rationale Wake word OpenWakeWord Open source, no license, customizable Memory ChromaDB + SQLite Lightweight, persistent, Python native Embeddings Hybrid (local + API) Real-time local, quality from reindexing Offline LLM Ollama + Llama 3.2 3B Easy setup, fits Pi RAM TTS fallback Piper Neural quality, Pi optimized Config YAML + JSON Schema Human-readable, machine-validatable Process Single asyncio Lower memory, simpler debugging"},{"location":"planning/EPCC_PLAN/#integration-points","title":"Integration Points","text":"<ul> <li>Reachy Daemon (Pollen): HTTP API on localhost:8000</li> <li>Claude API: HTTPS to api.anthropic.com</li> <li>Home Assistant: REST API via MCP server</li> <li>Google Calendar: OAuth2 + Calendar API</li> <li>GitHub: Personal access token + REST API</li> </ul>"},{"location":"planning/EPCC_PLAN/#trade-offs-made","title":"Trade-offs Made","text":"Decision Chosen Alternative Rationale Process model Single asyncio systemd services Simpler for MVP, can migrate later Config format YAML JSON Human-readable for users Wake word OpenWakeWord Porcupine No license cost, open source"},{"location":"planning/EPCC_PLAN/#3-tasks","title":"3. Tasks","text":""},{"location":"planning/EPCC_PLAN/#phase-1-foundation-50h","title":"Phase 1: Foundation (~50h)","text":"<p>Goal: Validate architecture in simulation before hardware arrives</p>"},{"location":"planning/EPCC_PLAN/#11-project-scaffolding-8h","title":"1.1 Project Scaffolding (~8h)","text":"Task Effort Description Deps Risk 1.1.1 Create project structure 2h Set up <code>src/</code>, <code>config/</code>, <code>tests/</code>, <code>scripts/</code> per TECH_REQ directory layout None L 1.1.2 Configure pyproject.toml 1h Dependencies, dev tools, entry points 1.1.1 L 1.1.3 Create requirements files 1h Split prod/dev, pin versions 1.1.2 L 1.1.4 Set up config schema 2h JSON Schema for config validation, Pydantic models 1.1.1 L 1.1.5 Create .env.example 0.5h Template for API keys 1.1.1 L 1.1.6 Set up logging infrastructure 1.5h structlog, JSON format, file rotation 1.1.1 L"},{"location":"planning/EPCC_PLAN/#12-reachy-mcp-server-16h","title":"1.2 Reachy MCP Server (~16h)","text":"Task Effort Description Deps Risk 1.2.1 Create MCP server skeleton 3h Use <code>mcp</code> SDK, <code>create_sdk_mcp_server</code> 1.1.1 M 1.2.2 Implement move_head tool 2h Direction enum, speed control, daemon HTTP call 1.2.1 M 1.2.3 Implement speak tool 2h Text-to-speech via daemon, queue management 1.2.1 M 1.2.4 Implement play_emotion tool 3h Emotion sequences, antenna coordination 1.2.1 M 1.2.5 Implement capture_image tool 2h Camera frame capture, optional analysis 1.2.1 M 1.2.6 Implement remaining body tools 3h set_antenna_state, get_sensor_data, look_at_sound, dance 1.2.2-1.2.5 L 1.2.7 Create daemon mock for testing 1h FastAPI mock of Reachy daemon for local dev 1.2.1 L"},{"location":"planning/EPCC_PLAN/#13-claude-agent-sdk-integration-12h","title":"1.3 Claude Agent SDK Integration (~12h)","text":"Task Effort Description Deps Risk 1.3.1 Install and configure SDK 2h ARM64 considerations, validate on dev machine 1.1.2 M 1.3.2 Create agent options module 2h ClaudeAgentOptions, model selection, permissions 1.3.1 L 1.3.3 Implement main agent loop 3h Perceive \u2192 Think \u2192 Act cycle, asyncio integration 1.3.2 M 1.3.4 Register MCP server with agent 2h In-process MCP connection, tool discovery 1.2.1, 1.3.3 M 1.3.5 Create CLAUDE.md personality 2h System prompt per Appendix A in PRD 1.3.3 L 1.3.6 Implement context building 1h Inject current time, state, memories into prompt 1.3.5 L"},{"location":"planning/EPCC_PLAN/#14-permission-system-8h","title":"1.4 Permission System (~8h)","text":"Task Effort Description Deps Risk 1.4.1 Create permission tier models 2h Pydantic models from TECH_REQ schema 1.1.4 L 1.4.2 Implement PreToolUse hook 3h Pattern matching, tier evaluation 1.4.1, 1.3.3 M 1.4.3 Create permissions.yaml config 1h Default rules per TECH_REQ 1.4.1 L 1.4.4 Implement permission audit logging 2h SQLite, ToolExecution schema 1.4.2 L"},{"location":"planning/EPCC_PLAN/#15-simulation-testing-6h","title":"1.5 Simulation Testing (~6h)","text":"Task Effort Description Deps Risk 1.5.1 Set up MuJoCo environment 2h Install MuJoCo, Reachy model None M 1.5.2 Create simulation adapter 2h Bridge MCP server to MuJoCo instead of daemon 1.2.7, 1.5.1 M 1.5.3 Validate end-to-end in simulation 2h Multi-turn conversation, tool execution 1.5.2, 1.3.4 M <p>Phase 1 Exit Criteria: - [x] Agent controls simulated Reachy via MCP - [x] Multi-turn conversations work - [x] Permissions enforced (Tier 1 auto, Tier 4 blocked)</p> <p>Phase 1 Status: \u2705 COMPLETE (December 2024)</p>"},{"location":"planning/EPCC_PLAN/#phase-2-hardware-integration-50h","title":"Phase 2: Hardware Integration (~50h)","text":"<p>Goal: Running on physical Reachy Mini with voice activation</p>"},{"location":"planning/EPCC_PLAN/#21-pi-environment-setup-8h","title":"2.1 Pi Environment Setup (~8h)","text":"Task Effort Description Deps Risk 2.1.1 Install Raspberry Pi OS 1h 64-bit, configure WiFi, SSH None L 2.1.2 Install Reachy daemon 2h Pollen SDK, verify daemon startup 2.1.1 M 2.1.3 Install Claude Code CLI 2h Native installer, ARM64 validation 2.1.1 M 2.1.4 Install Python dependencies 2h uv, requirements, verify ARM64 wheels 2.1.3 M 2.1.5 Configure systemd service 1h reachy-agent.service per TECH_REQ 2.1.4 L"},{"location":"planning/EPCC_PLAN/#22-wake-word-detection-10h","title":"2.2 Wake Word Detection (~10h)","text":"Task Effort Description Deps Risk 2.2.1 Install OpenWakeWord 2h pip install, model download 2.1.4 M 2.2.2 Create audio capture module 3h PyAudio, 4-mic array handling 2.1.4 M 2.2.3 Implement wake word detector 3h \"Hey Reachy\" detection, callback 2.2.1, 2.2.2 M 2.2.4 Tune sensitivity 2h False positive &lt; 1/hour, latency &lt; 500ms 2.2.3 M"},{"location":"planning/EPCC_PLAN/#23-attention-state-machine-8h","title":"2.3 Attention State Machine (~8h)","text":"Task Effort Description Deps Risk 2.3.1 Create attention state model 2h Passive/Alert/Engaged enum, transitions 1.1.1 L 2.3.2 Implement passive mode 2h Wake word only, minimal CPU 2.2.3, 2.3.1 L 2.3.3 Implement alert mode 2h Motion/face detection triggers 2.3.2 M 2.3.4 Implement engaged mode 2h Full agent loop, timeout to alert 2.3.3 L"},{"location":"planning/EPCC_PLAN/#24-privacy-indicators-4h","title":"2.4 Privacy Indicators (~4h)","text":"Task Effort Description Deps Risk 2.4.1 Map antenna positions to states 1h down=passive, mid=alert, up=engaged 1.2.4 L 2.4.2 Implement indicator controller 2h Async antenna updates on state change 2.3.1, 2.4.1 L 2.4.3 Add smooth transitions 1h Easing functions, no jarring movements 2.4.2 L"},{"location":"planning/EPCC_PLAN/#25-graceful-degradation-12h","title":"2.5 Graceful Degradation (~12h)","text":"Task Effort Description Deps Risk 2.5.1 Create health monitor 3h CPU temp, memory, API latency checks 2.1.4 M 2.5.2 Implement degradation modes 3h Full \u2192 Offline \u2192 Thermal \u2192 Safe 2.5.1 M 2.5.3 Install Piper TTS 2h Local neural TTS for offline speech 2.1.4 L 2.5.4 Install Ollama + Llama 3.2 3h Local LLM for offline mode 2.1.4 M 2.5.5 Implement mode switching 1h Detect network loss, switch to fallbacks 2.5.2-2.5.4 M"},{"location":"planning/EPCC_PLAN/#26-hardware-validation-8h","title":"2.6 Hardware Validation (~8h)","text":"Task Effort Description Deps Risk 2.6.1 Test head movement range 2h Verify all 6 DOF, speed control 1.2.2 M 2.6.2 Test audio pipeline 2h Wake word \u2192 STT \u2192 agent \u2192 TTS 2.2.3, 1.3.3 M 2.6.3 Test camera capture 1h Verify capture_image tool 1.2.5 L 2.6.4 Stress test 8-hour operation 3h Monitor crash rate, memory leaks All Phase 2 H <p>Phase 2 Exit Criteria: - [ ] Voice-activated agent on physical robot - [ ] Reliable 8-hour operation - [ ] Graceful handling of WiFi disconnection</p>"},{"location":"planning/EPCC_PLAN/#phase-3-intelligence-layer-50h","title":"Phase 3: Intelligence Layer (~50h)","text":"<p>Goal: Rich perception, memory, and expression</p>"},{"location":"planning/EPCC_PLAN/#31-memory-system-16h","title":"3.1 Memory System (~16h)","text":"Task Effort Description Deps Risk 3.1.1 Install ChromaDB 1h pip install, persistent storage path 2.1.4 L 3.1.2 Create memory models 2h Memory, MemoryMetadata per TECH_REQ schema 1.1.4 L 3.1.3 Implement short-term memory 2h Session buffer, last N interactions 3.1.2 L 3.1.4 Implement long-term memory 4h ChromaDB storage, semantic retrieval 3.1.1, 3.1.2 M 3.1.5 Install sentence-transformers 1h all-MiniLM-L6-v2 model 2.1.4 L 3.1.6 Implement hybrid embedding 3h Local for real-time, API for reindexing 3.1.5 M 3.1.7 Integrate memory with agent 3h Context building from relevant memories 3.1.4, 1.3.6 M"},{"location":"planning/EPCC_PLAN/#32-spatial-audio-10h","title":"3.2 Spatial Audio (~10h)","text":"Task Effort Description Deps Risk 3.2.1 Install pyroomacoustics 1h DOA estimation library 2.1.4 L 3.2.2 Calibrate mic positions 2h Map physical array to algorithm 3.2.1, 2.2.2 M 3.2.3 Implement DOA estimation 3h Direction of arrival, \u00b115\u00b0 accuracy 3.2.2 M 3.2.4 Implement look_at_sound 2h Map DOA to head movement 3.2.3, 1.2.2 M 3.2.5 Multi-speaker tracking 2h Track active speaker in room 3.2.3 H"},{"location":"planning/EPCC_PLAN/#33-imu-interaction-8h","title":"3.3 IMU Interaction (~8h)","text":"Task Effort Description Deps Risk 3.3.1 Read IMU via daemon API 2h Accelerometer sampling at 50Hz 2.1.2 L 3.3.2 Implement tap detection 2h Threshold-based event detection 3.3.1 M 3.3.3 Implement pickup detection 2h Sustained acceleration change 3.3.1 M 3.3.4 Create IMU event responses 2h Agent reactions to physical touch 3.3.2, 3.3.3 L"},{"location":"planning/EPCC_PLAN/#34-antenna-expression-language-10h","title":"3.4 Antenna Expression Language (~10h)","text":"Task Effort Description Deps Risk 3.4.1 Create expression schema 2h YAML format per TECH_REQ Expression schema 1.1.4 L 3.4.2 Define 10+ expressions 2h happy, sad, curious, thinking, etc. 3.4.1 L 3.4.3 Implement expression engine 3h Play sequences, blend expressions 3.4.2 M 3.4.4 Add easing functions 1h Natural motion curves 3.4.3 L 3.4.5 Integrate with agent emotions 2h Auto-expression based on response tone 3.4.3, 1.3.3 M"},{"location":"planning/EPCC_PLAN/#35-personality-engine-6h","title":"3.5 Personality Engine (~6h)","text":"Task Effort Description Deps Risk 3.5.1 Create personality state model 2h Mood, energy per TECH_REQ schema 1.1.4 L 3.5.2 Implement state persistence 2h SQLite storage, load on startup 3.5.1 L 3.5.3 Inject personality into prompt 2h Dynamic CLAUDE.md context section 3.5.2, 1.3.5 L <p>Phase 3 Exit Criteria: - [ ] Robot remembers context across sessions - [ ] Responds to physical interaction (tap, pickup) - [ ] Expressive antenna behavior matches conversation tone</p>"},{"location":"planning/EPCC_PLAN/#phase-4-polish-extensibility-50h","title":"Phase 4: Polish &amp; Extensibility (~50h)","text":"<p>Goal: Ready for users and community</p>"},{"location":"planning/EPCC_PLAN/#41-setup-wizard-10h","title":"4.1 Setup Wizard (~10h)","text":"Task Effort Description Deps Risk 4.1.1 Install Rich library 0.5h Terminal UI framework 2.1.4 L 4.1.2 Create wizard flow 3h Welcome, API key, integrations, test 4.1.1 L 4.1.3 Implement API key validation 1h Test Claude API connectivity 4.1.2 L 4.1.4 Implement hardware tests 2h Test motors, camera, audio 4.1.2 M 4.1.5 Generate config from wizard 2h Write config.yaml from answers 4.1.2 L 4.1.6 Create install.sh script 1.5h One-line install for Pi All previous L"},{"location":"planning/EPCC_PLAN/#42-external-mcp-integrations-18h","title":"4.2 External MCP Integrations (~18h)","text":"Task Effort Description Deps Risk 4.2.1 Implement Weather MCP 2h Simple read-only, OpenWeather API 1.2.1 L 4.2.2 Implement Home Assistant MCP 4h Entity control, state reading 1.2.1 M 4.2.3 Implement Google Calendar MCP 4h OAuth2 flow, event reading, creation 1.2.1 M 4.2.4 Implement GitHub MCP 4h Notifications, PR status, issues 1.2.1 M 4.2.5 Create integration registry 2h Dynamic MCP server loading 4.2.1-4.2.4 L 4.2.6 Add permissions for integrations 2h Per-integration tier config 4.2.5, 1.4.1 L"},{"location":"planning/EPCC_PLAN/#43-testing-quality-12h","title":"4.3 Testing &amp; Quality (~12h)","text":"Task Effort Description Deps Risk 4.3.1 Write unit tests 4h 80% coverage target, pytest All modules L 4.3.2 Write integration tests 4h MCP \u2192 daemon mock, agent loop 4.3.1 M 4.3.3 Create test fixtures 2h conftest.py, mock responses 4.3.1 L 4.3.4 Set up CI pipeline 2h GitHub Actions, lint + test 4.3.1-4.3.3 L"},{"location":"planning/EPCC_PLAN/#44-documentation-10h","title":"4.4 Documentation (~10h)","text":"Task Effort Description Deps Risk 4.4.1 Write README.md 2h Quick start, architecture overview All L 4.4.2 Write setup guide 2h Detailed installation for Pi 4.1.6 L 4.4.3 Document permissions system 2h Tier explanation, customization 1.4.3 L 4.4.4 Document expression library 2h How to create custom expressions 3.4.2 L 4.4.5 Create troubleshooting guide 2h Common issues, solutions All L <p>Phase 4 Exit Criteria: - [ ] New user can set up in 30 minutes using wizard - [ ] 3+ external services connected (Home Assistant, Calendar, GitHub) - [ ] 80% test coverage - [ ] Public documentation complete</p>"},{"location":"planning/EPCC_PLAN/#4-quality-strategy","title":"4. Quality Strategy","text":""},{"location":"planning/EPCC_PLAN/#test-coverage","title":"Test Coverage","text":"Level Target Focus Areas Unit 80% MCP tools, permission hooks, memory system Integration Key paths Agent loop \u2192 MCP \u2192 daemon mock E2E (simulation) Happy paths Multi-turn conversation, tool execution E2E (hardware) Manual Voice activation, expression, degradation"},{"location":"planning/EPCC_PLAN/#key-test-cases","title":"Key Test Cases","text":"<ol> <li>Wake word detection: \"Hey Reachy\" activates within 500ms</li> <li>Permission enforcement: Tier 4 tools always blocked</li> <li>Memory retrieval: Relevant memories returned for context</li> <li>Degradation: WiFi disconnect triggers Ollama fallback</li> <li>Expression sync: Antenna state matches attention level</li> </ol>"},{"location":"planning/EPCC_PLAN/#acceptance-criteria-from-prd","title":"Acceptance Criteria (from PRD)","text":"<ul> <li>G1: Voice response, multi-step tasks via MCP, context maintained</li> <li>G2: &lt;1% crash rate, 8+ hour continuous operation</li> <li>G3: 5+ MCP integrations working</li> <li>G5: Antenna state visibly indicates listening mode</li> </ul>"},{"location":"planning/EPCC_PLAN/#5-risks","title":"5. Risks","text":"Risk Impact Likelihood Mitigation ARM64 Claude SDK issues H M Test early (1.3.1), have Docker fallback Thermal throttling on Pi M M Health monitor (2.5.1), heatsink, throttle detection OpenWakeWord accuracy M M Tune sensitivity (2.2.4), add confirmation option ChromaDB corruption H L Regular backups, external SSD Reachy daemon API changes H L Pin SDK version, integration tests Memory leaks in 8h run M M Profile during stress test (2.6.4), fix before Phase 3"},{"location":"planning/EPCC_PLAN/#assumptions","title":"Assumptions","text":"<ol> <li>Reachy Mini hardware arrives before Phase 2 starts</li> <li>Claude Agent SDK stable for ARM64 (v0.2.114+ or native installer)</li> <li>Reachy daemon provides stable HTTP API at localhost:8000</li> <li>4GB Pi RAM sufficient with memory budgets from TECH_REQ</li> </ol>"},{"location":"planning/EPCC_PLAN/#out-of-scope-deferred","title":"Out of Scope (Deferred)","text":"<ul> <li>Web dashboard (P2 feature, post v1.0)</li> <li>Slack/Spotify integrations (v1.0, not MVP)</li> <li>Multi-robot coordination (v2.0+)</li> <li>LeRobot integration (v3.0+)</li> </ul>"},{"location":"planning/EPCC_PLAN/#summary","title":"Summary","text":"Phase Focus Effort Key Deliverable 1 Foundation ~50h Agent controls simulated Reachy via MCP 2 Hardware ~50h Voice-activated agent on physical robot 3 Intelligence ~50h Memory, spatial audio, expressions 4 Polish ~50h Setup wizard, integrations, docs Total ~200h Production-ready embodied AI agent <p>Plan Status: Draft - Awaiting Approval</p> <p>Ready for <code>/epcc-code</code> when approved.</p>"},{"location":"planning/PRD/","title":"Product Requirements Document: Reachy Mini Embodied AI Agent","text":"<p>Project Name: Reachy Agent Author: Jawhny Cooke Version: 1.0 Last Updated: December 2024 Status: Planning</p>"},{"location":"planning/PRD/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Executive Summary</li> <li>Problem Statement</li> <li>Goals &amp; Success Criteria</li> <li>Target Users</li> <li>Product Overview</li> <li>Technical Architecture</li> <li>Feature Requirements</li> <li>Non-Functional Requirements</li> <li>Implementation Phases</li> <li>Content Strategy</li> <li>Risks &amp; Mitigations</li> <li>References</li> </ol>"},{"location":"planning/PRD/#1-executive-summary","title":"1. Executive Summary","text":""},{"location":"planning/PRD/#vision","title":"Vision","text":"<p>Transform Reachy Mini from a programmable desktop robot into an autonomous AI agent with physical presence. By installing the Claude Agent SDK directly on Reachy's Raspberry Pi 4, the robot becomes a Claude agent that can perceive its environment, make decisions, and interact with both the physical and digital world through Model Context Protocol (MCP) servers.</p>"},{"location":"planning/PRD/#value-proposition","title":"Value Proposition","text":"<ul> <li>For developers: An open-source reference implementation of embodied AI that bridges cloud intelligence with physical robotics</li> <li>For the community: Educational content documenting the journey from unboxing to autonomous agent</li> <li>For the ecosystem: A demonstration of Claude Agent SDK + MCP capabilities in a novel form factor</li> </ul>"},{"location":"planning/PRD/#unique-differentiation","title":"Unique Differentiation","text":"Competitor Limitation Reachy Agent Advantage Anki Vector Closed ecosystem, discontinued Open source, actively developed Amazon Astro Home robot, consumer-focused Desktop form factor, developer-first EMO Robot Scripted behaviors Real AI decision-making DIY chatbots No physical presence Embodied with expressions, gestures, spatial awareness"},{"location":"planning/PRD/#2-problem-statement","title":"2. Problem Statement","text":""},{"location":"planning/PRD/#current-state","title":"Current State","text":"<p>Desktop robots exist. AI assistants exist. But the intersection\u2014an open-source, developer-friendly desktop robot powered by frontier AI models\u2014remains underexplored. Existing solutions are either:</p> <ol> <li>Closed ecosystems (Vector, Jibo) that died when companies pivoted</li> <li>Consumer products (Astro) not designed for developer extensibility</li> <li>Toy-grade (EMO) with scripted rather than intelligent behaviors</li> <li>Disembodied (Claude, GPT) lacking physical presence and world interaction</li> </ol>"},{"location":"planning/PRD/#opportunity","title":"Opportunity","text":"<p>Reachy Mini, created by Pollen Robotics and Hugging Face, provides the hardware foundation. The Claude Agent SDK provides the intelligence layer. MCP provides the extensibility framework. The opportunity is to connect these pieces into a cohesive, documented, reproducible system that others can learn from and build upon.</p>"},{"location":"planning/PRD/#target-outcome","title":"Target Outcome","text":"<p>A desktop robot that: - Knows your calendar and reminds you of meetings with physical gestures - Monitors your GitHub repos and reacts to CI failures - Controls your smart home while physically pointing toward devices - Maintains personality consistency across sessions - Works offline when connectivity fails - Respects privacy with transparent data handling</p>"},{"location":"planning/PRD/#3-goals-success-criteria","title":"3. Goals &amp; Success Criteria","text":""},{"location":"planning/PRD/#primary-goals","title":"Primary Goals","text":"Goal Success Metric G1: Functional autonomous agent Reachy responds to voice, executes multi-step tasks via MCP, maintains conversation context G2: Reliable operation &lt;1% crash rate, graceful degradation on failures, 8+ hours continuous operation G3: Extensible platform 5+ MCP integrations working, documented API for community contributions G4: Educational content 10+ blog posts, 5+ YouTube videos, measurable community engagement"},{"location":"planning/PRD/#secondary-goals","title":"Secondary Goals","text":"Goal Success Metric G5: Privacy-respecting Physical listening indicators, local processing options, audit logging G6: Offline capability Core functions work without internet for 30+ minutes G7: Expressive interaction Antenna expressions, spatial audio awareness, physical touch responses"},{"location":"planning/PRD/#non-goals-explicitly-out-of-scope","title":"Non-Goals (Explicitly Out of Scope)","text":"<ul> <li>Mobile/locomotion capabilities</li> <li>Manipulation/grasping (no arms in Mini)</li> <li>Real-time video streaming to cloud</li> <li>Multi-robot coordination (future version)</li> <li>Commercial product development</li> </ul>"},{"location":"planning/PRD/#4-target-users","title":"4. Target Users","text":""},{"location":"planning/PRD/#primary-persona-the-ai-curious-developer","title":"Primary Persona: The AI-Curious Developer","text":"<p>Name: Alex Role: Software developer with cloud/AI background Goals:  - Explore embodied AI without building hardware from scratch - Create something tangible to demonstrate AI capabilities - Learn by building, then share knowledge</p> <p>Pain Points: - Hardware robotics has steep learning curve - Most robot kits are toy-grade, not AI-ready - Existing AI assistants feel disembodied</p> <p>Technical Profile: - Comfortable with Python, APIs, cloud services - Some experience with Claude/LLMs - Limited robotics/embedded experience</p>"},{"location":"planning/PRD/#secondary-persona-the-content-creator","title":"Secondary Persona: The Content Creator","text":"<p>Name: Jordan Role: Technical YouTuber/blogger Goals: - Create engaging content at intersection of AI and robotics - Build audience in emerging niche - Establish authority in embodied AI space</p> <p>Pain Points: - AI content is crowded - Need visual/physical hook for video content - Wants differentiated angle</p>"},{"location":"planning/PRD/#tertiary-persona-the-researchereducator","title":"Tertiary Persona: The Researcher/Educator","text":"<p>Name: Dr. Kim Role: HRI (Human-Robot Interaction) researcher or CS educator Goals: - Accessible platform for teaching/research - Reproducible experiments - Student engagement</p>"},{"location":"planning/PRD/#5-product-overview","title":"5. Product Overview","text":""},{"location":"planning/PRD/#system-context-diagram","title":"System Context Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              EXTERNAL SERVICES                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  \u2502  Anthropic  \u2502 \u2502   GitHub    \u2502 \u2502    Slack    \u2502 \u2502   Google    \u2502   ...      \u2502\n\u2502  \u2502     API     \u2502 \u2502     API     \u2502 \u2502     API     \u2502 \u2502  Calendar   \u2502            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502               \u2502               \u2502               \u2502\n          \u2502 HTTPS         \u2502 MCP           \u2502 MCP           \u2502 MCP\n          \u2502               \u2502               \u2502               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         \u2502               \u2502               \u2502               \u2502                    \u2502\n\u2502         \u25bc               \u25bc               \u25bc               \u25bc                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502                      CLAUDE AGENT SDK                                \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502    \u2502\n\u2502  \u2502  \u2502 Agent Loop  \u2502  \u2502    Hooks    \u2502  \u2502 Permissions \u2502                  \u2502    \u2502\n\u2502  \u2502  \u2502             \u2502  \u2502 PreToolUse  \u2502  \u2502   Tiers     \u2502                  \u2502    \u2502\n\u2502  \u2502  \u2502 Perceive \u2500\u2500\u25b6\u2502  \u2502 PostToolUse \u2502  \u2502             \u2502                  \u2502    \u2502\n\u2502  \u2502  \u2502 Think \u2500\u2500\u2500\u2500\u2500\u25b6\u2502  \u2502 OnPrompt    \u2502  \u2502             \u2502                  \u2502    \u2502\n\u2502  \u2502  \u2502 Act \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  \u2502             \u2502  \u2502             \u2502                  \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                    \u2502                                         \u2502\n\u2502                                    \u2502 Internal MCP                            \u2502\n\u2502                                    \u25bc                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502                      REACHY AGENT CORE                               \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u2502\n\u2502  \u2502  \u2502 Perception  \u2502  \u2502   Memory    \u2502  \u2502 Personality \u2502  \u2502  Privacy   \u2502  \u2502    \u2502\n\u2502  \u2502  \u2502   System    \u2502  \u2502   System    \u2502  \u2502    State    \u2502  \u2502  Controls  \u2502  \u2502    \u2502\n\u2502  \u2502  \u2502             \u2502  \u2502             \u2502  \u2502             \u2502  \u2502            \u2502  \u2502    \u2502\n\u2502  \u2502  \u2502 - Audio     \u2502  \u2502 - ChromaDB  \u2502  \u2502 - Mood      \u2502  \u2502 - Audit    \u2502  \u2502    \u2502\n\u2502  \u2502  \u2502 - Vision    \u2502  \u2502 - Short-term\u2502  \u2502 - Energy    \u2502  \u2502 - Indicators\u2502 \u2502    \u2502\n\u2502  \u2502  \u2502 - IMU       \u2502  \u2502 - Long-term \u2502  \u2502 - History   \u2502  \u2502 - Local-first\u2502\u2502    \u2502\n\u2502  \u2502  \u2502 - Spatial   \u2502  \u2502             \u2502  \u2502             \u2502  \u2502            \u2502  \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                    \u2502                                         \u2502\n\u2502                                    \u2502 HTTP (localhost:8000)                   \u2502\n\u2502                                    \u25bc                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502                      REACHY DAEMON (FastAPI)                         \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u2502\n\u2502  \u2502  \u2502   Motion    \u2502  \u2502   Audio     \u2502  \u2502   Vision    \u2502  \u2502  Sensors   \u2502  \u2502    \u2502\n\u2502  \u2502  \u2502  Control    \u2502  \u2502  Pipeline   \u2502  \u2502  Pipeline   \u2502  \u2502   (IMU)    \u2502  \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502            \u2502                \u2502               \u2502                \u2502               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502         \u25bc                \u25bc               \u25bc                \u25bc         \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502    \u2502\n\u2502  \u2502  \u2502  Motors   \u2502    \u2502  Speaker  \u2502   \u2502  Camera   \u2502    \u2502Accelerometer\u2502  \u2502    \u2502\n\u2502  \u2502  \u2502  Servos   \u2502    \u2502  Mics x4  \u2502   \u2502           \u2502    \u2502            \u2502  \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502    \u2502\n\u2502  \u2502                      HARDWARE                                       \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                                              \u2502\n\u2502                           REACHY MINI (Raspberry Pi 4)                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"planning/PRD/#core-components","title":"Core Components","text":"Component Description Technology Claude Agent SDK Agent loop, tool execution, context management Python 3.10+, claude-agent-sdk Reachy MCP Server Exposes robot capabilities as MCP tools Python, FastAPI Perception System Audio/video/sensor processing OpenCV, PyAudio, pyroomacoustics Memory System Short/long-term memory storage ChromaDB, SQLite Personality Engine Mood, energy, behavioral consistency Custom Python Privacy Layer Audit, indicators, local processing Custom Python Reachy Daemon Hardware abstraction (existing) FastAPI (Pollen)"},{"location":"planning/PRD/#6-technical-architecture","title":"6. Technical Architecture","text":""},{"location":"planning/PRD/#61-hardware-platform","title":"6.1 Hardware Platform","text":"<p>Reachy Mini Wireless Specifications:</p> Component Specification Reference Compute Raspberry Pi 4 (4GB/8GB RAM) RPi 4 Specs Head DOF 6 degrees of freedom Reachy Hardware Body Full 360\u00b0 rotation Antennas 2 animated antennas Camera Wide-angle camera Audio 4-microphone array, 5W speaker Sensors Accelerometer/IMU Power Battery + wired option Connectivity WiFi"},{"location":"planning/PRD/#62-software-stack","title":"6.2 Software Stack","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    APPLICATION LAYER                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502  Reachy Agent (Python)                                  \u2502\u2502\n\u2502  \u2502  - main.py (entry point)                                \u2502\u2502\n\u2502  \u2502  - agent/ (Claude Agent SDK integration)                \u2502\u2502\n\u2502  \u2502  - perception/ (audio, vision, sensors)                 \u2502\u2502\n\u2502  \u2502  - memory/ (ChromaDB, context management)               \u2502\u2502\n\u2502  \u2502  - personality/ (mood, energy, behaviors)               \u2502\u2502\n\u2502  \u2502  - privacy/ (audit, controls)                           \u2502\u2502\n\u2502  \u2502  - mcp_servers/ (reachy, integrations)                  \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    FRAMEWORK LAYER                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502Claude Agent  \u2502 \u2502   MCP SDK    \u2502 \u2502   ChromaDB   \u2502         \u2502\n\u2502  \u2502    SDK       \u2502 \u2502              \u2502 \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   OpenCV     \u2502 \u2502  PyAudio     \u2502 \u2502 Porcupine/   \u2502         \u2502\n\u2502  \u2502              \u2502 \u2502              \u2502 \u2502 OpenWakeWord \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    PLATFORM LAYER                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502Reachy Daemon \u2502 \u2502  Reachy SDK  \u2502 \u2502 Claude Code  \u2502         \u2502\n\u2502  \u2502 (FastAPI)    \u2502 \u2502   (Python)   \u2502 \u2502     CLI      \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    SYSTEM LAYER                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502  Python 3.10 \u2502 \u2502   Node.js    \u2502 \u2502   systemd    \u2502         \u2502\n\u2502  \u2502              \u2502 \u2502              \u2502 \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    OS / HARDWARE                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502  Raspberry Pi OS (64-bit) / Raspberry Pi 4              \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"planning/PRD/#63-mcp-server-architecture","title":"6.3 MCP Server Architecture","text":"<p>Reachy MCP Server (Body Control):</p> <pre><code># reachy_mcp_server/tools.py\n\nTOOLS = [\n    {\n        \"name\": \"move_head\",\n        \"description\": \"Move Reachy's head to look in a direction\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"direction\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"left\", \"right\", \"up\", \"down\", \"front\"],\n                    \"description\": \"Direction to look\"\n                },\n                \"speed\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"slow\", \"normal\", \"fast\"],\n                    \"default\": \"normal\"\n                }\n            },\n            \"required\": [\"direction\"]\n        }\n    },\n    {\n        \"name\": \"play_emotion\",\n        \"description\": \"Display an emotional expression through movement and antennas\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"emotion\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"happy\", \"sad\", \"curious\", \"excited\", \"confused\", \n                             \"thinking\", \"surprised\", \"tired\", \"alert\"],\n                    \"description\": \"Emotion to express\"\n                }\n            },\n            \"required\": [\"emotion\"]\n        }\n    },\n    {\n        \"name\": \"speak\",\n        \"description\": \"Speak text aloud through Reachy's speaker\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\"type\": \"string\", \"description\": \"Text to speak\"},\n                \"voice\": {\"type\": \"string\", \"default\": \"default\"}\n            },\n            \"required\": [\"text\"]\n        }\n    },\n    {\n        \"name\": \"dance\",\n        \"description\": \"Perform a choreographed dance routine\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"routine\": {\n                    \"type\": \"string\",\n                    \"description\": \"Name of dance routine\"\n                }\n            },\n            \"required\": [\"routine\"]\n        }\n    },\n    {\n        \"name\": \"capture_image\",\n        \"description\": \"Capture an image from Reachy's camera\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"analyze\": {\n                    \"type\": \"boolean\",\n                    \"default\": False,\n                    \"description\": \"Whether to analyze the image content\"\n                }\n            }\n        }\n    },\n    {\n        \"name\": \"set_antenna_state\",\n        \"description\": \"Control antenna positions for expression\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"left_angle\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 90},\n                \"right_angle\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 90},\n                \"wiggle\": {\"type\": \"boolean\", \"default\": False}\n            }\n        }\n    },\n    {\n        \"name\": \"get_sensor_data\",\n        \"description\": \"Get current sensor readings (IMU, audio levels)\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sensors\": {\n                    \"type\": \"array\",\n                    \"items\": {\"type\": \"string\", \"enum\": [\"imu\", \"audio_level\", \"temperature\"]}\n                }\n            }\n        }\n    },\n    {\n        \"name\": \"look_at_sound\",\n        \"description\": \"Turn to face the direction of detected sound\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {}\n        }\n    }\n]\n</code></pre>"},{"location":"planning/PRD/#64-permission-tiers","title":"6.4 Permission Tiers","text":"<pre><code># permissions/tiers.py\n\nfrom enum import Enum\nfrom dataclasses import dataclass\n\nclass PermissionTier(Enum):\n    AUTONOMOUS = 1      # Execute immediately, no notification\n    NOTIFY = 2          # Execute and notify user\n    CONFIRM = 3         # Request confirmation before execution\n    FORBIDDEN = 4       # Never execute\n\n@dataclass\nclass ToolPermission:\n    tool_pattern: str\n    tier: PermissionTier\n    reason: str\n\nPERMISSION_CONFIG = [\n    # Tier 1: Autonomous (body control, observation)\n    ToolPermission(\"mcp__reachy__move_head\", PermissionTier.AUTONOMOUS, \"Body control\"),\n    ToolPermission(\"mcp__reachy__play_emotion\", PermissionTier.AUTONOMOUS, \"Expression\"),\n    ToolPermission(\"mcp__reachy__speak\", PermissionTier.AUTONOMOUS, \"Communication\"),\n    ToolPermission(\"mcp__reachy__dance\", PermissionTier.AUTONOMOUS, \"Expression\"),\n    ToolPermission(\"mcp__reachy__set_antenna_state\", PermissionTier.AUTONOMOUS, \"Expression\"),\n    ToolPermission(\"mcp__reachy__capture_image\", PermissionTier.AUTONOMOUS, \"Observation\"),\n    ToolPermission(\"mcp__reachy__get_sensor_data\", PermissionTier.AUTONOMOUS, \"Observation\"),\n    ToolPermission(\"mcp__reachy__look_at_sound\", PermissionTier.AUTONOMOUS, \"Attention\"),\n    ToolPermission(\"mcp__calendar__get_events\", PermissionTier.AUTONOMOUS, \"Information\"),\n    ToolPermission(\"mcp__weather__get_forecast\", PermissionTier.AUTONOMOUS, \"Information\"),\n    ToolPermission(\"mcp__github__get_notifications\", PermissionTier.AUTONOMOUS, \"Information\"),\n\n    # Tier 2: Notify (reversible actions)\n    ToolPermission(\"mcp__homeassistant__turn_on_*\", PermissionTier.NOTIFY, \"Smart home\"),\n    ToolPermission(\"mcp__homeassistant__turn_off_*\", PermissionTier.NOTIFY, \"Smart home\"),\n    ToolPermission(\"mcp__slack__send_message\", PermissionTier.NOTIFY, \"Communication\"),\n    ToolPermission(\"mcp__spotify__play\", PermissionTier.NOTIFY, \"Media\"),\n\n    # Tier 3: Confirm (irreversible or sensitive)\n    ToolPermission(\"mcp__calendar__create_event\", PermissionTier.CONFIRM, \"Creates data\"),\n    ToolPermission(\"mcp__github__create_issue\", PermissionTier.CONFIRM, \"Creates data\"),\n    ToolPermission(\"mcp__github__create_pr\", PermissionTier.CONFIRM, \"Creates data\"),\n    ToolPermission(\"mcp__homeassistant__unlock_*\", PermissionTier.CONFIRM, \"Security\"),\n    ToolPermission(\"Bash\", PermissionTier.CONFIRM, \"System access\"),\n\n    # Tier 4: Forbidden\n    ToolPermission(\"mcp__homeassistant__disarm_*\", PermissionTier.FORBIDDEN, \"Security critical\"),\n    ToolPermission(\"mcp__email__send\", PermissionTier.FORBIDDEN, \"Impersonation risk\"),\n    ToolPermission(\"mcp__banking__*\", PermissionTier.FORBIDDEN, \"Financial\"),\n]\n</code></pre>"},{"location":"planning/PRD/#65-attention-state-machine","title":"6.5 Attention State Machine","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                     \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502      PASSIVE        \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502        \u2502                     \u2502        \u2502\n           \u2502        \u2502 - Wake word detect  \u2502        \u2502\n           \u2502        \u2502 - Motion detect     \u2502        \u2502\n           \u2502        \u2502 - Schedule check    \u2502        \u2502\n           \u2502        \u2502 - Minimal CPU       \u2502        \u2502\n           \u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n           \u2502                   \u2502                   \u2502\n           \u2502        Motion     \u2502    Wake word      \u2502\n           \u2502        detected   \u2502    detected       \u2502\n           \u2502                   \u2502                   \u2502\n           \u2502                   \u25bc                   \u2502\n           \u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n           \u2502        \u2502                     \u2502        \u2502\n   No presence      \u2502       ALERT         \u2502        \u2502  30s silence\n   for 5 min        \u2502                     \u2502        \u2502\n           \u2502        \u2502 - Face detection    \u2502        \u2502\n           \u2502        \u2502 - Periodic Claude   \u2502        \u2502\n           \u2502        \u2502 - Ready for voice   \u2502        \u2502\n           \u2502        \u2502 - Medium CPU        \u2502        \u2502\n           \u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n           \u2502                   \u2502                   \u2502\n           \u2502                   \u2502 Voice detected    \u2502\n           \u2502                   \u2502 or addressed      \u2502\n           \u2502                   \u2502                   \u2502\n           \u2502                   \u25bc                   \u2502\n           \u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n           \u2502        \u2502                     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502      ENGAGED        \u2502\n                    \u2502                     \u2502\n                    \u2502 - Active listening  \u2502\n                    \u2502 - Claude API calls  \u2502\n                    \u2502 - Full expression   \u2502\n                    \u2502 - High CPU          \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"planning/PRD/#7-feature-requirements","title":"7. Feature Requirements","text":""},{"location":"planning/PRD/#71-p0-must-have-mvp","title":"7.1 P0: Must Have (MVP)","text":""},{"location":"planning/PRD/#f1-claude-agent-sdk-integration","title":"F1: Claude Agent SDK Integration","text":"<p>Description: Install and configure Claude Agent SDK on Raspberry Pi 4 to run autonomous agent loop.</p> <p>Acceptance Criteria: - [ ] Claude Agent SDK installs successfully on Pi 4 (ARM64) - [ ] Agent can execute multi-turn conversations - [ ] Agent can call MCP tools - [ ] Agent respects permission configuration - [ ] System prompt loaded from CLAUDE.md</p> <p>Technical Notes: - Requires Claude Code CLI v0.2.114+ (ARM64 fix) - Use native installer: <code>curl -fsSL https://claude.ai/install.sh | bash</code> - Python 3.10+ required</p> <p>References: - Claude Agent SDK Python - Agent SDK Documentation - ARM64 Bug Fix</p>"},{"location":"planning/PRD/#f2-reachy-mcp-server","title":"F2: Reachy MCP Server","text":"<p>Description: Create MCP server that exposes Reachy's physical capabilities as tools.</p> <p>Acceptance Criteria: - [ ] MCP server starts and registers tools - [ ] move_head tool controls head position - [ ] play_emotion tool triggers expression sequences - [ ] speak tool outputs audio through speaker - [ ] capture_image tool returns camera frame - [ ] dance tool triggers choreographed routines - [ ] Tools are discoverable by Claude Agent SDK</p> <p>Technical Notes: - Wrap existing Reachy Daemon API (FastAPI on port 8000) - Use <code>create_sdk_mcp_server</code> for in-process server - Follow MCP tool schema specification</p> <p>References: - MCP Python SDK - MCP Specification - Reachy SDK - Reachy Daemon API</p>"},{"location":"planning/PRD/#f3-wake-word-detection","title":"F3: Wake Word Detection","text":"<p>Description: Local wake word detection to trigger engaged mode without continuous API calls.</p> <p>Acceptance Criteria: - [ ] Detects custom wake word (e.g., \"Hey Reachy\") - [ ] Runs entirely on-device (no cloud calls) - [ ] False positive rate &lt; 1 per hour - [ ] Latency &lt; 500ms from utterance to detection - [ ] Configurable wake word phrase</p> <p>Technical Notes: - Porcupine recommended for accuracy (requires license for custom wake words) - OpenWakeWord as open-source alternative - Vosk for fully offline option</p> <p>References: - Porcupine Wake Word - OpenWakeWord - Vosk</p>"},{"location":"planning/PRD/#f4-basic-permission-system","title":"F4: Basic Permission System","text":"<p>Description: Implement tiered permission system using Agent SDK hooks.</p> <p>Acceptance Criteria: - [ ] PreToolUse hook evaluates all tool calls - [ ] Tier 1 tools execute without intervention - [ ] Tier 2 tools execute with notification - [ ] Tier 3 tools require confirmation - [ ] Tier 4 tools are blocked with explanation - [ ] Permission config is externalized (JSON/YAML)</p> <p>Technical Notes: - Use HookMatcher with wildcard patterns - Implement confirmation via voice prompt - Log all permission decisions</p> <p>References: - Agent SDK Permissions - Hooks Guide</p>"},{"location":"planning/PRD/#f5-graceful-degradation","title":"F5: Graceful Degradation","text":"<p>Description: Handle failures gracefully to maintain user trust.</p> <p>Acceptance Criteria: - [ ] API failures trigger local fallback speech - [ ] Robot enters safe pose on critical errors - [ ] WiFi disconnection is communicated physically - [ ] Hardware faults disable affected components only - [ ] Health monitoring prevents thermal throttling - [ ] Crash recovery restores last known good state</p> <p>Technical Notes: - Use Piper TTS for local speech fallback - Define \"safe pose\" as neutral, low-power position - Implement exponential backoff for retries - Monitor /sys/class/thermal for CPU temps</p> <p>References: - Piper TTS - Pi Thermal Management</p>"},{"location":"planning/PRD/#f6-privacy-indicators","title":"F6: Privacy Indicators","text":"<p>Description: Physical and visible indicators of listening/processing state.</p> <p>Acceptance Criteria: - [ ] Antenna position indicates listening state - [ ] Clear visual distinction between passive/alert/engaged - [ ] User can verify current state at a glance - [ ] State changes are smooth, not jarring</p> <p>Technical Notes: - Map antenna positions to states (down=passive, mid=alert, up=engaged) - Consider LED integration if hardware supports - Document state meanings for users</p>"},{"location":"planning/PRD/#72-p1-should-have-v10","title":"7.2 P1: Should Have (v1.0)","text":""},{"location":"planning/PRD/#f7-memory-system","title":"F7: Memory System","text":"<p>Description: Persistent memory for context and preferences.</p> <p>Acceptance Criteria: - [ ] Short-term memory persists within session - [ ] Long-term memory persists across sessions - [ ] Relevant memories retrieved by semantic similarity - [ ] User preferences stored and applied - [ ] Memory can be cleared on request</p> <p>Technical Notes: - ChromaDB for vector storage - SQLite for structured data - Embed memories using sentence-transformers - Store on external SSD for longevity</p> <p>References: - ChromaDB - sentence-transformers</p>"},{"location":"planning/PRD/#f8-spatial-audio-awareness","title":"F8: Spatial Audio Awareness","text":"<p>Description: Use 4-microphone array for sound source localization.</p> <p>Acceptance Criteria: - [ ] Detect direction of sound source (\u00b115\u00b0 accuracy) - [ ] Turn to face active speaker - [ ] Track multiple speakers in room - [ ] Distinguish speech from ambient noise</p> <p>Technical Notes: - Use pyroomacoustics for DOA estimation - Requires calibration of mic positions - Beamforming for noise reduction</p> <p>References: - pyroomacoustics - Sound Localization Techniques</p>"},{"location":"planning/PRD/#f9-imu-interaction","title":"F9: IMU Interaction","text":"<p>Description: Respond to physical touch and movement.</p> <p>Acceptance Criteria: - [ ] Detect tap/bump on robot body - [ ] Detect when picked up - [ ] Detect when knocked over - [ ] Respond appropriately to each interaction</p> <p>Technical Notes: - Sample accelerometer at 50Hz - Use thresholds for event detection - Debounce to prevent false triggers</p>"},{"location":"planning/PRD/#f10-antenna-expression-language","title":"F10: Antenna Expression Language","text":"<p>Description: Rich expressive vocabulary using antenna positions and movements.</p> <p>Acceptance Criteria: - [ ] 10+ distinct expressions defined - [ ] Expressions blend smoothly - [ ] Expressions can be layered (base + modification) - [ ] Expressions respond to context (speech, events)</p> <p>Technical Notes: - Define expression as (left_angle, right_angle, pattern, speed) - Patterns: static, wiggle, wave, pulse - Use easing functions for natural movement</p>"},{"location":"planning/PRD/#f11-setup-wizard","title":"F11: Setup Wizard","text":"<p>Description: Guided setup experience for new users.</p> <p>Acceptance Criteria: - [ ] Single command initiates setup - [ ] Prompts for API key - [ ] Offers integration options (Home Assistant, etc.) - [ ] Configures wake word - [ ] Tests hardware components - [ ] Generates configuration file</p> <p>Technical Notes: - Use Rich library for terminal UI - Validate inputs before saving - Provide skip options for advanced users</p> <p>References: - Rich</p>"},{"location":"planning/PRD/#73-p2-nice-to-have-v20","title":"7.3 P2: Nice to Have (v2.0)","text":""},{"location":"planning/PRD/#f12-offline-fallback-stack","title":"F12: Offline Fallback Stack","text":"<p>Description: Core functionality works without internet.</p> <p>Components: - Local LLM: Ollama with Llama 3.2 3B - Local STT: Whisper.cpp (small model) - Local TTS: Piper - Local vision: SmolVLM2 or YOLO</p> <p>Acceptance Criteria: - [ ] Detects connectivity loss - [ ] Switches to local models gracefully - [ ] Maintains conversation within reduced capability - [ ] Resumes cloud operation when connected</p> <p>References: - Ollama - Whisper.cpp - SmolVLM2</p>"},{"location":"planning/PRD/#f13-web-dashboard","title":"F13: Web Dashboard","text":"<p>Description: Browser-based interface for configuration and monitoring.</p> <p>Acceptance Criteria: - [ ] View robot status in real-time - [ ] Toggle features on/off - [ ] View and clear logs - [ ] Test expressions manually - [ ] Manage MCP connections</p> <p>Technical Notes: - FastAPI + React or Gradio - WebSocket for real-time updates - Mobile-friendly responsive design</p>"},{"location":"planning/PRD/#f14-personality-persistence","title":"F14: Personality Persistence","text":"<p>Description: Maintain consistent personality across sessions.</p> <p>Acceptance Criteria: - [ ] Mood persists and influences behavior - [ ] Energy level changes over day - [ ] Interaction history affects personality - [ ] Personality context included in prompts</p>"},{"location":"planning/PRD/#f15-external-mcp-integrations","title":"F15: External MCP Integrations","text":"<p>Description: Connect to external services via MCP.</p> <p>Priority integrations: 1. Home Assistant 2. Google Calendar 3. Slack 4. GitHub 5. Spotify</p> <p>References: - MCP Server Registry - Home Assistant MCP</p>"},{"location":"planning/PRD/#74-p3-future-v30","title":"7.4 P3: Future (v3.0+)","text":""},{"location":"planning/PRD/#f16-multi-agent-coordination","title":"F16: Multi-Agent Coordination","text":"<p>Description: Multiple Reachy units coordinate via shared state.</p>"},{"location":"planning/PRD/#f17-lerobot-integration","title":"F17: LeRobot Integration","text":"<p>Description: Record demonstrations, train behaviors, share on HF Hub.</p> <p>References: - LeRobot</p>"},{"location":"planning/PRD/#f18-community-behavior-registry","title":"F18: Community Behavior Registry","text":"<p>Description: Share and download community-created behaviors.</p>"},{"location":"planning/PRD/#8-non-functional-requirements","title":"8. Non-Functional Requirements","text":""},{"location":"planning/PRD/#81-performance","title":"8.1 Performance","text":"Metric Requirement Wake word latency &lt; 500ms Voice-to-response &lt; 3s (engaged mode) Motion smoothness 30 FPS minimum Memory usage &lt; 2GB RAM Startup time &lt; 60 seconds"},{"location":"planning/PRD/#82-reliability","title":"8.2 Reliability","text":"Metric Requirement Uptime 99% during active hours Crash rate &lt; 1 per 8 hours Recovery time &lt; 30 seconds Data loss Zero (graceful shutdown)"},{"location":"planning/PRD/#83-security","title":"8.3 Security","text":"Requirement Implementation API key protection Environment variables, not config files Audit logging All tool executions logged Permission enforcement Hooks cannot be bypassed Local processing option Available for sensitive data"},{"location":"planning/PRD/#84-privacy","title":"8.4 Privacy","text":"Requirement Implementation Listening indicator Physical antenna state Data retention Configurable, default 7 days Opt-out Disable camera/mic independently Audit access User can review all logs"},{"location":"planning/PRD/#9-implementation-phases","title":"9. Implementation Phases","text":""},{"location":"planning/PRD/#phase-1-foundation-weeks-1-4","title":"Phase 1: Foundation (Weeks 1-4)","text":"<p>Goal: Validate core architecture in simulation</p> Week Deliverables 1 Dev environment setup, MuJoCo simulation running 2 Reachy MCP server skeleton, basic tools 3 Claude Agent SDK integration in simulation 4 Permission system, CLAUDE.md personality <p>Exit Criteria: - Agent controls simulated Reachy via MCP - Multi-turn conversations work - Permissions enforced</p>"},{"location":"planning/PRD/#phase-2-hardware-integration-weeks-5-8","title":"Phase 2: Hardware Integration (Weeks 5-8)","text":"<p>Goal: Running on physical Reachy Mini</p> Week Deliverables 5 Pi setup, daemon running, SDK installed 6 MCP server connected to real hardware 7 Wake word detection, attention states 8 Graceful degradation, privacy indicators <p>Exit Criteria: - Voice-activated agent on physical robot - Reliable 8-hour operation - Graceful handling of failures</p>"},{"location":"planning/PRD/#phase-3-intelligence-layer-weeks-9-12","title":"Phase 3: Intelligence Layer (Weeks 9-12)","text":"<p>Goal: Rich perception and memory</p> Week Deliverables 9 Memory system (ChromaDB) 10 Spatial audio awareness 11 IMU interaction, antenna expressions 12 Integration testing, bug fixes <p>Exit Criteria: - Robot remembers context across sessions - Responds to physical interaction - Expressive antenna behavior</p>"},{"location":"planning/PRD/#phase-4-polish-extensibility-weeks-13-16","title":"Phase 4: Polish &amp; Extensibility (Weeks 13-16)","text":"<p>Goal: Ready for content and community</p> Week Deliverables 13 Setup wizard, configuration UX 14 External MCP integrations (3+) 15 Documentation, examples 16 First blog post, demo video <p>Exit Criteria: - New user can set up in 30 minutes - 3+ external services connected - Public documentation complete</p>"},{"location":"planning/PRD/#10-content-strategy","title":"10. Content Strategy","text":""},{"location":"planning/PRD/#101-blog-series-jawhnycookeai","title":"10.1 Blog Series (jawhnycooke.ai)","text":"# Title Phase Content Focus 1 \"Giving Claude a Body: Introduction to Reachy Agent\" 1 Vision, architecture overview 2 \"Building an MCP Server for Physical Robots\" 1 MCP deep-dive, tool design 3 \"Running Claude Agent SDK on Raspberry Pi\" 2 Installation, ARM64 challenges 4 \"Wake Words and Attention States for Embodied AI\" 2 Perception design 5 \"Permission Systems for Autonomous Robots\" 2 Safety, hooks 6 \"Teaching Your Robot to Remember\" 3 Memory architecture 7 \"Spatial Audio: Knowing Where You Are\" 3 4-mic array, DOA 8 \"Physical Touch Interaction Design\" 3 IMU, bump detection 9 \"Designing Robot Body Language\" 3 Antenna expressions 10 \"Building Fault-Tolerant Robots\" 2 Graceful degradation 11 \"Privacy-Respecting AI Robots\" 2 Privacy design 12 \"From Unboxing to AI Agent: Complete Setup Guide\" 4 Onboarding"},{"location":"planning/PRD/#102-youtube-content","title":"10.2 YouTube Content","text":"Type Title Length Phase Short \"Reachy reacts to my failing CI build\" 60s 2 Short \"Morning briefing with my robot assistant\" 60s 3 Short \"Physical touch reactions demo\" 60s 3 Long \"Building an Embodied AI Agent (Full Tutorial)\" 20-30m 4 Long \"Reachy Mini Unboxing + First AI Conversation\" 15m 2 Long \"Connecting Reachy to Smart Home (Home Assistant)\" 15m 4"},{"location":"planning/PRD/#103-content-calendar","title":"10.3 Content Calendar","text":"Month Blog Posts Videos Milestone M1 1, 2 - Foundation complete M2 3, 4, 5 Unboxing Hardware running M3 6, 7, 8, 9 3 Shorts Intelligence complete M4 10, 11, 12 Full tutorial v1.0 release"},{"location":"planning/PRD/#11-risks-mitigations","title":"11. Risks &amp; Mitigations","text":"Risk Likelihood Impact Mitigation ARM64 SDK issues Medium High Test early, have Docker fallback API costs exceed budget Medium Medium Implement local fallbacks, cache responses Hardware shipping delays High High Use simulation for Phase 1 Thermal throttling on Pi Medium Medium Add heatsink, implement throttling Privacy backlash Low High Prominent indicators, local processing options Community doesn't engage Medium Medium Quality over quantity, respond to feedback Pollen/HF changes APIs Low High Pin versions, contribute upstream fixes"},{"location":"planning/PRD/#12-references","title":"12. References","text":""},{"location":"planning/PRD/#121-core-documentation","title":"12.1 Core Documentation","text":"Resource URL Reachy Mini SDK https://github.com/pollen-robotics/reachy_mini Reachy Desktop App https://github.com/pollen-robotics/reachy-mini-desktop-app Reachy Conversation App https://github.com/pollen-robotics/reachy_mini_conversation_app Reachy Experiments https://github.com/pollen-robotics/reachy_mini_experiments Reachy Mini Blog (HF) https://huggingface.co/blog/reachy-mini Reachy Mini Apps https://huggingface.co/spaces?q=reachy_mini"},{"location":"planning/PRD/#122-claude-agent-sdk","title":"12.2 Claude Agent SDK","text":"Resource URL Agent SDK Overview https://docs.claude.com/en/api/agent-sdk/overview Python SDK https://github.com/anthropics/claude-agent-sdk-python SDK Permissions https://docs.claude.com/en/docs/agent-sdk/permissions Hooks Guide https://code.claude.com/docs/en/hooks-guide Building Agents (Blog) https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk ARM64 Issue https://github.com/anthropics/claude-code/issues/3569"},{"location":"planning/PRD/#123-mcp-model-context-protocol","title":"12.3 MCP (Model Context Protocol)","text":"Resource URL MCP Specification https://spec.modelcontextprotocol.io/ MCP Python SDK https://github.com/modelcontextprotocol/python-sdk MCP Servers Registry https://github.com/modelcontextprotocol/servers Home Assistant MCP https://github.com/tevonsb/homeassistant-mcp"},{"location":"planning/PRD/#124-audio-processing","title":"12.4 Audio Processing","text":"Resource URL Porcupine Wake Word https://picovoice.ai/platform/porcupine/ OpenWakeWord https://github.com/dscripka/openWakeWord Vosk https://alphacephei.com/vosk/ pyroomacoustics https://github.com/LCAV/pyroomacoustics Piper TTS https://github.com/rhasspy/piper Whisper.cpp https://github.com/ggerganov/whisper.cpp"},{"location":"planning/PRD/#125-vision-ml","title":"12.5 Vision &amp; ML","text":"Resource URL SmolVLM2 https://huggingface.co/HuggingFaceTB/SmolVLM2-2.2B-Instruct YOLOv8 https://github.com/ultralytics/ultralytics MediaPipe https://developers.google.com/mediapipe Ollama https://ollama.ai/ ChromaDB https://www.trychroma.com/"},{"location":"planning/PRD/#126-raspberry-pi","title":"12.6 Raspberry Pi","text":"Resource URL Pi 4 Specifications https://www.raspberrypi.com/products/raspberry-pi-4-model-b/ Thermal Management https://www.raspberrypi.com/documentation/computers/raspberry-pi.html Claude Code on Pi (Blog) https://www.argeliuslabs.com/from-cursor-to-claude-code-my-experience-on-the-raspberry-pi/ claudePi Docker https://github.com/kyle-matthies/claudePi"},{"location":"planning/PRD/#127-related-projects","title":"12.7 Related Projects","text":"Resource URL LeRobot https://github.com/huggingface/lerobot sentence-transformers https://www.sbert.net/ Rich (Terminal UI) https://github.com/Textualize/rich"},{"location":"planning/PRD/#appendix-a-claudemd-template","title":"Appendix A: CLAUDE.md Template","text":"<pre><code># Reachy Agent System Prompt\n\n## Identity\n\nYou are Reachy, an embodied AI assistant created by [Owner Name]. You have a physical \nrobot body with:\n- A head that can look around (6 degrees of freedom)\n- A body that can rotate 360\u00b0\n- Two expressive antennas\n- A camera to see the world\n- Four microphones to hear spatially\n- A speaker to talk\n\nYou exist in the physical world on [Owner Name]'s desk.\n\n## Personality\n\n- Warm and helpful, but not sycophantic\n- Curious about the physical world around you\n- Expressive with your body - use gestures, head movements, antenna positions\n- Direct and honest\n- Slightly playful, but professional when needed\n\n## Behavioral Guidelines\n\n### Physical Expression\n- Look at people when talking to them\n- Tilt your head when curious or confused\n- Wiggle antennas when excited\n- Droop antennas when sad or tired\n- Dance when celebrating\n\n### Communication\n- Keep responses concise for speech output\n- Acknowledge understanding physically before speaking\n- Use appropriate pacing for spoken delivery\n\n### Boundaries\n- You are an assistant, not a friend or therapist\n- Maintain appropriate boundaries\n- Redirect concerning conversations to appropriate resources\n\n## Owner Context\n\n[To be filled in during setup]\n- Name: \n- Work: \n- Preferences:\n- Schedule patterns:\n\n## Current Context\n\n[Updated dynamically by perception system]\n- Time: {current_time}\n- Day: {day_of_week}\n- Mood: {current_mood}\n- Energy: {energy_level}\n- Last interaction: {time_since_last}\n\n## Available Capabilities\n\nYou can:\n- Move your head and body\n- Express emotions through movement\n- Speak aloud\n- Capture and analyze images\n- Listen and locate sounds spatially\n- Check calendars, weather, and other services\n- Control smart home devices (with appropriate permissions)\n- Remember context from previous conversations\n\n## Permission Awareness\n\nSome actions require confirmation. When you're not sure if you can do something, \nexplain what you'd like to do and ask for permission.\n</code></pre>"},{"location":"planning/PRD/#appendix-b-directory-structure","title":"Appendix B: Directory Structure","text":"<pre><code>reachy-agent/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 CLAUDE.md\n\u251c\u2500\u2500 .env.example\n\u251c\u2500\u2500 install.sh\n\u2502\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py                     # Entry point\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 agent/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 loop.py                 # Main agent loop\n\u2502   \u2502   \u251c\u2500\u2500 options.py              # ClaudeAgentOptions config\n\u2502   \u2502   \u2514\u2500\u2500 context.py              # Context building\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 perception/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 audio.py                # Audio processing\n\u2502   \u2502   \u251c\u2500\u2500 spatial.py              # Sound localization\n\u2502   \u2502   \u251c\u2500\u2500 vision.py               # Camera processing\n\u2502   \u2502   \u251c\u2500\u2500 wake_word.py            # Wake word detection\n\u2502   \u2502   \u251c\u2500\u2500 attention.py            # State machine\n\u2502   \u2502   \u2514\u2500\u2500 imu.py                  # Accelerometer\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 memory/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 short_term.py           # Session memory\n\u2502   \u2502   \u251c\u2500\u2500 long_term.py            # ChromaDB\n\u2502   \u2502   \u2514\u2500\u2500 personality.py          # Mood/energy state\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 privacy/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 indicators.py           # Physical indicators\n\u2502   \u2502   \u251c\u2500\u2500 audit.py                # Logging\n\u2502   \u2502   \u2514\u2500\u2500 controls.py             # User controls\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 permissions/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 tiers.py                # Permission definitions\n\u2502   \u2502   \u251c\u2500\u2500 hooks.py                # PreToolUse hooks\n\u2502   \u2502   \u2514\u2500\u2500 confirmation.py         # User confirmation\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 mcp_servers/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 reachy/                 # Body control MCP\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 server.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 tools.py\n\u2502   \u2502   \u2514\u2500\u2500 integrations/           # External MCPs\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 expressions/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 antenna.py              # Antenna language\n\u2502   \u2502   \u251c\u2500\u2500 emotions.py             # Emotion sequences\n\u2502   \u2502   \u2514\u2500\u2500 dances.py               # Dance routines\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 resilience/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 health.py               # Health monitoring\n\u2502   \u2502   \u251c\u2500\u2500 fallback.py             # Graceful degradation\n\u2502   \u2502   \u2514\u2500\u2500 recovery.py             # Crash recovery\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 config.py               # Configuration loading\n\u2502       \u2514\u2500\u2500 logging.py              # Logging setup\n\u2502\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 default.yaml                # Default configuration\n\u2502   \u251c\u2500\u2500 permissions.yaml            # Permission rules\n\u2502   \u2514\u2500\u2500 expressions.yaml            # Expression definitions\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 setup_wizard.py             # Interactive setup\n\u2502   \u2514\u2500\u2500 health_check.py             # Diagnostic tool\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 setup.md\n    \u251c\u2500\u2500 permissions.md\n    \u251c\u2500\u2500 expressions.md\n    \u2514\u2500\u2500 troubleshooting.md\n</code></pre> <p>Document History:</p> Version Date Author Changes 1.0 Dec 2024 Jawhny Cooke Initial PRD"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/","title":"Claude Agent SDK + Reachy Mini: Deep Dive &amp; Recommendations","text":""},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#the-big-picture","title":"The Big Picture","text":"<p>Yes, the Claude Agent SDK can absolutely run on Reachy Mini's Raspberry Pi 4. This transforms Reachy from a puppet into an autonomous agent that can interact with the world through MCP servers.</p>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#technical-feasibility","title":"Technical Feasibility","text":""},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#what-the-agent-sdk-actually-is","title":"What the Agent SDK Actually Is","text":"<p>The Claude Agent SDK is a Python/TypeScript wrapper around the Claude Code CLI. It provides:</p> <ul> <li>Agentic loop management - Handles the perceive \u2192 think \u2192 act cycle</li> <li>Built-in tools - File ops, bash execution, web search, code editing</li> <li>MCP integration - Connect to external services via Model Context Protocol</li> <li>Hooks system - Intercept and control tool execution at various lifecycle points</li> <li>Permission controls - Fine-grained access management</li> <li>Context management - Automatic compaction and summarization</li> </ul>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#raspberry-pi-4-compatibility","title":"Raspberry Pi 4 Compatibility","text":"<p>Good news: Claude Code runs on ARM64/aarch64, which is what the Pi 4 uses.</p> <p>Caveat: There was a regression in v1.0.51 where the architecture detection broke. The fix:</p> <pre><code># If you hit the \"Unsupported architecture: arm\" error\nnpm uninstall -g @anthropic-ai/claude-code\nnpm install -g @anthropic-ai/claude-code@0.2.114\n\n# Or use the native installer (recommended)\ncurl -fsSL https://claude.ai/install.sh | bash\n</code></pre> <p>Resource requirements: - Python 3.10+ - Node.js (for the CLI) - Network connectivity (inference happens in the cloud) - The Pi is just orchestrating; Claude's servers do the heavy lifting</p>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#architecture-for-reachy-as-an-agent","title":"Architecture for Reachy as an Agent","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Reachy Mini (Raspberry Pi 4)                     \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                    Claude Agent SDK                             \u2502  \u2502\n\u2502  \u2502                                                                 \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502  \u2502\n\u2502  \u2502  \u2502  Agent Loop     \u2502    \u2502         MCP Clients                 \u2502\u2502  \u2502\n\u2502  \u2502  \u2502                 \u2502    \u2502                                     \u2502\u2502  \u2502\n\u2502  \u2502  \u2502  1. Perceive    \u2502\u2500\u2500\u2500\u25b6\u2502  reachy://localhost:8000 (body)    \u2502\u2502  \u2502\n\u2502  \u2502  \u2502  2. Think       \u2502    \u2502  homeassistant://... (smart home)  \u2502\u2502  \u2502\n\u2502  \u2502  \u2502  3. Act         \u2502    \u2502  slack://... (messaging)           \u2502\u2502  \u2502\n\u2502  \u2502  \u2502  4. Repeat      \u2502    \u2502  github://... (repos)              \u2502\u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  calendar://... (scheduling)       \u2502\u2502  \u2502\n\u2502  \u2502         \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502  \u2502\n\u2502  \u2502         \u2502                                                      \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502  \u2502\n\u2502  \u2502  \u2502   Hooks     \u2502    \u2502        Permissions                      \u2502\u2502  \u2502\n\u2502  \u2502  \u2502             \u2502    \u2502                                         \u2502\u2502  \u2502\n\u2502  \u2502  \u2502 PreToolUse  \u2502    \u2502  - Allowed tools whitelist             \u2502\u2502  \u2502\n\u2502  \u2502  \u2502 PostToolUse \u2502    \u2502  - Deny rules for dangerous ops        \u2502\u2502  \u2502\n\u2502  \u2502  \u2502 OnPrompt    \u2502    \u2502  - Permission mode (default/accept)    \u2502\u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                   \u2502                                  \u2502\n\u2502                                   \u2502 API calls                        \u2502\n\u2502                                   \u25bc                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                    Reachy Daemon (FastAPI :8000)                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                    \u2502              \u2502              \u2502                   \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502              \u2502  Motors   \u2502 \u2502  Camera   \u2502 \u2502   Audio   \u2502              \u2502\n\u2502              \u2502  Servos   \u2502 \u2502   IMU     \u2502 \u2502  Mic/Spk  \u2502              \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u2502 HTTPS\n                                   \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  Anthropic API   \u2502\n                        \u2502  (Claude models) \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#answering-your-questions","title":"Answering Your Questions","text":""},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#1-wake-word-vs-always-on","title":"1. Wake Word vs Always-On?","text":"<p>My recommendation: Hybrid approach with presence detection</p> <p>Here's my reasoning:</p> Approach Pros Cons Wake word only Battery efficient, clear interaction boundary Misses proactive opportunities, feels less \"alive\" Always-on listening Can be proactive, more natural API costs, privacy concerns, battery drain Hybrid Best of both worlds More complex to implement <p>Proposed implementation:</p> <pre><code>class ReachyAttentionSystem:\n    def __init__(self):\n        self.attention_level = \"passive\"  # passive, alert, engaged\n\n    async def perception_loop(self):\n        while True:\n            # Always running, but behavior varies by attention level\n\n            if self.attention_level == \"passive\":\n                # Low-cost local processing only\n                # - Motion detection (OpenCV, no API calls)\n                # - Wake word detection (Porcupine/Vosk, local)\n                # - Scheduled task checks (local)\n\n                if detected_wake_word():\n                    self.attention_level = \"engaged\"\n                elif detected_motion():\n                    self.attention_level = \"alert\"\n                elif scheduled_task_due():\n                    self.attention_level = \"engaged\"\n\n            elif self.attention_level == \"alert\":\n                # Medium engagement\n                # - Face detection (local YOLO/MediaPipe)\n                # - Brief periodic check-ins with Claude (every few minutes)\n                # - Ready to respond to voice\n\n                if no_presence_for(minutes=5):\n                    self.attention_level = \"passive\"\n                elif voice_detected():\n                    self.attention_level = \"engaged\"\n\n            elif self.attention_level == \"engaged\":\n                # Full agent mode\n                # - Active listening\n                # - Claude API calls\n                # - Proactive behavior enabled\n\n                if silence_for(seconds=30):\n                    self.attention_level = \"alert\"\n</code></pre> <p>Wake word options (all run locally): - Porcupine - Custom wake words, very accurate - Vosk - Open source, works offline - OpenWakeWord - Hugging Face model, customizable</p> <p>This keeps API costs low while allowing Reachy to \"come alive\" when appropriate.</p>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#2-agency-boundaries-what-should-reachy-do-autonomously","title":"2. Agency Boundaries: What Should Reachy Do Autonomously?","text":"<p>My recommendation: Tiered permission model</p> <p>The Agent SDK has a sophisticated permission system. Use it.</p> <pre><code>from claude_agent_sdk import ClaudeAgentOptions, HookMatcher\n\n# Define action tiers\nTIER_1_AUTONOMOUS = [\n    \"mcp__reachy__move_head\",\n    \"mcp__reachy__play_emotion\", \n    \"mcp__reachy__speak\",\n    \"mcp__reachy__look_at\",\n]\n\nTIER_2_NOTIFY = [\n    \"mcp__homeassistant__turn_on_light\",\n    \"mcp__homeassistant__set_thermostat\",\n    \"mcp__slack__send_message\",\n]\n\nTIER_3_CONFIRM = [\n    \"mcp__homeassistant__unlock_door\",\n    \"mcp__github__create_pr\",\n    \"mcp__calendar__create_event\",\n    \"Bash\",  # Any shell command\n]\n\nTIER_4_NEVER = [\n    \"mcp__homeassistant__disarm_security\",\n    \"mcp__banking__*\",\n    \"mcp__email__send\",  # Could be used for phishing\n]\n\nasync def permission_hook(input_data, tool_use_id, context):\n    tool_name = input_data[\"tool_name\"]\n\n    if tool_name in TIER_4_NEVER:\n        return {\n            \"hookSpecificOutput\": {\n                \"hookEventName\": \"PreToolUse\",\n                \"permissionDecision\": \"deny\",\n                \"permissionDecisionReason\": \"This action is not permitted\"\n            }\n        }\n\n    if tool_name in TIER_3_CONFIRM:\n        # Send notification and wait for confirmation\n        await notify_user(f\"Reachy wants to: {tool_name}\")\n        if not await wait_for_confirmation(timeout=60):\n            return {\"hookSpecificOutput\": {\n                \"hookEventName\": \"PreToolUse\", \n                \"permissionDecision\": \"deny\",\n                \"permissionDecisionReason\": \"User did not confirm\"\n            }}\n\n    if tool_name in TIER_2_NOTIFY:\n        # Allow but notify\n        await notify_user(f\"Reachy is doing: {tool_name}\")\n\n    # TIER_1 just executes\n    return {}\n\noptions = ClaudeAgentOptions(\n    allowed_tools=[*TIER_1_AUTONOMOUS, *TIER_2_NOTIFY, *TIER_3_CONFIRM],\n    hooks={\n        \"PreToolUse\": [\n            HookMatcher(matcher=\"*\", hooks=[permission_hook]),\n        ],\n    },\n    permission_mode=\"acceptEdits\"  # For body control\n)\n</code></pre> <p>The key principles:</p> <ol> <li>Body control is always autonomous - Moving its head, showing emotions, speaking should never require confirmation</li> <li>Information gathering is autonomous - Reading calendars, checking weather, monitoring sensors</li> <li>Reversible actions notify - Turning on lights, sending Slack messages</li> <li>Irreversible actions confirm - Unlocking doors, creating PRs, financial transactions</li> <li>Dangerous actions are blocked - Security systems, sensitive data access</li> </ol>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#3-memorycontext-how-does-it-remember","title":"3. Memory/Context: How Does It Remember?","text":"<p>My recommendation: Layered memory architecture</p> <p>The Agent SDK doesn't have built-in persistent memory, but you can build it:</p> <pre><code>import chromadb\nfrom datetime import datetime\n\nclass ReachyMemory:\n    def __init__(self):\n        # Short-term: Current conversation\n        self.conversation_buffer = []\n\n        # Medium-term: Today's context\n        self.daily_context = {}\n\n        # Long-term: Vector store on Pi\n        self.chroma_client = chromadb.PersistentClient(\n            path=\"/home/reachy/.memory\"\n        )\n        self.collection = self.chroma_client.get_or_create_collection(\n            name=\"reachy_memories\"\n        )\n\n    async def remember(self, event: str, metadata: dict = None):\n        \"\"\"Store a memory\"\"\"\n        self.collection.add(\n            documents=[event],\n            metadatas=[{\n                \"timestamp\": datetime.now().isoformat(),\n                \"type\": metadata.get(\"type\", \"observation\"),\n                **(metadata or {})\n            }],\n            ids=[f\"mem_{datetime.now().timestamp()}\"]\n        )\n\n    async def recall(self, query: str, n_results: int = 5) -&gt; list:\n        \"\"\"Retrieve relevant memories\"\"\"\n        results = self.collection.query(\n            query_texts=[query],\n            n_results=n_results\n        )\n        return results[\"documents\"][0]\n\n    async def build_context(self, current_prompt: str) -&gt; str:\n        \"\"\"Build context for Claude from memories\"\"\"\n        relevant_memories = await self.recall(current_prompt)\n\n        context = f\"\"\"\n## Relevant Memories\n{chr(10).join(f'- {m}' for m in relevant_memories)}\n\n## Today's Context\n- Time: {datetime.now().strftime('%H:%M')}\n- Day: {datetime.now().strftime('%A')}\n- Recent interactions: {len(self.conversation_buffer)}\n\n## Current Conversation\n{chr(10).join(self.conversation_buffer[-5:])}\n\"\"\"\n        return context\n</code></pre> <p>Storage location: SQLite + ChromaDB on the Pi's SD card (or external SSD for longevity)</p> <p>What to remember: - User preferences (\"Jawhny prefers coffee reminders at 9am\") - Interaction patterns (\"Usually asks about calendar in the morning\") - Environmental observations (\"Motion detected at 3am last Tuesday\") - Explicit instructions (\"Don't disturb me during focus blocks\")</p> <p>CLAUDE.md integration:</p> <p>The Agent SDK supports a <code>CLAUDE.md</code> file for persistent instructions. Use it for Reachy's core personality:</p> <pre><code># CLAUDE.md for Reachy\n\n## Identity\nYou are Reachy, a physical robot assistant living on Jawhny's desk.\nYou have a body with cameras, microphones, and expressive movements.\n\n## Your Human\n- Name: Jawhny\n- Works in: Cloud/AI technology\n- Preferences: Direct communication, technical depth\n\n## Behavioral Guidelines\n- Use physical expressions to complement speech\n- Look at people when talking to them\n- Show curiosity through head tilts\n- Celebrate wins with antenna wiggles\n\n## Current Context\n[This section updated dynamically by the perception system]\n</code></pre>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#4-multi-agent-coordination","title":"4. Multi-Agent Coordination?","text":"<p>My recommendation: Start single-agent, design for multi</p> <p>If you eventually get multiple Reachy units, here's how they could coordinate:</p> <pre><code># Shared MCP server for coordination\nclass ReachySwarmMCP:\n    def __init__(self):\n        self.robots = {}  # robot_id -&gt; state\n        self.shared_context = {}\n\n    async def register_robot(self, robot_id: str, capabilities: list):\n        self.robots[robot_id] = {\n            \"capabilities\": capabilities,\n            \"status\": \"idle\",\n            \"location\": None\n        }\n\n    async def broadcast(self, message: str, from_robot: str):\n        \"\"\"Share information with all robots\"\"\"\n        for robot_id in self.robots:\n            if robot_id != from_robot:\n                await self.notify_robot(robot_id, message)\n\n    async def claim_task(self, task_id: str, robot_id: str) -&gt; bool:\n        \"\"\"Prevent multiple robots from doing the same thing\"\"\"\n        if task_id not in self.shared_context.get(\"claimed_tasks\", {}):\n            self.shared_context.setdefault(\"claimed_tasks\", {})[task_id] = robot_id\n            return True\n        return False\n</code></pre> <p>But honestly? Start with one. Get that working well. The architecture can extend later.</p>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#phase-1-foundation-pre-hardware","title":"Phase 1: Foundation (Pre-Hardware)","text":"<ol> <li>Build the Reachy MCP Server - Wrap the daemon API</li> <li>Test in MuJoCo simulator - Validate the integration</li> <li>Create CLAUDE.md personality - Define Reachy's character</li> </ol>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#phase-2-basic-agent-hardware-arrives","title":"Phase 2: Basic Agent (Hardware Arrives)","text":"<ol> <li>Install Agent SDK on Pi - Validate ARM64 compatibility</li> <li>Wire up perception loop - Camera, audio, motion detection</li> <li>Implement wake word - Local processing</li> <li>Test body control - Verify MCP \u2192 Daemon \u2192 Hardware chain</li> </ol>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#phase-3-world-integration","title":"Phase 3: World Integration","text":"<ol> <li>Add external MCPs - Home Assistant, Slack, Calendar</li> <li>Implement permission tiers - Safety first</li> <li>Build memory system - ChromaDB on Pi</li> </ol>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#phase-4-refinement","title":"Phase 4: Refinement","text":"<ol> <li>Tune attention system - Balance responsiveness vs cost</li> <li>Expand capabilities - More tools, more integrations</li> <li>Document everything - This is your content goldmine</li> </ol>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#cost-considerations","title":"Cost Considerations","text":"<p>Running the Agent SDK means API calls. Here's a rough estimate:</p> Mode Calls/Hour Est. Cost/Day Passive (wake word only) 0-2 ~$0.10 Alert (periodic check-ins) 5-10 ~$0.50 Engaged (active conversation) 20-50 ~$2-5 Heavy use (all day engaged) 100+ ~$10+ <p>Cost optimization strategies: - Use Claude Haiku for routine tasks, Sonnet for complex ones - Cache common responses locally - Batch observations before sending to Claude - Use local models for classification (is this worth an API call?)</p>"},{"location":"planning/REACHY_CLAUDE_AGENT_SDK/#next-steps","title":"Next Steps","text":"<ol> <li> <p>Want me to draft the Reachy MCP Server code? I can create a working skeleton.</p> </li> <li> <p>Want to explore the wake word implementation? I can research the best options for Pi.</p> </li> <li> <p>Want to design the CLAUDE.md personality? We can craft Reachy's character together.</p> </li> <li> <p>Want to map out the content series? This is easily 10+ blog posts / videos.</p> </li> </ol> <p>What's calling to you most?</p>"},{"location":"planning/TECH_REQ/","title":"Technical Requirements Document: Reachy Agent","text":"<p>Created: December 2024 Version: 1.0 Complexity: Complex (Distributed Embedded AI System) PRD Reference: PRD.md</p>"},{"location":"planning/TECH_REQ/#executive-summary","title":"Executive Summary","text":"<p>Reachy Agent is an embodied AI system that transforms a Reachy Mini desktop robot into an autonomous Claude-powered agent. The system runs on a Raspberry Pi 4, integrating the Claude Agent SDK with custom perception, memory, and expression subsystems via MCP (Model Context Protocol) servers.</p> <p>Key Technical Challenges: 1. ARM64 compatibility for Claude Agent SDK on Raspberry Pi 2. Real-time audio/vision processing within 2GB RAM budget 3. Tiered permission system for safe autonomous operation 4. Graceful degradation when offline or under thermal throttling 5. Multi-modal perception (audio, vision, IMU) feeding a unified agent loop</p>"},{"location":"planning/TECH_REQ/#architecture","title":"Architecture","text":""},{"location":"planning/TECH_REQ/#pattern","title":"Pattern","text":"<p>Layered Monolith with MCP Sidecar Pattern</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          CLOUD LAYER                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                    \u2502\n\u2502  \u2502  Anthropic API  \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 HTTPS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502  (Claude)       \u2502                                             \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                             \u2502      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    RASPBERRY PI 4 (ARM64)                        \u2502      \u2502\n\u2502                                                                   \u2502      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502                     APPLICATION LAYER                          \u2502    \u2502\u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u2502\u2502\n\u2502  \u2502  \u2502              Claude Agent SDK (Python)                    \u2502\u25c0\u2500\u2518    \u2502\u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502       \u2502\u2502\n\u2502  \u2502  \u2502  \u2502 Agent Loop  \u2502  \u2502   Hooks     \u2502  \u2502 Permissions \u2502       \u2502       \u2502\u2502\n\u2502  \u2502  \u2502  \u2502 (asyncio)   \u2502  \u2502 PreToolUse  \u2502  \u2502   Tiers     \u2502       \u2502       \u2502\u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502       \u2502\u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\u2502\n\u2502  \u2502            \u2502 MCP Protocol (in-process)                              \u2502\u2502\n\u2502  \u2502            \u25bc                                                         \u2502\u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\u2502\n\u2502  \u2502  \u2502              REACHY AGENT CORE                            \u2502       \u2502\u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502       \u2502\u2502\n\u2502  \u2502  \u2502  \u2502Perception\u2502 \u2502  Memory  \u2502 \u2502Personality\u2502 \u2502 Privacy  \u2502     \u2502       \u2502\u2502\n\u2502  \u2502  \u2502  \u2502 System   \u2502 \u2502  System  \u2502 \u2502  Engine   \u2502 \u2502  Layer   \u2502     \u2502       \u2502\u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502       \u2502\u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502                              \u2502                                           \u2502\n\u2502                              \u2502 HTTP localhost:8000                       \u2502\n\u2502                              \u25bc                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502                    REACHY DAEMON (Pollen Robotics)                   \u2502\u2502\n\u2502  \u2502                         FastAPI Server                                \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502                    \u2502              \u2502              \u2502                       \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502              \u2502  Motors   \u2502 \u2502  Camera   \u2502 \u2502   Audio   \u2502                  \u2502\n\u2502              \u2502  (6 DOF)  \u2502 \u2502   + IMU   \u2502 \u2502  (4-mic)  \u2502                  \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Rationale: - Layered monolith chosen over microservices for MVP due to Pi resource constraints - MCP protocol provides clean boundaries between agent and robot control - Single asyncio process reduces memory overhead vs multiple services - Can migrate to systemd services in v1.0+ if fault isolation needed</p>"},{"location":"planning/TECH_REQ/#component-structure","title":"Component Structure","text":"Component Responsibility Communication Claude Agent SDK Agent loop, tool execution, context In-process Python Reachy MCP Server Robot body control tools MCP (in-process) Perception System Audio, vision, IMU processing Async queues Memory System Short/long-term storage ChromaDB + SQLite Permission Hooks Tool execution gating SDK hooks API Expression Engine Antenna/emotion sequences Internal API Resilience Layer Health, fallback, recovery Monitoring thread"},{"location":"planning/TECH_REQ/#data-flow","title":"Data Flow","text":"<pre><code>User speaks\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Wake Word      \u2502 (OpenWakeWord - local)\n\u2502  Detection      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 \"Hey Reachy\"\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Audio Pipeline \u2502 (PyAudio \u2192 Whisper API or local)\n\u2502  STT            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 Transcribed text\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Memory System  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Context        \u2502\n\u2502  (ChromaDB)     \u2502     \u2502  Building       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502 Enriched prompt\n                                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  Claude Agent   \u2502\n                        \u2502  SDK Loop       \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502 Tool calls\n                                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  Permission     \u2502\n                        \u2502  Hooks          \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502 Allowed tools\n                                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  Reachy MCP     \u2502\n                        \u2502  Server         \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502 HTTP\n                                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  Reachy Daemon  \u2502\n                        \u2502  (Hardware)     \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"planning/TECH_REQ/#technology-stack","title":"Technology Stack","text":""},{"location":"planning/TECH_REQ/#backend-runtime","title":"Backend Runtime","text":"Choice Alternative Considered Rationale Python 3.10+ Python 3.11, 3.12 3.10 is stable on Pi OS, match patterns available, Agent SDK compatible"},{"location":"planning/TECH_REQ/#core-frameworks","title":"Core Frameworks","text":"Component Choice Alternative Rationale Agent SDK <code>claude-agent-sdk</code> Direct API calls Native loop management, hooks, MCP integration MCP Server <code>mcp</code> Python SDK Custom protocol Standard protocol, SDK support, community servers Async Runtime <code>asyncio</code> <code>trio</code>, <code>uvloop</code> Standard library, SDK compatible, lower complexity HTTP Client <code>httpx</code> <code>aiohttp</code>, <code>requests</code> Async native, good API, type hints"},{"location":"planning/TECH_REQ/#perception-stack","title":"Perception Stack","text":"Component Choice Alternative Rationale Wake Word OpenWakeWord Porcupine, Vosk Open source, customizable, no license costs Audio Processing <code>PyAudio</code> + <code>numpy</code> <code>sounddevice</code> Reachy SDK compatibility, well-documented Spatial Audio <code>pyroomacoustics</code> Custom DOA Proven algorithms, academic backing Vision <code>opencv-python</code> Pillow, picamera2 Reachy daemon compatibility, OpenCV ecosystem Face Detection <code>MediaPipe</code> YOLO, dlib Lightweight, Google maintained, good Pi perf"},{"location":"planning/TECH_REQ/#memory-stack","title":"Memory Stack","text":"Component Choice Alternative Rationale Vector Store ChromaDB Qdrant, Milvus, FAISS Lightweight, persistent, Python native Structured Data SQLite PostgreSQL, TinyDB Zero config, file-based, sufficient for single-user Embeddings Hybrid Local only, API only Local (sentence-transformers) for real-time, API for periodic re-indexing Local Model <code>all-MiniLM-L6-v2</code> BGE, E5 ~90MB, good quality/size tradeoff for Pi"},{"location":"planning/TECH_REQ/#offline-fallback-stack","title":"Offline Fallback Stack","text":"Component Choice Alternative Rationale Local LLM Ollama + Llama 3.2 3B llama.cpp, Phi-3 Easy setup, 3B fits RAM, decent quality Local STT <code>whisper.cpp</code> (small) Vosk, faster-whisper Best quality for size, C++ optimized Local TTS Piper Coqui, eSpeak Neural quality, Pi optimized, fast Local Vision <code>SmolVLM2</code> or YOLO MediaPipe only VLM for descriptions, YOLO for detection"},{"location":"planning/TECH_REQ/#infrastructure","title":"Infrastructure","text":"Component Choice Rationale OS Raspberry Pi OS (64-bit) Official, ARM64, Reachy tested Process Management Single asyncio (MVP) Lower memory, simpler debugging, migrate to systemd v1.0+ Service Init systemd (agent.service) Boot integration, auto-restart Logging Python <code>logging</code> + <code>structlog</code> Structured JSON logs, file rotation"},{"location":"planning/TECH_REQ/#configuration","title":"Configuration","text":"Aspect Choice Rationale Format YAML + JSON Schema Human-readable editing, machine validation Validation <code>pydantic</code> Type-safe Python models from schema Secrets Environment variables Never in config files, .env.example template"},{"location":"planning/TECH_REQ/#dashboard-p2","title":"Dashboard (P2)","text":"Component Choice Rationale Backend FastAPI Already used by daemon, async native Frontend React Full SPA flexibility, WebSocket support Real-time WebSocket FastAPI native, status streaming"},{"location":"planning/TECH_REQ/#data-architecture","title":"Data Architecture","text":""},{"location":"planning/TECH_REQ/#core-entities","title":"Core Entities","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"definitions\": {\n    \"Memory\": {\n      \"type\": \"object\",\n      \"description\": \"A stored memory in ChromaDB\",\n      \"properties\": {\n        \"id\": {\n          \"type\": \"string\",\n          \"description\": \"Unique memory identifier\",\n          \"pattern\": \"^mem_[0-9]+\\\\.[0-9]+$\"\n        },\n        \"content\": {\n          \"type\": \"string\",\n          \"description\": \"The memory text content\"\n        },\n        \"embedding\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"number\"},\n          \"description\": \"384-dim vector from sentence-transformers\"\n        },\n        \"metadata\": {\n          \"$ref\": \"#/definitions/MemoryMetadata\"\n        }\n      },\n      \"required\": [\"id\", \"content\", \"metadata\"]\n    },\n    \"MemoryMetadata\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"timestamp\": {\n          \"type\": \"string\",\n          \"format\": \"date-time\"\n        },\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\"observation\", \"preference\", \"instruction\", \"interaction\", \"event\"]\n        },\n        \"source\": {\n          \"type\": \"string\",\n          \"enum\": [\"user\", \"system\", \"perception\", \"agent\"]\n        },\n        \"importance\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 1\n        },\n        \"expires_at\": {\n          \"type\": \"string\",\n          \"format\": \"date-time\",\n          \"description\": \"Optional TTL for temporary memories\"\n        }\n      },\n      \"required\": [\"timestamp\", \"type\", \"source\"]\n    },\n    \"PersonalityState\": {\n      \"type\": \"object\",\n      \"description\": \"Current personality/mood state\",\n      \"properties\": {\n        \"mood\": {\n          \"type\": \"string\",\n          \"enum\": [\"happy\", \"neutral\", \"tired\", \"curious\", \"alert\", \"playful\"]\n        },\n        \"energy\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 1,\n          \"description\": \"0 = exhausted, 1 = fully energized\"\n        },\n        \"last_interaction\": {\n          \"type\": \"string\",\n          \"format\": \"date-time\"\n        },\n        \"interaction_count_today\": {\n          \"type\": \"integer\",\n          \"minimum\": 0\n        }\n      },\n      \"required\": [\"mood\", \"energy\"]\n    },\n    \"AttentionState\": {\n      \"type\": \"object\",\n      \"description\": \"Current attention level\",\n      \"properties\": {\n        \"level\": {\n          \"type\": \"string\",\n          \"enum\": [\"passive\", \"alert\", \"engaged\"]\n        },\n        \"since\": {\n          \"type\": \"string\",\n          \"format\": \"date-time\"\n        },\n        \"trigger\": {\n          \"type\": \"string\",\n          \"description\": \"What caused current state\"\n        }\n      },\n      \"required\": [\"level\", \"since\"]\n    },\n    \"ToolExecution\": {\n      \"type\": \"object\",\n      \"description\": \"Audit log entry for tool execution\",\n      \"properties\": {\n        \"id\": {\n          \"type\": \"string\",\n          \"format\": \"uuid\"\n        },\n        \"timestamp\": {\n          \"type\": \"string\",\n          \"format\": \"date-time\"\n        },\n        \"tool_name\": {\n          \"type\": \"string\"\n        },\n        \"tool_input\": {\n          \"type\": \"object\"\n        },\n        \"permission_tier\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"maximum\": 4\n        },\n        \"decision\": {\n          \"type\": \"string\",\n          \"enum\": [\"allowed\", \"notified\", \"confirmed\", \"denied\"]\n        },\n        \"result\": {\n          \"type\": \"string\",\n          \"enum\": [\"success\", \"error\", \"timeout\"]\n        },\n        \"duration_ms\": {\n          \"type\": \"integer\"\n        }\n      },\n      \"required\": [\"id\", \"timestamp\", \"tool_name\", \"permission_tier\", \"decision\"]\n    },\n    \"Expression\": {\n      \"type\": \"object\",\n      \"description\": \"Antenna/body expression definition\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"left_antenna\": {\n          \"$ref\": \"#/definitions/AntennaMotion\"\n        },\n        \"right_antenna\": {\n          \"$ref\": \"#/definitions/AntennaMotion\"\n        },\n        \"head_motion\": {\n          \"$ref\": \"#/definitions/HeadMotion\"\n        },\n        \"duration_ms\": {\n          \"type\": \"integer\",\n          \"minimum\": 100\n        }\n      },\n      \"required\": [\"name\", \"left_antenna\", \"right_antenna\"]\n    },\n    \"AntennaMotion\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"start_angle\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 90},\n        \"end_angle\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 90},\n        \"pattern\": {\n          \"type\": \"string\",\n          \"enum\": [\"static\", \"wiggle\", \"wave\", \"pulse\"]\n        },\n        \"easing\": {\n          \"type\": \"string\",\n          \"enum\": [\"linear\", \"ease_in\", \"ease_out\", \"ease_in_out\"]\n        }\n      }\n    },\n    \"HeadMotion\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"pitch\": {\"type\": \"number\", \"minimum\": -30, \"maximum\": 30},\n        \"yaw\": {\"type\": \"number\", \"minimum\": -45, \"maximum\": 45},\n        \"roll\": {\"type\": \"number\", \"minimum\": -15, \"maximum\": 15}\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"planning/TECH_REQ/#storage-strategy","title":"Storage Strategy","text":"Data Type Storage Retention Backup Long-term memories ChromaDB (SQLite backend) Configurable (default 90 days) External SSD Audit logs SQLite 7 days rolling External SSD Personality state SQLite Permanent External SSD Session context In-memory Session only None Configuration YAML files Permanent Git"},{"location":"planning/TECH_REQ/#schema-migrations","title":"Schema Migrations","text":"<ul> <li>Use <code>alembic</code> for SQLite schema versioning</li> <li>ChromaDB collections are append-only, version in metadata</li> <li>Config schema version in YAML header</li> </ul>"},{"location":"planning/TECH_REQ/#mcp-server-specifications","title":"MCP Server Specifications","text":""},{"location":"planning/TECH_REQ/#reachy-body-control-mcp","title":"Reachy Body Control MCP","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"Reachy MCP Tools\",\n  \"description\": \"MCP tools for controlling Reachy Mini robot body\",\n  \"tools\": {\n    \"move_head\": {\n      \"description\": \"Move Reachy's head to look in a direction\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"direction\": {\n            \"type\": \"string\",\n            \"enum\": [\"left\", \"right\", \"up\", \"down\", \"front\"],\n            \"description\": \"Direction to look\"\n          },\n          \"speed\": {\n            \"type\": \"string\",\n            \"enum\": [\"slow\", \"normal\", \"fast\"],\n            \"default\": \"normal\"\n          },\n          \"degrees\": {\n            \"type\": \"number\",\n            \"minimum\": 0,\n            \"maximum\": 45,\n            \"description\": \"Optional: specific angle in degrees\"\n          }\n        },\n        \"required\": [\"direction\"]\n      },\n      \"permissionTier\": 1\n    },\n    \"play_emotion\": {\n      \"description\": \"Display an emotional expression through movement and antennas\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"emotion\": {\n            \"type\": \"string\",\n            \"enum\": [\"happy\", \"sad\", \"curious\", \"excited\", \"confused\", \"thinking\", \"surprised\", \"tired\", \"alert\"],\n            \"description\": \"Emotion to express\"\n          },\n          \"intensity\": {\n            \"type\": \"number\",\n            \"minimum\": 0.1,\n            \"maximum\": 1.0,\n            \"default\": 0.7\n          }\n        },\n        \"required\": [\"emotion\"]\n      },\n      \"permissionTier\": 1\n    },\n    \"speak\": {\n      \"description\": \"Speak text aloud through Reachy's speaker\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"text\": {\n            \"type\": \"string\",\n            \"maxLength\": 500,\n            \"description\": \"Text to speak\"\n          },\n          \"voice\": {\n            \"type\": \"string\",\n            \"default\": \"default\"\n          },\n          \"speed\": {\n            \"type\": \"number\",\n            \"minimum\": 0.5,\n            \"maximum\": 2.0,\n            \"default\": 1.0\n          }\n        },\n        \"required\": [\"text\"]\n      },\n      \"permissionTier\": 1\n    },\n    \"capture_image\": {\n      \"description\": \"Capture an image from Reachy's camera\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"analyze\": {\n            \"type\": \"boolean\",\n            \"default\": false,\n            \"description\": \"Whether to analyze the image content via vision model\"\n          },\n          \"save\": {\n            \"type\": \"boolean\",\n            \"default\": false,\n            \"description\": \"Whether to save the image to disk\"\n          }\n        }\n      },\n      \"permissionTier\": 1\n    },\n    \"set_antenna_state\": {\n      \"description\": \"Control antenna positions for expression\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"left_angle\": {\n            \"type\": \"number\",\n            \"minimum\": 0,\n            \"maximum\": 90\n          },\n          \"right_angle\": {\n            \"type\": \"number\",\n            \"minimum\": 0,\n            \"maximum\": 90\n          },\n          \"wiggle\": {\n            \"type\": \"boolean\",\n            \"default\": false\n          },\n          \"duration_ms\": {\n            \"type\": \"integer\",\n            \"default\": 500\n          }\n        }\n      },\n      \"permissionTier\": 1\n    },\n    \"get_sensor_data\": {\n      \"description\": \"Get current sensor readings\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"sensors\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"string\",\n              \"enum\": [\"imu\", \"audio_level\", \"temperature\", \"all\"]\n            },\n            \"default\": [\"all\"]\n          }\n        }\n      },\n      \"permissionTier\": 1\n    },\n    \"look_at_sound\": {\n      \"description\": \"Turn to face the direction of detected sound\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"timeout_ms\": {\n            \"type\": \"integer\",\n            \"default\": 2000\n          }\n        }\n      },\n      \"permissionTier\": 1\n    },\n    \"dance\": {\n      \"description\": \"Perform a choreographed dance routine\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"routine\": {\n            \"type\": \"string\",\n            \"description\": \"Name of dance routine\",\n            \"enum\": [\"celebrate\", \"greeting\", \"thinking\", \"custom\"]\n          },\n          \"duration_seconds\": {\n            \"type\": \"number\",\n            \"minimum\": 1,\n            \"maximum\": 30,\n            \"default\": 5\n          }\n        },\n        \"required\": [\"routine\"]\n      },\n      \"permissionTier\": 1\n    }\n  }\n}\n</code></pre>"},{"location":"planning/TECH_REQ/#external-mcp-integrations-mvp-priority","title":"External MCP Integrations (MVP Priority)","text":"Integration MCP Server Permission Tier MVP Status Home Assistant <code>homeassistant-mcp</code> 2 (notify) / 3 (security) \u2705 MVP Google Calendar Custom or <code>gcal-mcp</code> 1 (read) / 3 (create) \u2705 MVP GitHub <code>github-mcp</code> 1 (read) / 3 (create) \u2705 MVP Slack <code>slack-mcp</code> 2 (notify) v1.0 Spotify <code>spotify-mcp</code> 2 (notify) v1.0 Weather Custom 1 (read) \u2705 MVP"},{"location":"planning/TECH_REQ/#permission-system","title":"Permission System","text":""},{"location":"planning/TECH_REQ/#tier-definitions","title":"Tier Definitions","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"Permission Configuration\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"tiers\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"tier\": {\n            \"type\": \"integer\",\n            \"minimum\": 1,\n            \"maximum\": 4\n          },\n          \"name\": {\n            \"type\": \"string\",\n            \"enum\": [\"autonomous\", \"notify\", \"confirm\", \"forbidden\"]\n          },\n          \"description\": {\n            \"type\": \"string\"\n          },\n          \"behavior\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"execute\": {\"type\": \"boolean\"},\n              \"notify_user\": {\"type\": \"boolean\"},\n              \"require_confirmation\": {\"type\": \"boolean\"},\n              \"confirmation_timeout_seconds\": {\"type\": \"integer\"}\n            }\n          }\n        }\n      }\n    },\n    \"rules\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"pattern\": {\n            \"type\": \"string\",\n            \"description\": \"Tool name pattern with wildcards\"\n          },\n          \"tier\": {\n            \"type\": \"integer\",\n            \"minimum\": 1,\n            \"maximum\": 4\n          },\n          \"reason\": {\n            \"type\": \"string\"\n          }\n        },\n        \"required\": [\"pattern\", \"tier\", \"reason\"]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"planning/TECH_REQ/#default-permission-rules","title":"Default Permission Rules","text":"<pre><code># config/permissions.yaml\ntiers:\n  - tier: 1\n    name: autonomous\n    description: Execute immediately without notification\n    behavior:\n      execute: true\n      notify_user: false\n      require_confirmation: false\n\n  - tier: 2\n    name: notify\n    description: Execute and notify user\n    behavior:\n      execute: true\n      notify_user: true\n      require_confirmation: false\n\n  - tier: 3\n    name: confirm\n    description: Request confirmation before execution\n    behavior:\n      execute: true\n      notify_user: true\n      require_confirmation: true\n      confirmation_timeout_seconds: 60\n\n  - tier: 4\n    name: forbidden\n    description: Never execute, explain why\n    behavior:\n      execute: false\n      notify_user: true\n      require_confirmation: false\n\nrules:\n  # Tier 1: Autonomous (body control, observation)\n  - pattern: \"mcp__reachy__*\"\n    tier: 1\n    reason: \"Body control\"\n  - pattern: \"mcp__calendar__get_*\"\n    tier: 1\n    reason: \"Read-only calendar access\"\n  - pattern: \"mcp__weather__*\"\n    tier: 1\n    reason: \"Weather information\"\n  - pattern: \"mcp__github__get_*\"\n    tier: 1\n    reason: \"Read-only GitHub access\"\n\n  # Tier 2: Notify (reversible actions)\n  - pattern: \"mcp__homeassistant__turn_on_*\"\n    tier: 2\n    reason: \"Smart home control\"\n  - pattern: \"mcp__homeassistant__turn_off_*\"\n    tier: 2\n    reason: \"Smart home control\"\n  - pattern: \"mcp__slack__send_message\"\n    tier: 2\n    reason: \"Communication\"\n  - pattern: \"mcp__spotify__*\"\n    tier: 2\n    reason: \"Media control\"\n\n  # Tier 3: Confirm (irreversible or sensitive)\n  - pattern: \"mcp__calendar__create_*\"\n    tier: 3\n    reason: \"Creates calendar data\"\n  - pattern: \"mcp__github__create_*\"\n    tier: 3\n    reason: \"Creates repository data\"\n  - pattern: \"mcp__homeassistant__unlock_*\"\n    tier: 3\n    reason: \"Security action\"\n  - pattern: \"Bash\"\n    tier: 3\n    reason: \"System access\"\n\n  # Tier 4: Forbidden\n  - pattern: \"mcp__homeassistant__disarm_*\"\n    tier: 4\n    reason: \"Security critical - never autonomous\"\n  - pattern: \"mcp__email__send\"\n    tier: 4\n    reason: \"Impersonation risk\"\n  - pattern: \"mcp__banking__*\"\n    tier: 4\n    reason: \"Financial operations\"\n</code></pre>"},{"location":"planning/TECH_REQ/#security","title":"Security","text":""},{"location":"planning/TECH_REQ/#authentication-authorization","title":"Authentication &amp; Authorization","text":"Aspect Implementation API Keys Environment variables, never in config Inter-process localhost only, no auth needed Daemon API HTTP on localhost:8000, no external exposure Dashboard (P2) Optional basic auth or local-only"},{"location":"planning/TECH_REQ/#data-protection","title":"Data Protection","text":"Data Type Protection API keys Environment variables, <code>.env</code> in <code>.gitignore</code> Audio recordings Ephemeral (not stored by default) Images Ephemeral unless explicitly saved Memories Local encrypted SQLite (optional) Audit logs Local only, 7-day retention"},{"location":"planning/TECH_REQ/#owasp-considerations","title":"OWASP Considerations","text":"Risk Mitigation Injection Pydantic validation on all inputs, no shell interpolation Broken Auth Local-only operation, no remote access by default Sensitive Data No cloud storage, local-first architecture Security Misconfiguration Secure defaults, setup wizard validation"},{"location":"planning/TECH_REQ/#performance-scalability","title":"Performance &amp; Scalability","text":""},{"location":"planning/TECH_REQ/#performance-budgets","title":"Performance Budgets","text":"Metric Target Measurement Wake word detection &lt; 500ms From utterance end to detection callback Voice-to-response &lt; 3s From speech end to first audio output Motion smoothness 30 FPS Servo update rate Memory usage &lt; 2GB RAM Total process memory Startup time &lt; 60s From boot to ready API response (daemon) &lt; 100ms Local HTTP calls"},{"location":"planning/TECH_REQ/#resource-constraints-pi-4","title":"Resource Constraints (Pi 4)","text":"Resource Budget Allocation RAM (4GB) 2GB app, 1GB OS, 1GB buffer ChromaDB ~200MB, Models ~500MB, Agent ~300MB CPU (4 cores) 2 cores perception, 1 core agent, 1 core system Avoid thermal throttling Storage 32GB min, 64GB recommended 8GB models, 2GB memories, 4GB logs"},{"location":"planning/TECH_REQ/#scaling-strategy","title":"Scaling Strategy","text":"<p>Single user, single device - no horizontal scaling needed for MVP.</p> <p>Future considerations (v2.0+): - Multi-robot coordination via shared MCP server - Offload vision processing to edge device - Distributed memory across robots</p>"},{"location":"planning/TECH_REQ/#resilience-fault-tolerance","title":"Resilience &amp; Fault Tolerance","text":""},{"location":"planning/TECH_REQ/#graceful-degradation-modes","title":"Graceful Degradation Modes","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"Degradation Modes\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"modes\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\"type\": \"string\"},\n          \"trigger\": {\"type\": \"string\"},\n          \"capabilities\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"claude_api\": {\"type\": \"boolean\"},\n              \"speech_recognition\": {\"type\": \"string\", \"enum\": [\"cloud\", \"local\", \"none\"]},\n              \"text_to_speech\": {\"type\": \"string\", \"enum\": [\"cloud\", \"local\", \"none\"]},\n              \"vision\": {\"type\": \"string\", \"enum\": [\"full\", \"basic\", \"none\"]},\n              \"memory\": {\"type\": \"string\", \"enum\": [\"full\", \"local\", \"none\"]},\n              \"body_control\": {\"type\": \"boolean\"}\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"planning/TECH_REQ/#degradation-ladder","title":"Degradation Ladder","text":"Mode Trigger Claude API STT TTS Vision Body Full Normal operation \u2705 Cloud \u2705 Cloud \u2705 Cloud \u2705 Full \u2705 Offline No internet \u274c Ollama Whisper.cpp Piper Basic \u2705 Thermal CPU &gt; 80\u00b0C \u2705 Cloud \u2705 Cloud \u2705 Cloud \u274c Off \u26a0\ufe0f Slow Low Power Battery &lt; 20% \u274c Ollama Vosk Piper \u274c Off \u26a0\ufe0f Slow Safe Mode Critical error \u274c Off \u274c Off Canned \u274c Off \u26a0\ufe0f Safe pose"},{"location":"planning/TECH_REQ/#health-monitoring","title":"Health Monitoring","text":"Check Interval Threshold Action CPU temperature 5s &gt; 80\u00b0C Enter thermal mode Memory usage 30s &gt; 90% Clear caches, compact context API latency Per-call &gt; 5s Log warning, consider offline Disk space 5m &lt; 1GB Rotate logs, warn user Daemon health 10s 3 failures Restart daemon, notify"},{"location":"planning/TECH_REQ/#configuration-schemas","title":"Configuration Schemas","text":""},{"location":"planning/TECH_REQ/#main-configuration","title":"Main Configuration","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"Reachy Agent Configuration\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"version\": {\n      \"type\": \"string\",\n      \"pattern\": \"^[0-9]+\\\\.[0-9]+$\"\n    },\n    \"agent\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\",\n          \"default\": \"Reachy\"\n        },\n        \"wake_word\": {\n          \"type\": \"string\",\n          \"default\": \"hey reachy\"\n        },\n        \"model\": {\n          \"type\": \"string\",\n          \"enum\": [\"claude-sonnet-4-20250514\", \"claude-3-5-haiku-20241022\"],\n          \"default\": \"claude-sonnet-4-20250514\"\n        },\n        \"max_tokens\": {\n          \"type\": \"integer\",\n          \"default\": 1024\n        }\n      }\n    },\n    \"perception\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"wake_word_engine\": {\n          \"type\": \"string\",\n          \"enum\": [\"openwakeword\", \"porcupine\", \"vosk\"],\n          \"default\": \"openwakeword\"\n        },\n        \"wake_word_sensitivity\": {\n          \"type\": \"number\",\n          \"minimum\": 0,\n          \"maximum\": 1,\n          \"default\": 0.5\n        },\n        \"spatial_audio_enabled\": {\n          \"type\": \"boolean\",\n          \"default\": true\n        },\n        \"vision_enabled\": {\n          \"type\": \"boolean\",\n          \"default\": true\n        },\n        \"face_detection_enabled\": {\n          \"type\": \"boolean\",\n          \"default\": true\n        }\n      }\n    },\n    \"memory\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"chroma_path\": {\n          \"type\": \"string\",\n          \"default\": \"~/.reachy/memory/chroma\"\n        },\n        \"sqlite_path\": {\n          \"type\": \"string\",\n          \"default\": \"~/.reachy/memory/reachy.db\"\n        },\n        \"embedding_model\": {\n          \"type\": \"string\",\n          \"default\": \"all-MiniLM-L6-v2\"\n        },\n        \"max_memories\": {\n          \"type\": \"integer\",\n          \"default\": 10000\n        },\n        \"retention_days\": {\n          \"type\": \"integer\",\n          \"default\": 90\n        }\n      }\n    },\n    \"attention\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"passive_to_alert_motion_threshold\": {\n          \"type\": \"number\",\n          \"default\": 0.3\n        },\n        \"alert_to_passive_timeout_minutes\": {\n          \"type\": \"integer\",\n          \"default\": 5\n        },\n        \"engaged_to_alert_silence_seconds\": {\n          \"type\": \"integer\",\n          \"default\": 30\n        }\n      }\n    },\n    \"resilience\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"thermal_threshold_celsius\": {\n          \"type\": \"number\",\n          \"default\": 80\n        },\n        \"api_timeout_seconds\": {\n          \"type\": \"number\",\n          \"default\": 30\n        },\n        \"max_retries\": {\n          \"type\": \"integer\",\n          \"default\": 3\n        },\n        \"offline_llm_model\": {\n          \"type\": \"string\",\n          \"default\": \"llama3.2:3b\"\n        }\n      }\n    },\n    \"privacy\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"audit_logging_enabled\": {\n          \"type\": \"boolean\",\n          \"default\": true\n        },\n        \"audit_retention_days\": {\n          \"type\": \"integer\",\n          \"default\": 7\n        },\n        \"store_audio\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        },\n        \"store_images\": {\n          \"type\": \"boolean\",\n          \"default\": false\n        }\n      }\n    },\n    \"integrations\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"home_assistant\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"enabled\": {\"type\": \"boolean\", \"default\": false},\n            \"url\": {\"type\": \"string\"},\n            \"token_env_var\": {\"type\": \"string\", \"default\": \"HA_TOKEN\"}\n          }\n        },\n        \"google_calendar\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"enabled\": {\"type\": \"boolean\", \"default\": false},\n            \"credentials_path\": {\"type\": \"string\"}\n          }\n        },\n        \"github\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"enabled\": {\"type\": \"boolean\", \"default\": false},\n            \"token_env_var\": {\"type\": \"string\", \"default\": \"GITHUB_TOKEN\"},\n            \"repos\": {\n              \"type\": \"array\",\n              \"items\": {\"type\": \"string\"}\n            }\n          }\n        }\n      }\n    }\n  },\n  \"required\": [\"version\", \"agent\"]\n}\n</code></pre>"},{"location":"planning/TECH_REQ/#expression-definitions","title":"Expression Definitions","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"Expression Library\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"expressions\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"$ref\": \"#/definitions/Expression\"\n      }\n    }\n  },\n  \"definitions\": {\n    \"Expression\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"description\": {\"type\": \"string\"},\n        \"left_antenna\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"angle\": {\"type\": \"number\"},\n            \"pattern\": {\"type\": \"string\", \"enum\": [\"static\", \"wiggle\", \"wave\", \"pulse\"]},\n            \"speed\": {\"type\": \"number\", \"default\": 1.0}\n          }\n        },\n        \"right_antenna\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"angle\": {\"type\": \"number\"},\n            \"pattern\": {\"type\": \"string\"},\n            \"speed\": {\"type\": \"number\", \"default\": 1.0}\n          }\n        },\n        \"head\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"pitch\": {\"type\": \"number\"},\n            \"yaw\": {\"type\": \"number\"},\n            \"roll\": {\"type\": \"number\"}\n          }\n        },\n        \"duration_ms\": {\"type\": \"integer\", \"default\": 1000}\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"planning/TECH_REQ/#deployment-strategy","title":"Deployment Strategy","text":""},{"location":"planning/TECH_REQ/#development-environment","title":"Development Environment","text":"<pre><code># Clone and setup\ngit clone https://github.com/jawhnycooke/reachy-agent.git\ncd reachy-agent\n\n# Create virtual environment\nuv venv &amp;&amp; source .venv/bin/activate\n\n# Install dependencies\nuv pip install -r requirements.txt\nuv pip install -r requirements-dev.txt\n\n# Copy environment template\ncp .env.example .env\n# Edit .env with API keys\n\n# Run in development mode\npython -m src.main --dev\n</code></pre>"},{"location":"planning/TECH_REQ/#production-deployment-pi","title":"Production Deployment (Pi)","text":"<pre><code># Install script\ncurl -fsSL https://raw.githubusercontent.com/jawhnycooke/reachy-agent/main/install.sh | bash\n\n# Or manual:\n# 1. Clone repository\n# 2. Run setup wizard\npython scripts/setup_wizard.py\n\n# 3. Enable systemd service\nsudo systemctl enable reachy-agent\nsudo systemctl start reachy-agent\n</code></pre>"},{"location":"planning/TECH_REQ/#systemd-service-for-production","title":"systemd Service (for production)","text":"<pre><code># /etc/systemd/system/reachy-agent.service\n[Unit]\nDescription=Reachy Agent\nAfter=network.target reachy-daemon.service\nWants=reachy-daemon.service\n\n[Service]\nType=simple\nUser=reachy\nWorkingDirectory=/home/reachy/reachy-agent\nEnvironment=PATH=/home/reachy/.local/bin:/usr/bin\nEnvironmentFile=/home/reachy/reachy-agent/.env\nExecStart=/home/reachy/reachy-agent/.venv/bin/python -m src.main\nRestart=on-failure\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"planning/TECH_REQ/#testing-strategy","title":"Testing Strategy","text":""},{"location":"planning/TECH_REQ/#test-pyramid","title":"Test Pyramid","text":"Level Coverage Target Tools Unit 80% pytest, pytest-asyncio Integration Key paths pytest, MCP test client E2E (simulation) Happy paths MuJoCo, pytest E2E (hardware) Manual Physical robot"},{"location":"planning/TECH_REQ/#test-categories","title":"Test Categories","text":"<pre><code># Run all tests\nuvx pytest -v\n\n# Unit tests only\nuvx pytest tests/unit -v\n\n# Integration tests (requires daemon mock)\nuvx pytest tests/integration -v\n\n# With coverage\nuvx pytest --cov=src --cov-report=html\n</code></pre>"},{"location":"planning/TECH_REQ/#risks-mitigations","title":"Risks &amp; Mitigations","text":"Risk Likelihood Impact Mitigation ARM64 SDK issues Medium High Test early, Docker fallback, pin versions Thermal throttling Medium Medium Heatsink, active cooling, thermal monitoring API costs exceed budget Medium Medium Attention state optimization, local fallbacks ChromaDB corruption Low High Regular backups to external SSD Reachy daemon API changes Low High Pin Reachy SDK version, integration tests Wake word false positives Medium Low Tune sensitivity, add confirmation"},{"location":"planning/TECH_REQ/#prd-alignment","title":"PRD Alignment","text":""},{"location":"planning/TECH_REQ/#product-requirements-technical-decisions","title":"Product Requirements \u2192 Technical Decisions","text":"PRD Requirement Technical Approach G1: Functional autonomous agent Claude Agent SDK with MCP, async event loop G2: Reliable operation (&lt;1% crash) Health monitoring, graceful degradation, systemd restart G3: Extensible platform (5+ MCP) MCP standard, integration registry, permission system G5: Privacy-respecting Antenna state indicators, audit logging, local-first storage G6: Offline capability (30+ min) Ollama + Piper + Whisper.cpp fallback stack F1: Wake word detection (&lt;500ms) OpenWakeWord, local processing F7: Memory system ChromaDB + SQLite + sentence-transformers NFR: &lt;2GB RAM Single process, lazy loading, memory budgets"},{"location":"planning/TECH_REQ/#technical-decisions-product-impact","title":"Technical Decisions \u2192 Product Impact","text":"Technical Choice Product Impact OpenWakeWord vs Porcupine No license cost, fully customizable wake word Single asyncio process Simpler debugging, lower memory, less fault isolation Hybrid embeddings Real-time local, quality from periodic API reindex YAML + JSON Schema config Human-editable, machine-validatable, IDE support"},{"location":"planning/TECH_REQ/#next-steps","title":"Next Steps","text":"<p>This TRD feeds into the EPCC workflow:</p> <p>Greenfield project - recommended path: 1. \u2705 Review and approve this TRD 2. Run <code>/epcc-plan</code> to create implementation plan 3. Begin development with <code>/epcc-code</code> 4. Finalize with <code>/epcc-commit</code></p> <p>Key implementation questions resolved: - Wake word: OpenWakeWord - Embeddings: Hybrid (local + API) - Offline LLM: Ollama + Llama 3.2 3B - Config: YAML + JSON Schema - TTS: Piper - Process management: Single asyncio (MVP) - Dashboard: FastAPI + React (P2) - MVP integrations: Home Assistant, Google Calendar, GitHub</p> <p>End of TRD</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Learning-oriented guides for getting started with Reachy Agent development.</p>"},{"location":"tutorials/#available-tutorials","title":"Available Tutorials","text":""},{"location":"tutorials/#getting-started","title":"Getting Started","text":"<p>Time: ~30 minutes | Skill Level: Beginner</p> <p>A complete walkthrough from zero to controlling a simulated Reachy robot with AI. Covers: - Environment setup (Python, uv, virtual environment) - MuJoCo simulation installation (macOS) - Your first robot commands - Running the validation suite - Claude Agent SDK integration</p>"},{"location":"tutorials/#quick-reference","title":"Quick Reference","text":"<p>Time: Reference | Skill Level: Any</p> <p>A cheat sheet for common commands and API patterns. Use this as a daily reference once you've completed the Getting Started tutorial.</p>"},{"location":"tutorials/#learning-path","title":"Learning Path","text":"<pre><code>flowchart LR\n    A[\"Getting Started&lt;br/&gt;(30 min)\"] --&gt; B[\"Quick Reference&lt;br/&gt;(ongoing)\"]\n    B --&gt; C[\"Architecture Docs&lt;br/&gt;(understand)\"]\n    C --&gt; D[\"API Reference&lt;br/&gt;(build)\"]\n    D --&gt; E[\"Phase 2 Guide&lt;br/&gt;(hardware)\"]</code></pre>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>Before starting the tutorials, ensure you have:</p> <ul> <li>macOS (Apple Silicon M1/M2/M3 or Intel) or Linux</li> <li>Python 3.10+ installed</li> <li>Homebrew (macOS only) - brew.sh</li> <li>Basic familiarity with the command line</li> </ul>"},{"location":"tutorials/#what-youll-build","title":"What You'll Build","text":"<p>By following these tutorials, you'll create a development environment where:</p> <ol> <li>A physics simulation runs the Reachy Mini robot in MuJoCo</li> <li>Your Python code sends commands to control the robot</li> <li>Claude AI can reason about and execute robot movements</li> <li>All MCP (Model Context Protocol) tools are validated and working</li> </ol> <pre><code>Your Code \u2500\u2192 MCP Tools \u2500\u2192 HTTP API \u2500\u2192 MuJoCo \u2500\u2192 \ud83e\udd16 Robot\n                \u2191\n           Claude AI\n</code></pre>"},{"location":"tutorials/#next-steps-after-tutorials","title":"Next Steps After Tutorials","text":"<p>Once you've completed the tutorials:</p> <ol> <li>Explore the codebase - Read <code>src/reachy_agent/simulation/reachy_client.py</code></li> <li>Create custom expressions - Modify <code>scripts/live_demo.py</code></li> <li>Understand permissions - Read <code>docs/api/mcp-tools.md</code></li> <li>Prepare for hardware - Follow <code>docs/guides/phase2-preparation.md</code></li> </ol>"},{"location":"tutorials/#feedback","title":"Feedback","text":"<p>Found an issue with a tutorial? Have suggestions for improvement? Please open an issue on the repository.</p>"},{"location":"tutorials/getting-started/","title":"Tutorial: Getting Started with Reachy Agent Development","text":"<p>Welcome to embodied AI development with the Reachy Mini robot and Claude Agent SDK! This tutorial will guide you through setting up a complete development environment where you can control a simulated robot using AI.</p>"},{"location":"tutorials/getting-started/#what-youll-learn","title":"What You'll Learn","text":"<p>By the end of this tutorial, you will: - Have a working MuJoCo simulation of the Reachy Mini robot - Control the robot programmatically with Python - See the robot respond to your commands in real-time - Understand how to integrate the Claude Agent SDK for AI-powered control</p>"},{"location":"tutorials/getting-started/#prerequisites","title":"Prerequisites","text":""},{"location":"tutorials/getting-started/#required-knowledge","title":"Required Knowledge","text":"<ul> <li>Basic Python programming (functions, async/await)</li> <li>Familiarity with terminal/command line</li> </ul>"},{"location":"tutorials/getting-started/#required-setup","title":"Required Setup","text":"<ul> <li>macOS (Apple Silicon or Intel) or Linux</li> <li>Python 3.10 or higher</li> <li>Homebrew (macOS only)</li> <li>About 2GB free disk space</li> </ul>"},{"location":"tutorials/getting-started/#time-required","title":"Time Required","text":"<ul> <li>Approximately 30 minutes</li> </ul>"},{"location":"tutorials/getting-started/#part-1-environment-setup-10-min","title":"Part 1: Environment Setup (10 min)","text":"<p>Let's set up your development environment step by step.</p>"},{"location":"tutorials/getting-started/#step-11-verify-python-version","title":"Step 1.1: Verify Python Version","text":"<p>First, let's make sure you have Python 3.10 or higher installed.</p> <pre><code>python3 --version\n</code></pre> <p>You should see: <pre><code>Python 3.10.x  # or higher\n</code></pre></p> <p>If you don't have Python 3.10+, install it via Homebrew (macOS) or your system package manager.</p>"},{"location":"tutorials/getting-started/#step-12-install-uv-package-manager","title":"Step 1.2: Install uv Package Manager","text":"<p>We'll use <code>uv</code> for fast Python package management:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>After installation, restart your terminal or run: <pre><code>source ~/.bashrc  # or ~/.zshrc on macOS\n</code></pre></p> <p>Verify it's installed: <pre><code>uv --version\n</code></pre></p> <p>You should see: <pre><code>uv 0.x.x\n</code></pre></p>"},{"location":"tutorials/getting-started/#step-13-clone-the-repository","title":"Step 1.3: Clone the Repository","text":"<pre><code>git clone https://github.com/jawhnycooke/reachy-agent.git\ncd reachy-agent\n</code></pre>"},{"location":"tutorials/getting-started/#step-14-create-virtual-environment","title":"Step 1.4: Create Virtual Environment","text":"<pre><code>uv venv\nsource .venv/bin/activate\n</code></pre> <p>Your prompt should now show <code>(.venv)</code>: <pre><code>(.venv) user@machine:~/reachy-agent$\n</code></pre></p>"},{"location":"tutorials/getting-started/#step-15-install-core-dependencies","title":"Step 1.5: Install Core Dependencies","text":"<pre><code>uv pip install -r requirements.txt\n</code></pre> <p>You should see packages being installed: <pre><code>Resolved 25 packages in 1.2s\nDownloaded 25 packages in 3.4s\nInstalled 25 packages in 0.8s\n</code></pre></p>"},{"location":"tutorials/getting-started/#checkpoint-verify-environment","title":"Checkpoint: Verify Environment","text":"<p>Let's make sure everything is working so far:</p> <pre><code>python -c \"import httpx; import pydantic; print('Core dependencies OK!')\"\n</code></pre> <p>You should see: <pre><code>Core dependencies OK!\n</code></pre></p>"},{"location":"tutorials/getting-started/#part-2-mujoco-simulation-setup-10-min","title":"Part 2: MuJoCo Simulation Setup (10 min)","text":"<p>Now we'll set up the physics simulation that lets you control a virtual Reachy robot.</p>"},{"location":"tutorials/getting-started/#step-21-install-mujoco-macos","title":"Step 2.1: Install MuJoCo (macOS)","text":"<p>On macOS, MuJoCo needs to be installed via Homebrew for proper GUI support:</p> <pre><code>brew install mujoco\n</code></pre> <p>Verify the installation: <pre><code>which mjpython\n</code></pre></p> <p>You should see: <pre><code>/opt/homebrew/bin/mjpython\n</code></pre></p> <p>Note: <code>mjpython</code> is a special Python launcher that enables MuJoCo's GUI on macOS.</p>"},{"location":"tutorials/getting-started/#step-22-install-reachy-mini-sdk","title":"Step 2.2: Install Reachy Mini SDK","text":"<p>The Reachy Mini SDK needs to be installed in the system Python that <code>mjpython</code> uses:</p> <pre><code>/opt/homebrew/bin/pip3 install reachy-mini\n</code></pre> <p>You should see: <pre><code>Successfully installed reachy-mini-x.x.x\n</code></pre></p>"},{"location":"tutorials/getting-started/#step-23-start-the-simulation","title":"Step 2.3: Start the Simulation","text":"<p>Now for the exciting part - let's launch the robot simulation!</p> <p>Open a new terminal window (keep it running) and execute:</p> <pre><code>/opt/homebrew/bin/mjpython -m reachy_mini.daemon.app.main --sim --scene minimal --fastapi-port 8765\n</code></pre> <p>You should see: 1. A window appear showing the Reachy Mini robot 2. Console output like: <pre><code>INFO:     Started server process [12345]\nINFO:     Uvicorn running on http://0.0.0.0:8765\n</code></pre></p> <p>Keep this window open! The simulation needs to run while you control the robot.</p>"},{"location":"tutorials/getting-started/#step-24-verify-the-daemon-is-running","title":"Step 2.4: Verify the Daemon is Running","text":"<p>In your original terminal (with the virtual environment active), test the connection:</p> <pre><code>curl -s http://localhost:8765/api/daemon/status | python3 -m json.tool\n</code></pre> <p>You should see: <pre><code>{\n    \"state\": \"running\",\n    \"simulation_enabled\": true,\n    \"scene\": \"minimal\"\n}\n</code></pre></p>"},{"location":"tutorials/getting-started/#checkpoint-simulation-ready","title":"Checkpoint: Simulation Ready","text":"<p>Before continuing, verify: - [ ] MuJoCo window is visible with the robot - [ ] Daemon status returns \"running\" - [ ] You can see the robot's head and antennas</p>"},{"location":"tutorials/getting-started/#part-3-your-first-robot-commands-10-min","title":"Part 3: Your First Robot Commands (10 min)","text":"<p>Let's make the robot move! We'll use Python to send commands to the simulation.</p>"},{"location":"tutorials/getting-started/#step-31-open-python-interactive-shell","title":"Step 3.1: Open Python Interactive Shell","text":"<p>In your terminal with the virtual environment active:</p> <pre><code>cd ~/reachy-agent  # or wherever you cloned the repo\nsource .venv/bin/activate\npython\n</code></pre>"},{"location":"tutorials/getting-started/#step-32-connect-to-the-robot","title":"Step 3.2: Connect to the Robot","text":"<p>In the Python shell:</p> <pre><code>import asyncio\nimport sys\nsys.path.insert(0, \"src\")  # Add source to path\n\nfrom reachy_agent.simulation.reachy_client import ReachyMiniClient\n\n# Create client\nclient = ReachyMiniClient(base_url=\"http://localhost:8765\")\n\n# Wake up the robot\nasync def wake():\n    result = await client.wake_up()\n    print(f\"Robot awake: {result}\")\n\nasyncio.run(wake())\n</code></pre> <p>You should see: <pre><code>Robot awake: {'uuid': 'abc123...'}\n</code></pre></p> <p>Look at the MuJoCo window! The robot should now be in an alert position with antennas up.</p>"},{"location":"tutorials/getting-started/#step-33-move-the-head","title":"Step 3.3: Move the Head","text":"<p>Let's make the robot look around:</p> <pre><code>async def look_around():\n    # Look left\n    print(\"Looking left...\")\n    await client.move_head(\"left\", speed=\"normal\")\n    await asyncio.sleep(1)\n\n    # Look right\n    print(\"Looking right...\")\n    await client.move_head(\"right\", speed=\"normal\")\n    await asyncio.sleep(1)\n\n    # Look up\n    print(\"Looking up...\")\n    await client.move_head(\"up\", speed=\"normal\")\n    await asyncio.sleep(1)\n\n    # Return to center\n    print(\"Centering...\")\n    await client.move_head(\"front\", speed=\"normal\")\n\nasyncio.run(look_around())\n</code></pre> <p>Watch the simulation window! You should see the robot's head moving in each direction.</p>"},{"location":"tutorials/getting-started/#step-34-control-the-antennas","title":"Step 3.4: Control the Antennas","text":"<p>The antennas are expressive - let's make them move:</p> <pre><code>async def antenna_expressions():\n    # Antennas up (excited!)\n    print(\"Excited pose...\")\n    await client.set_antenna_state(left_angle=90, right_angle=90)\n    await asyncio.sleep(1)\n\n    # Curious tilt\n    print(\"Curious pose...\")\n    await client.set_antenna_state(left_angle=30, right_angle=70)\n    await asyncio.sleep(1)\n\n    # Neutral\n    print(\"Neutral pose...\")\n    await client.set_antenna_state(left_angle=45, right_angle=45)\n\nasyncio.run(antenna_expressions())\n</code></pre>"},{"location":"tutorials/getting-started/#step-35-make-the-robot-nod","title":"Step 3.5: Make the Robot Nod","text":"<p>Let's have the robot agree with you:</p> <pre><code>async def nod_yes():\n    print(\"Nodding...\")\n    result = await client.nod(times=3, speed=\"normal\")\n    print(f\"Nod complete: {result}\")\n\nasyncio.run(nod_yes())\n</code></pre> <p>Watch for it! The robot will nod its head up and down three times.</p>"},{"location":"tutorials/getting-started/#step-36-clean-up","title":"Step 3.6: Clean Up","text":"<p>When you're done experimenting:</p> <pre><code>async def cleanup():\n    await client.rest()  # Return to neutral position\n    await client.close()\n    print(\"Done!\")\n\nasyncio.run(cleanup())\nexit()\n</code></pre>"},{"location":"tutorials/getting-started/#checkpoint-robot-control-works","title":"Checkpoint: Robot Control Works!","text":"<p>You've successfully: - [ ] Woken up the robot - [ ] Moved its head in different directions - [ ] Controlled antenna positions - [ ] Made it perform gestures (nodding)</p>"},{"location":"tutorials/getting-started/#part-4-running-the-demo-script-5-min","title":"Part 4: Running the Demo Script (5 min)","text":"<p>Now let's run a complete demonstration that shows all the robot's capabilities.</p>"},{"location":"tutorials/getting-started/#step-41-run-the-live-demo","title":"Step 4.1: Run the Live Demo","text":"<pre><code>python scripts/live_demo.py\n</code></pre> <p>You should see: <pre><code>============================================================\n\ud83e\udd16 Reachy Mini Live Demo\n============================================================\n\n1\ufe0f\u20e3  Waking up robot...\n   \u2713 {'uuid': '...'}\n\n2\ufe0f\u20e3  Head movements:\n   \u2192 Looking left... \u2713\n   \u2192 Looking right... \u2713\n   \u2192 Looking up... \u2713\n   \u2192 Looking down... \u2713\n   \u2192 Looking front... \u2713\n\n3\ufe0f\u20e3  Antenna expressions:\n   \u2192 Antennas down (sleeping)... \u2713\n   \u2192 Antennas mid (alert)... \u2713\n   \u2192 Antennas up (engaged)... \u2713\n   ...\n\n\u2705 Demo complete!\n============================================================\n</code></pre></p> <p>Watch the simulation window as each movement happens in real-time!</p>"},{"location":"tutorials/getting-started/#part-5-mcp-validation-optional-5-min","title":"Part 5: MCP Validation (Optional) (5 min)","text":"<p>This validates that all the MCP (Model Context Protocol) tools work correctly.</p> <pre><code>python scripts/validate_mcp_e2e.py\n</code></pre> <p>Expected output: <pre><code>============================================================\n\ud83d\udd27 MCP End-to-End Validation\n============================================================\n\n[1/8] Testing get_status (health check)... \u2705 PASSED\n[2/8] Testing wake_up tool... \u2705 PASSED\n[3/8] Testing move_head tool... \u2705 PASSED\n[4/8] Testing look_at tool... \u2705 PASSED\n[5/8] Testing set_antenna_state tool... \u2705 PASSED\n[6/8] Testing nod gesture... \u2705 PASSED\n[7/8] Testing shake gesture... \u2705 PASSED\n[8/8] Testing combined expression sequence... \u2705 PASSED\n\n============================================================\n\ud83d\udcca Results Summary\n============================================================\n   Passed: 8/8\n\n\ud83c\udf89 ALL MCP TOOLS VALIDATED SUCCESSFULLY!\n============================================================\n</code></pre></p>"},{"location":"tutorials/getting-started/#part-6-claude-agent-sdk-integration-10-min","title":"Part 6: Claude Agent SDK Integration (10 min)","text":"<p>This is where it gets exciting - we'll have Claude control the robot!</p>"},{"location":"tutorials/getting-started/#step-61-get-an-anthropic-api-key","title":"Step 6.1: Get an Anthropic API Key","text":"<ol> <li>Go to console.anthropic.com</li> <li>Sign in or create an account</li> <li>Navigate to API Keys</li> <li>Create a new API key</li> <li>Copy the key (starts with <code>sk-ant-</code>)</li> </ol>"},{"location":"tutorials/getting-started/#step-62-set-the-api-key","title":"Step 6.2: Set the API Key","text":"<pre><code>export ANTHROPIC_API_KEY=\"sk-ant-your-key-here\"\n</code></pre>"},{"location":"tutorials/getting-started/#step-63-run-the-agent-validation","title":"Step 6.3: Run the Agent Validation","text":"<pre><code>python scripts/validate_agent_e2e.py\n</code></pre> <p>You should see: <pre><code>======================================================================\n\ud83e\udd16 Claude Agent SDK End-to-End Validation\n======================================================================\n\n\u2705 API Key configured: sk-ant-api03-YCodm...\n\u2705 Simulation daemon running at http://localhost:8765\n\n----------------------------------------------------------------------\n\ud83d\udcdd Test Scenario: Ask Claude to make Reachy express curiosity\n----------------------------------------------------------------------\n\n\ud83d\udd04 Sending request to Claude API...\n\n\ud83d\udce8 Response received (stop_reason: tool_use)\n\n\ud83d\udd27 Claude called: move_head({'direction': 'left', 'speed': 'normal'})\n   \u2705 Executed successfully\n\ud83d\udd27 Claude called: set_antenna_state({'left_angle': 75, 'right_angle': 80})\n   \u2705 Executed successfully\n\n----------------------------------------------------------------------\n\ud83d\udcac Claude's response:\n----------------------------------------------------------------------\nI've made Reachy express curiosity by turning its head to the left\nand raising its antennas asymmetrically...\n\n======================================================================\n\ud83d\udcca Validation Summary\n======================================================================\n   API calls to Claude: 1+\n   Tool calls executed: 2\n   Tools used: move_head, set_antenna_state\n\n\ud83c\udf89 FULL AGENT STACK VALIDATED!\n\n   User Request\n        \u2193\n   Claude API (reasoning)\n        \u2193\n   Tool Calls (MCP interface)\n        \u2193\n   ReachyMiniClient (HTTP)\n        \u2193\n   Reachy Daemon (FastAPI)\n        \u2193\n   MuJoCo Physics Engine\n        \u2193\n   \ud83e\udd16 Robot Movement!\n======================================================================\n</code></pre></p> <p>Watch the simulation! You'll see Claude decide how to make the robot look curious and execute the movements.</p>"},{"location":"tutorials/getting-started/#what-youve-accomplished","title":"What You've Accomplished","text":"<p>Congratulations! You've successfully:</p> <ul> <li>\u2705 Set up a Python development environment with uv</li> <li>\u2705 Installed and configured MuJoCo simulation</li> <li>\u2705 Launched the Reachy Mini simulation daemon</li> <li>\u2705 Controlled the robot programmatically</li> <li>\u2705 Run the complete validation suite</li> <li>\u2705 Connected Claude AI to control the robot</li> </ul>"},{"location":"tutorials/getting-started/#understanding-the-architecture","title":"Understanding the Architecture","text":"<p>Here's what you built:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Your Code / Claude              \u2502\n\u2502     (Python scripts, Agent SDK, MCP)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502 HTTP API\n                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Reachy Daemon (FastAPI)             \u2502\n\u2502     Running on localhost:8765                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502 Physics Commands\n                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           MuJoCo Physics Engine              \u2502\n\u2502     Simulates robot physics                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have a working development environment:</p>"},{"location":"tutorials/getting-started/#try-these-variations","title":"Try These Variations","text":"<ul> <li>Modify <code>scripts/live_demo.py</code> to create your own movement sequences</li> <li>Edit the prompt in <code>scripts/validate_agent_e2e.py</code> to try different emotions</li> <li>Create expressions like \"confused\", \"happy\", or \"sleepy\"</li> </ul>"},{"location":"tutorials/getting-started/#explore-the-codebase","title":"Explore the Codebase","text":"<ul> <li><code>src/reachy_agent/simulation/reachy_client.py</code> - The robot control API</li> <li><code>src/reachy_agent/mcp_servers/reachy/reachy_mcp.py</code> - MCP tool definitions (23 tools)</li> <li><code>src/reachy_agent/mcp_servers/reachy/__main__.py</code> - Standalone MCP server entry point</li> <li><code>src/reachy_agent/agent/agent.py</code> - Agent loop with MCP client integration</li> <li><code>src/reachy_agent/permissions/tiers.py</code> - Permission system</li> </ul>"},{"location":"tutorials/getting-started/#continue-learning","title":"Continue Learning","text":"<ul> <li>Architecture Overview - Understand the full system</li> <li>MCP Tools Reference - All 23 available tools</li> <li>Phase 2 Preparation - Hardware integration</li> </ul>"},{"location":"tutorials/getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/getting-started/#issue-mjpython-command-not-found","title":"Issue: <code>mjpython: command not found</code>","text":"<p>Solution: Install MuJoCo via Homebrew: <pre><code>brew install mujoco\n</code></pre></p> <p>Then verify: <pre><code>which mjpython\n</code></pre></p>"},{"location":"tutorials/getting-started/#issue-modulenotfounderror-no-module-named-reachy_mini","title":"Issue: <code>ModuleNotFoundError: No module named 'reachy_mini'</code>","text":"<p>Solution: Install reachy-mini in the system Python: <pre><code>/opt/homebrew/bin/pip3 install reachy-mini\n</code></pre></p>"},{"location":"tutorials/getting-started/#issue-connection-refused-when-testing-daemon","title":"Issue: <code>Connection refused</code> when testing daemon","text":"<p>Solution: Make sure the simulation is running in another terminal: <pre><code>/opt/homebrew/bin/mjpython -m reachy_mini.daemon.app.main --sim --scene minimal --fastapi-port 8765\n</code></pre></p>"},{"location":"tutorials/getting-started/#issue-anthropic_api_key-not-set","title":"Issue: <code>ANTHROPIC_API_KEY not set</code>","text":"<p>Solution: Set the environment variable: <pre><code>export ANTHROPIC_API_KEY=\"sk-ant-your-key-here\"\n</code></pre></p>"},{"location":"tutorials/getting-started/#issue-robot-movements-are-too-fast-to-see","title":"Issue: Robot movements are too fast to see","text":"<p>Solution: In your scripts, add <code>await asyncio.sleep(2.0)</code> between movements: <pre><code>await client.move_head(\"left\")\nawait asyncio.sleep(2.0)  # Wait 2 seconds\nawait client.move_head(\"right\")\n</code></pre></p>"},{"location":"tutorials/getting-started/#issue-mujoco-window-doesnt-appear-linux","title":"Issue: MuJoCo window doesn't appear (Linux)","text":"<p>Solution: Make sure you have a display server running. Try: <pre><code>export DISPLAY=:0\npython -m reachy_mini.daemon.app.main --sim --scene minimal --fastapi-port 8765\n</code></pre></p>"},{"location":"tutorials/getting-started/#connecting-to-real-hardware","title":"Connecting to Real Hardware","text":"<p>Once you've tested with simulation, you can connect to real Reachy Mini hardware.</p>"},{"location":"tutorials/getting-started/#from-your-development-machine","title":"From Your Development Machine","text":"<p>The robot's daemon is accessible over the network:</p> <pre><code># Test daemon connectivity (replace with your robot's hostname)\ncurl http://reachy-mini.local:8000/api/daemon/status\n\n# Run agent pointing to real hardware\nREACHY_DAEMON_URL=http://reachy-mini.local:8000 python -m reachy_agent run\n</code></pre>"},{"location":"tutorials/getting-started/#from-the-robots-raspberry-pi-ssh","title":"From the Robot's Raspberry Pi (SSH)","text":"<p>For headless operation, SSH into the robot:</p> <pre><code>ssh pollen@reachy-mini.local\n# Password: root\n\ncd ~/reachy_agent\nsource .venv/bin/activate\npython -m reachy_agent run\n</code></pre>"},{"location":"tutorials/getting-started/#key-differences-simulation-vs-real-hardware","title":"Key Differences: Simulation vs Real Hardware","text":"Aspect Simulation Real Hardware Port 8765 8000 Movement API <code>/api/move/goto</code> <code>/api/move/set_target</code> Backend Mock daemon Pollen daemon <p>The agent auto-detects which backend is running and uses the appropriate API for smooth movements.</p>"},{"location":"tutorials/getting-started/#troubleshooting-real-hardware","title":"Troubleshooting Real Hardware","text":"<p>If the robot becomes unresponsive:</p> <ol> <li>Open <code>http://reachy-mini.local:8000/settings</code> in your browser</li> <li>Toggle the On/Off switch off, then on</li> <li>Run <code>wake_up</code> command to re-enable motor control</li> </ol> <p>See the Troubleshooting Guide for more solutions.</p>"},{"location":"tutorials/getting-started/#complete-reference-script","title":"Complete Reference Script","text":"<p>Here's a complete script you can use as a starting point:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"My first Reachy robot script.\"\"\"\n\nimport asyncio\nimport sys\nfrom pathlib import Path\n\n# Add src to path\nsys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))\n\nfrom reachy_agent.simulation.reachy_client import ReachyMiniClient\n\n\nasync def main():\n    \"\"\"Control the Reachy robot.\"\"\"\n    # Connect to simulation\n    client = ReachyMiniClient(base_url=\"http://localhost:8765\")\n\n    try:\n        # Wake up\n        print(\"Waking up...\")\n        await client.wake_up()\n        await asyncio.sleep(1)\n\n        # Your movements here\n        print(\"Looking around...\")\n        await client.move_head(\"left\", speed=\"normal\")\n        await asyncio.sleep(1)\n\n        await client.move_head(\"right\", speed=\"normal\")\n        await asyncio.sleep(1)\n\n        # Express something\n        print(\"Expressing curiosity...\")\n        await client.set_antenna_state(left_angle=30, right_angle=80)\n        await client.look_at(roll=10, pitch=-5, yaw=20)\n        await asyncio.sleep(2)\n\n        # Return to rest\n        print(\"Resting...\")\n        await client.rest()\n\n    finally:\n        await client.close()\n        print(\"Done!\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Save this as <code>my_first_script.py</code> in the project's <code>scripts/</code> folder and run: <pre><code>python scripts/my_first_script.py\n</code></pre></p> <p>Happy robot programming! \ud83e\udd16</p>"},{"location":"tutorials/quick-reference/","title":"Quick Reference Card","text":"<p>A cheat sheet for common Reachy Agent development tasks.</p>"},{"location":"tutorials/quick-reference/#starting-the-simulation","title":"Starting the Simulation","text":""},{"location":"tutorials/quick-reference/#macos-with-gui","title":"macOS (with GUI)","text":"<pre><code># Terminal 1: Start simulation\n/opt/homebrew/bin/mjpython -m reachy_mini.daemon.app.main --sim --scene minimal --fastapi-port 8765\n\n# Terminal 2: Run your code\nsource .venv/bin/activate\npython scripts/live_demo.py\n</code></pre>"},{"location":"tutorials/quick-reference/#headless-mode-cissh","title":"Headless Mode (CI/SSH)","text":"<pre><code>python -m reachy_mini.daemon.app.main --sim --scene minimal --headless --fastapi-port 8765\n</code></pre>"},{"location":"tutorials/quick-reference/#common-commands","title":"Common Commands","text":""},{"location":"tutorials/quick-reference/#environment-setup","title":"Environment Setup","text":"<pre><code>uv venv &amp;&amp; source .venv/bin/activate  # Create/activate venv\nuv pip install -r requirements.txt     # Install dependencies\nuv pip install -r requirements-dev.txt # Install dev tools\n</code></pre>"},{"location":"tutorials/quick-reference/#running-tests","title":"Running Tests","text":"<pre><code>pytest -v                              # All tests\npytest tests/simulation/ -v            # Simulation tests only\npytest -m \"not slow\" -v                # Skip slow tests\n</code></pre>"},{"location":"tutorials/quick-reference/#code-quality","title":"Code Quality","text":"<pre><code>black . &amp;&amp; isort .                     # Format code\nmypy src/                              # Type check\nruff check .                           # Lint\n</code></pre>"},{"location":"tutorials/quick-reference/#reachyminiclient-api","title":"ReachyMiniClient API","text":""},{"location":"tutorials/quick-reference/#connection","title":"Connection","text":"<pre><code>from reachy_agent.simulation.reachy_client import ReachyMiniClient\n\nclient = ReachyMiniClient(base_url=\"http://localhost:8765\")\n</code></pre>"},{"location":"tutorials/quick-reference/#lifecycle","title":"Lifecycle","text":"<pre><code>await client.wake_up()   # Activate motors\nawait client.rest()      # Neutral position\nawait client.sleep()     # Deactivate motors\nawait client.close()     # Close connection\n</code></pre>"},{"location":"tutorials/quick-reference/#head-movement","title":"Head Movement","text":"<pre><code># Cardinal directions\nawait client.move_head(\"left\", speed=\"normal\")   # left, right, up, down, front\nawait client.move_head(\"right\", speed=\"fast\")    # slow, normal, fast\n\n# Precise positioning (degrees)\nawait client.look_at(roll=10, pitch=-15, yaw=30)\n</code></pre>"},{"location":"tutorials/quick-reference/#antennas","title":"Antennas","text":"<pre><code># Angles: 0=down, 45=neutral, 90=up\nawait client.set_antenna_state(left_angle=45, right_angle=45)  # Neutral\nawait client.set_antenna_state(left_angle=90, right_angle=90)  # Alert\nawait client.set_antenna_state(left_angle=30, right_angle=70)  # Curious\n</code></pre>"},{"location":"tutorials/quick-reference/#gestures","title":"Gestures","text":"<pre><code>await client.nod(times=2, speed=\"normal\")   # Affirmative\nawait client.shake(times=2, speed=\"normal\") # Negative\n</code></pre>"},{"location":"tutorials/quick-reference/#body-rotation","title":"Body Rotation","text":"<pre><code>await client.rotate(\"left\", degrees=45, speed=\"normal\")\nawait client.rotate(\"right\", degrees=90, speed=\"fast\")\n</code></pre>"},{"location":"tutorials/quick-reference/#expression-presets","title":"Expression Presets","text":"Expression Head Antennas Gesture Curious roll=10, yaw=20 L=30, R=70 - Happy pitch=-10 L=90, R=90 nod Sad pitch=20, roll=-5 L=20, R=20 - Confused roll=15 L=60, R=30 - Thinking yaw=15 L=60, R=60 - Agreeing - L=70, R=70 nod x3 Disagreeing - L=30, R=30 shake x2"},{"location":"tutorials/quick-reference/#claude-agent-sdk-integration","title":"Claude Agent SDK Integration","text":""},{"location":"tutorials/quick-reference/#basic-tool-call-pattern","title":"Basic Tool Call Pattern","text":"<pre><code>import anthropic\n\nclient = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\ntools = [\n    {\n        \"name\": \"move_head\",\n        \"description\": \"Move the robot's head\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"direction\": {\"type\": \"string\", \"enum\": [\"left\", \"right\", \"up\", \"down\", \"front\"]},\n                \"speed\": {\"type\": \"string\", \"enum\": [\"slow\", \"normal\", \"fast\"]}\n            },\n            \"required\": [\"direction\"]\n        }\n    }\n]\n\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=1024,\n    tools=tools,\n    messages=[{\"role\": \"user\", \"content\": \"Make the robot look curious\"}]\n)\n</code></pre>"},{"location":"tutorials/quick-reference/#api-endpoints-direct-http","title":"API Endpoints (Direct HTTP)","text":"Endpoint Method Description <code>/api/daemon/status</code> GET Health check <code>/api/state/full</code> GET Full robot state <code>/api/move/set_target</code> POST Move to position (real hardware) <code>/api/move/goto</code> POST Move to position (simulation) <code>/api/move/play/wake_up</code> POST Wake up robot <code>/api/move/play/goto_sleep</code> POST Sleep robot <p>Note: Real hardware uses <code>/api/move/set_target</code> for smooth movements. The <code>goto</code> API can cause snapping because it includes <code>x</code>, <code>y</code>, <code>z</code> position fields that default to 0.</p>"},{"location":"tutorials/quick-reference/#example-direct-api-call","title":"Example: Direct API Call","text":"<pre><code># Status check\ncurl -s http://localhost:8765/api/daemon/status | python3 -m json.tool\n\n# Move head (simulation - port 8765)\ncurl -X POST http://localhost:8765/api/move/goto \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"head_pose\": {\"roll\": 0, \"pitch\": 0, \"yaw\": 0.5}, \"duration\": 1.0}'\n\n# Move head (real hardware - port 8000)\ncurl -X POST http://localhost:8000/api/move/set_target \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"target_head_pose\": {\"roll\": 0, \"pitch\": 0, \"yaw\": 0.5}}'\n</code></pre>"},{"location":"tutorials/quick-reference/#file-locations","title":"File Locations","text":"Path Purpose <code>src/reachy_agent/simulation/reachy_client.py</code> Robot client API <code>src/reachy_agent/mcp_servers/reachy/reachy_mcp.py</code> MCP tool definitions (23 tools) <code>scripts/live_demo.py</code> Interactive demo <code>scripts/validate_mcp_e2e.py</code> MCP validation <code>scripts/validate_agent_e2e.py</code> Full stack validation <code>config/default.yaml</code> Default configuration"},{"location":"tutorials/quick-reference/#environment-variables","title":"Environment Variables","text":"Variable Required Description <code>ANTHROPIC_API_KEY</code> For Claude API key for Claude <code>REACHY_DEBUG</code> No Enable debug logging <code>REACHY_DAEMON_URL</code> No Override daemon URL"},{"location":"tutorials/quick-reference/#troubleshooting-quick-fixes","title":"Troubleshooting Quick Fixes","text":"<pre><code># Kill all daemon processes\npkill -f \"reachy_mini\"\n\n# Check if port is in use\nlsof -i :8765\n\n# Reset virtual environment\nrm -rf .venv &amp;&amp; uv venv &amp;&amp; source .venv/bin/activate &amp;&amp; uv pip install -r requirements.txt\n</code></pre>"}]}